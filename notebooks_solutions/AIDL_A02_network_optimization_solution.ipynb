{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDL_A02_network_optimization_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ounospanas/AIDL_A_02/blob/main/notebooks_solutions/AIDL_A02_network_optimization_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j25rklnwD1-m"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JenmhPusFZ06"
      },
      "source": [
        "#load dataset\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "V7uVONPjF1PP",
        "outputId": "7fa1df88-daf1-45ef-f823-5b503491abf1"
      },
      "source": [
        "#visualize some data\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "selected = [0,1,3,5,6,8,16,18,19,23]\n",
        "plt.figure(figsize=(11, 11))\n",
        "for i, s in enumerate(selected):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    #img = plt.imread(x_train[s])\n",
        "    plt.imshow(x_train[s], cmap='gray')\n",
        "    plt.xlabel(classes[y_train[s]],)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x1100 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8NenH6QEPlH"
      },
      "source": [
        "# normalize data\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)/255\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)/255"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zD2QsqukISO"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWtYiD_eEJsq"
      },
      "source": [
        "# select only t-shirts and ankle boots\n",
        "shirt_train = np.where(y_train==0)\n",
        "dress_train = np.where(y_train==3)\n",
        "\n",
        "shirt_test = np.where(y_test==0)\n",
        "dress_test = np.where(y_test==3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugGWzBtFJA_1"
      },
      "source": [
        "# concatenate them\n",
        "x_train_s = x_train[shirt_train]\n",
        "y_train_s = y_train[shirt_train]\n",
        "\n",
        "x_test_s = x_test[shirt_test]\n",
        "y_test_s = y_test[shirt_test]\n",
        "\n",
        "x_train_d = x_train[dress_train]\n",
        "y_train_d = y_train[dress_train]\n",
        "\n",
        "x_test_d = x_test[dress_test]\n",
        "y_test_d = y_test[dress_test]\n",
        "\n",
        "x_train_binary = np.concatenate([x_train_s, x_train_d])\n",
        "x_test_binary = np.concatenate([x_test_s, x_test_d])\n",
        "\n",
        "y_train_binary = np.concatenate([y_train_s, np.ones_like(y_train_d)])\n",
        "y_test_binary = np.concatenate([y_test_s, np.ones_like(y_test_d)])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtVPYLzMAIL"
      },
      "source": [
        "# Neural Network (Custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW49UROyzfn-"
      },
      "source": [
        "Compute the sigmoid function:\n",
        "$$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYBih-MhImZZ"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute sigmoid function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    g : The sigmoid function.\n",
        "    \"\"\"\n",
        "    a = 1./(1+np.exp(-z))\n",
        "\n",
        "    return a"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wm-R3JFEvH"
      },
      "source": [
        "$a = ReLU(z) = max(z,0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w8lOkkExcO"
      },
      "source": [
        "def relu(z):\n",
        "    \"\"\"\n",
        "    Compute relu function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    a : The relu function.\n",
        "    \"\"\"\n",
        "    a = np.maximum(z,0)\n",
        "\n",
        "    return a"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iElu6cUL-aZ"
      },
      "source": [
        "# check relu function\n",
        "assert relu(-1) == 0\n",
        "assert relu(2) == 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v435SmJ2hSGF"
      },
      "source": [
        "# Parameter Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dql9nfxkAd"
      },
      "source": [
        "Xavier initialization: $$[-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}}]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0CIw-A92vsz"
      },
      "source": [
        "def init_params_xavier(n_in, n_out):\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check np.random.uniform\n",
        "    np.random.seed(0)\n",
        "    # init random params and multiply it with 0.1\n",
        "    w = np.random.uniform(-np.sqrt(6./(n_in+n_out)), np.sqrt(6./(n_in+n_out)), (n_in, n_out))\n",
        "    b = np.random.randn(n_out)*0.01\n",
        "    return w, b"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyflAZhrXLu_",
        "outputId": "2db1ea85-1f6b-407d-ab28-3d236ebc71e7"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_xavier(2,3)\n",
        "print(np.round(w,4) == np.array([[ 0.1069,  0.4715,  0.2251],[ 0.0983, -0.1673,  0.3196]]))\n",
        "print(np.round(b,4) == np.array([0.0095, -0.0015, -0.001]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_vu4QwaBxU"
      },
      "source": [
        "He initialization: $$np.random.randn(n_{in}, n_{out})*\\sqrt{\\frac{2}{n_{in}}}$$, not multiply using 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK-3JBi2aAxp"
      },
      "source": [
        "def init_params_he(n_in, n_out):\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check np.random.uniform\n",
        "    np.random.seed(0)\n",
        "    # init random params and multiply it with 0.1\n",
        "    w = np.random.randn(n_in, n_out)*np.sqrt(2/n_in)\n",
        "    b = np.random.randn(n_out)*np.sqrt(2/n_in)\n",
        "    return w, b"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xEjaH9Vde3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c64a74f-fa83-4044-9200-37064c18bb09"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_he(2,3)\n",
        "print(np.round(w,4) == np.array([[ 1.7641,  0.4002,  0.9787], [ 2.2409,  1.8676, -0.9773]]))\n",
        "print(np.round(b,4) == np.array([0.9501, -0.1514, -0.1032]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M6bkSI46hd"
      },
      "source": [
        "Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAJJ67d4gmD"
      },
      "source": [
        "def costFunction(y, m, a):\n",
        "    \"\"\"\n",
        "    Computes cost for linear regression.\n",
        "    X : feature vector, shape (m x n+1)\n",
        "    y : labels (i.e., dog or cat), shape (m, )\n",
        "    w : parameters for the linear regression, shape (n+1, )\n",
        "    m: data legth\n",
        "\n",
        "    returns\n",
        "    -------\n",
        "    J : value of cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    J = -1/m * np.sum(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "\n",
        "\n",
        "    return J"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKNlaT8XZuG"
      },
      "source": [
        "#forward pass\n",
        "def forward(X, w, b, activation = 'relu'):\n",
        "    z = np.dot(X,w) + b\n",
        "    if activation=='relu':\n",
        "        a = relu(z)\n",
        "    else:\n",
        "        a = sigmoid(z)\n",
        "    return a"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AivWfcIMY5ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fb37a3-a924-4b4e-f58d-00d9a2e9da41"
      },
      "source": [
        "w, b  = init_params_he(2,3)\n",
        "forward(np.asarray([[1,2],[3,4]]), w, b, 'relu')[0].shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zraP5X4zhYq"
      },
      "source": [
        "\\begin{split}ReLU'(z)= \\begin{Bmatrix}1 & z>0 \\\\\n",
        "0 & z<0 \\end{Bmatrix}\\end{split}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOTSR774duI"
      },
      "source": [
        "#relu gradient\n",
        "def reluBackward(z):\n",
        "    z[z<=0] = 0\n",
        "    z[z>0] = 1\n",
        "    return z"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJK4wk_EA4t"
      },
      "source": [
        "Calculate the derivatives: $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A^{(i)}-Y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbHnEJsZX3Eu"
      },
      "source": [
        "#backpropagation\n",
        "def backward(a, dz):\n",
        "    m = len(a)\n",
        "    dw = np.dot(a.T, dz) / m\n",
        "    db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dw, db"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbgF0t0pRDfX"
      },
      "source": [
        "$$ w_j := w_j - \\alpha dw_j $$\n",
        "$$ b := b - \\alpha db $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7IbXv7rQgyt"
      },
      "source": [
        "# update parameters for optimization\n",
        "def update(w, b, dw, db, learning_rate=0.01):\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    return w, b"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPnS1ExVDffM"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural(X, y, n_layer_1, lr = 0.01, epochs = 100):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course).\n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = init_params_xavier(n_in, n_layer_1)\n",
        "    w2, b2 = init_params_xavier(n_layer_1, n_out)\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        dz1 = np.dot((dz2),w2.T)*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "\n",
        "        if i%10==0:\n",
        "\n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1v9e_cjDeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637791f7-6371-4d61-ac92-b40c32ba0dd6"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 with cost 0.73222077432145\n",
            "train: 0.5068333333333334\n",
            "test: 0.5\n",
            "epoch 10 with cost 0.3449637483262826\n",
            "train: 0.8908333333333334\n",
            "test: 0.882\n",
            "epoch 20 with cost 0.2831690845443682\n",
            "train: 0.897\n",
            "test: 0.894\n",
            "epoch 30 with cost 0.25933951989654164\n",
            "train: 0.9028333333333334\n",
            "test: 0.9015\n",
            "epoch 40 with cost 0.2452727337640431\n",
            "train: 0.9095833333333333\n",
            "test: 0.907\n",
            "epoch 50 with cost 0.2353781215718836\n",
            "train: 0.9139166666666667\n",
            "test: 0.913\n",
            "epoch 60 with cost 0.22785751996816328\n",
            "train: 0.9176666666666666\n",
            "test: 0.917\n",
            "epoch 70 with cost 0.22213162272876213\n",
            "train: 0.9193333333333333\n",
            "test: 0.921\n",
            "epoch 80 with cost 0.2175448627414139\n",
            "train: 0.9216666666666666\n",
            "test: 0.9225\n",
            "epoch 90 with cost 0.2136466884836102\n",
            "train: 0.9239166666666667\n",
            "test: 0.9235\n",
            "epoch 100 with cost 0.21042192808580465\n",
            "train: 0.9250833333333334\n",
            "test: 0.924\n",
            "epoch 110 with cost 0.20771532431056403\n",
            "train: 0.927\n",
            "test: 0.9255\n",
            "epoch 120 with cost 0.205382087686928\n",
            "train: 0.928\n",
            "test: 0.9275\n",
            "epoch 130 with cost 0.20328904868104014\n",
            "train: 0.9290833333333334\n",
            "test: 0.928\n",
            "epoch 140 with cost 0.2013800963774929\n",
            "train: 0.93\n",
            "test: 0.9285\n",
            "epoch 150 with cost 0.19961861534254216\n",
            "train: 0.9319166666666666\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.1980201693774248\n",
            "train: 0.93275\n",
            "test: 0.9295\n",
            "epoch 170 with cost 0.19658358890152214\n",
            "train: 0.9340833333333334\n",
            "test: 0.931\n",
            "epoch 180 with cost 0.19526674275538497\n",
            "train: 0.9343333333333333\n",
            "test: 0.931\n",
            "epoch 190 with cost 0.194094280049121\n",
            "train: 0.9348333333333333\n",
            "test: 0.9315\n",
            "epoch 200 with cost 0.19305596820873594\n",
            "train: 0.9354166666666667\n",
            "test: 0.9335\n",
            "epoch 210 with cost 0.19210040498229924\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 220 with cost 0.19120432289433184\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 230 with cost 0.19038629547882224\n",
            "train: 0.9366666666666666\n",
            "test: 0.933\n",
            "epoch 240 with cost 0.18963014105840983\n",
            "train: 0.9370833333333334\n",
            "test: 0.933\n",
            "epoch 250 with cost 0.18898113489003068\n",
            "train: 0.9373333333333334\n",
            "test: 0.9325\n",
            "epoch 260 with cost 0.1883895772365391\n",
            "train: 0.93825\n",
            "test: 0.933\n",
            "epoch 270 with cost 0.187873027260566\n",
            "train: 0.9393333333333334\n",
            "test: 0.9335\n",
            "epoch 280 with cost 0.18747552213510196\n",
            "train: 0.9395833333333333\n",
            "test: 0.9335\n",
            "epoch 290 with cost 0.1871796391116818\n",
            "train: 0.9396666666666667\n",
            "test: 0.9345\n",
            "epoch 300 with cost 0.18691658572297357\n",
            "train: 0.94025\n",
            "test: 0.935\n",
            "epoch 310 with cost 0.186685439028307\n",
            "train: 0.9406666666666667\n",
            "test: 0.9345\n",
            "epoch 320 with cost 0.1864629169300269\n",
            "train: 0.9413333333333334\n",
            "test: 0.9355\n",
            "epoch 330 with cost 0.1862074862746406\n",
            "train: 0.942\n",
            "test: 0.9355\n",
            "epoch 340 with cost 0.18595601232142633\n",
            "train: 0.943\n",
            "test: 0.936\n",
            "epoch 350 with cost 0.1856838792859512\n",
            "train: 0.9429166666666666\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.18555748742851816\n",
            "train: 0.9430833333333334\n",
            "test: 0.9355\n",
            "epoch 370 with cost 0.18556939168226164\n",
            "train: 0.9435833333333333\n",
            "test: 0.9355\n",
            "epoch 380 with cost 0.18558261349135038\n",
            "train: 0.9435833333333333\n",
            "test: 0.936\n",
            "epoch 390 with cost 0.18558809649233154\n",
            "train: 0.9436666666666667\n",
            "test: 0.936\n",
            "epoch 400 with cost 0.18560110893932233\n",
            "train: 0.9438333333333333\n",
            "test: 0.9365\n",
            "epoch 410 with cost 0.18559470291092012\n",
            "train: 0.9445\n",
            "test: 0.9365\n",
            "epoch 420 with cost 0.18554222013773664\n",
            "train: 0.9449166666666666\n",
            "test: 0.9365\n",
            "epoch 430 with cost 0.1854775006794311\n",
            "train: 0.9450833333333334\n",
            "test: 0.9365\n",
            "epoch 440 with cost 0.18540716338808852\n",
            "train: 0.9449166666666666\n",
            "test: 0.9355\n",
            "epoch 450 with cost 0.18531191270498387\n",
            "train: 0.9454166666666667\n",
            "test: 0.9355\n",
            "epoch 460 with cost 0.1852470697827609\n",
            "train: 0.9453333333333334\n",
            "test: 0.935\n",
            "epoch 470 with cost 0.18528987254863657\n",
            "train: 0.9456666666666667\n",
            "test: 0.935\n",
            "epoch 480 with cost 0.18534556042763517\n",
            "train: 0.9458333333333333\n",
            "test: 0.936\n",
            "epoch 490 with cost 0.18540955844456614\n",
            "train: 0.9461666666666667\n",
            "test: 0.9355\n",
            "epoch 500 with cost 0.18544449207712987\n",
            "train: 0.9466666666666667\n",
            "test: 0.9365\n",
            "epoch 510 with cost 0.18546397792969813\n",
            "train: 0.9470833333333334\n",
            "test: 0.9365\n",
            "epoch 520 with cost 0.1854318020703331\n",
            "train: 0.9471666666666667\n",
            "test: 0.9355\n",
            "epoch 530 with cost 0.18533391388229103\n",
            "train: 0.94725\n",
            "test: 0.9355\n",
            "epoch 540 with cost 0.18520164011777926\n",
            "train: 0.9474166666666667\n",
            "test: 0.9355\n",
            "epoch 550 with cost 0.18500245360806464\n",
            "train: 0.9474166666666667\n",
            "test: 0.936\n",
            "epoch 560 with cost 0.18479215772975674\n",
            "train: 0.94775\n",
            "test: 0.9365\n",
            "epoch 570 with cost 0.18457449350813657\n",
            "train: 0.9479166666666666\n",
            "test: 0.9365\n",
            "epoch 580 with cost 0.18436324952597358\n",
            "train: 0.9481666666666667\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.18413452920873039\n",
            "train: 0.9485\n",
            "test: 0.937\n",
            "epoch 600 with cost 0.18388537526060436\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 610 with cost 0.18365383403333743\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 620 with cost 0.18347515772253606\n",
            "train: 0.9489166666666666\n",
            "test: 0.938\n",
            "epoch 630 with cost 0.18335629214785285\n",
            "train: 0.949\n",
            "test: 0.938\n",
            "epoch 640 with cost 0.18329601244757218\n",
            "train: 0.9489166666666666\n",
            "test: 0.937\n",
            "epoch 650 with cost 0.18320729819398732\n",
            "train: 0.9488333333333333\n",
            "test: 0.937\n",
            "epoch 660 with cost 0.18303959212553536\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 670 with cost 0.1828566338471163\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 680 with cost 0.18263981999875423\n",
            "train: 0.94875\n",
            "test: 0.9375\n",
            "epoch 690 with cost 0.18242524939504207\n",
            "train: 0.949\n",
            "test: 0.937\n",
            "epoch 700 with cost 0.18217889142094842\n",
            "train: 0.9493333333333334\n",
            "test: 0.937\n",
            "epoch 710 with cost 0.18187524025287466\n",
            "train: 0.9495\n",
            "test: 0.9375\n",
            "epoch 720 with cost 0.18157715496196783\n",
            "train: 0.9494166666666667\n",
            "test: 0.9375\n",
            "epoch 730 with cost 0.1812786783802568\n",
            "train: 0.9493333333333334\n",
            "test: 0.938\n",
            "epoch 740 with cost 0.1809804301190431\n",
            "train: 0.9494166666666667\n",
            "test: 0.938\n",
            "epoch 750 with cost 0.18069876969145768\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 760 with cost 0.18038430345474743\n",
            "train: 0.94975\n",
            "test: 0.938\n",
            "epoch 770 with cost 0.18006445641494634\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 780 with cost 0.17972750066760862\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 790 with cost 0.1793622912055191\n",
            "train: 0.94975\n",
            "test: 0.9375\n",
            "epoch 800 with cost 0.17897629633334394\n",
            "train: 0.9498333333333333\n",
            "test: 0.938\n",
            "epoch 810 with cost 0.17855243286532704\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 820 with cost 0.178114482759856\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 830 with cost 0.1776524838533345\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 840 with cost 0.17717400769478717\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 850 with cost 0.17667338666793175\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 860 with cost 0.1761279791456748\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 870 with cost 0.17555430983877715\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 880 with cost 0.17496176963723117\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 890 with cost 0.17439185782164876\n",
            "train: 0.9498333333333333\n",
            "test: 0.9385\n",
            "epoch 900 with cost 0.17380425833356472\n",
            "train: 0.9498333333333333\n",
            "test: 0.939\n",
            "epoch 910 with cost 0.17316233668657124\n",
            "train: 0.95\n",
            "test: 0.939\n",
            "epoch 920 with cost 0.17243172368725493\n",
            "train: 0.9503333333333334\n",
            "test: 0.939\n",
            "epoch 930 with cost 0.17162811197693179\n",
            "train: 0.9504166666666667\n",
            "test: 0.94\n",
            "epoch 940 with cost 0.17083657717350387\n",
            "train: 0.9505833333333333\n",
            "test: 0.9405\n",
            "epoch 950 with cost 0.1700729248278461\n",
            "train: 0.95075\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.16932735637589205\n",
            "train: 0.9509166666666666\n",
            "test: 0.9405\n",
            "epoch 970 with cost 0.168593144310811\n",
            "train: 0.95125\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.16785928356436833\n",
            "train: 0.9514166666666667\n",
            "test: 0.9405\n",
            "epoch 990 with cost 0.16705609907695887\n",
            "train: 0.9515\n",
            "test: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3mztup8zhYw"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used xavier\n",
        "# assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9255\n",
        "# if you used he\n",
        "assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9405"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cR9l1cxie3T"
      },
      "source": [
        "# Learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itxh7L79idt_"
      },
      "source": [
        "def lr_scheduling(lr, epoch, schedule = 'step_decay'):\n",
        "\n",
        "  if schedule=='step_decay':\n",
        "    #TO DO, every 50 epochs divide lr by 2\n",
        "    if epoch%50==0:\n",
        "      lr = lr/2\n",
        "  elif schedule=='exponential_decay':\n",
        "    #TO DO, multiply rl every epoch by exp(k), where k = 0.01\n",
        "    k = 0.01\n",
        "    lr = lr * np.exp(-k)\n",
        "  else:\n",
        "    print('No scheduler, please define a correct scheduler!')\n",
        "\n",
        "  return lr"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9To5w026l8cO"
      },
      "source": [
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'step_decay')\n",
        "assert lr==0.00625\n",
        "\n",
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'exponential_decay')\n",
        "assert np.round(lr,4)==0.0135"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyY3lT0K8KKk"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.ones((100, 20))"
      ],
      "metadata": {
        "id": "1_V0cNSbsZaF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dr = np.random.rand(a.shape[0], a.shape[1]) < 0.8"
      ],
      "metadata": {
        "id": "rivBvP17sVuw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a*dr"
      ],
      "metadata": {
        "id": "BbPKh9ootBhR",
        "outputId": "7d398534-61e7-41db-b236-760063af3b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 0., 1., 0.],\n",
              "       [1., 1., 1., ..., 1., 0., 1.],\n",
              "       [1., 1., 1., ..., 1., 1., 1.],\n",
              "       ...,\n",
              "       [1., 1., 1., ..., 1., 1., 1.],\n",
              "       [1., 1., 0., ..., 0., 0., 1.],\n",
              "       [1., 1., 0., ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gft-lLiF8PcO"
      },
      "source": [
        "def dropout_forward(a, keep_prob):\n",
        "    dr = np.random.rand(a.shape[0], a.shape[1])                   # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
        "    dr = (dr < keep_prob)                                         # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
        "    a = a*dr                                                      # Step 3: shut down some neurons of A1\n",
        "    a = a/keep_prob                                               # Step 4: the expected value of the activations becomes smaller because some\n",
        "                                                                  # neurons are zeroed out.To compensate for this drop and keep the overall scale\n",
        "                                                                  # of activations the same, you divide by keep_prob.\n",
        "    return a, dr"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRAoEAQn8tUC"
      },
      "source": [
        "def dropout_backward(da, dr, keep_prob):\n",
        "    da = da*dr              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
        "    da = da/keep_prob       # Step 2: Scale the value of neurons that haven't been shut down\n",
        "    return da"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD346J1Z_eWt"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural_dr(X, y, n_layer_1, lr = 0.01, epochs = 100, keep_prob=1.0):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course).\n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = init_params_xavier(n_in, n_layer_1)\n",
        "    w2, b2 = init_params_xavier(n_layer_1, n_out)\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a1, dr = dropout_forward(a1, keep_prob)\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        da1 = np.dot((dz2),w2.T)\n",
        "        da1 = dropout_backward(da1,dr,keep_prob)\n",
        "        dz1 = da1*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "\n",
        "        if i%10==0:\n",
        "\n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZPqVszmBG08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a21b57-bd3e-4e8c-a455-1b9ff67cc22b"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural_dr(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000, 0.8)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 with cost 0.7400915087985281\n",
            "train: 0.5015\n",
            "test: 0.5\n",
            "epoch 10 with cost 0.3479958667942149\n",
            "train: 0.88675\n",
            "test: 0.883\n",
            "epoch 20 with cost 0.28823039171222914\n",
            "train: 0.8973333333333333\n",
            "test: 0.8935\n",
            "epoch 30 with cost 0.26628643339401936\n",
            "train: 0.9026666666666666\n",
            "test: 0.9035\n",
            "epoch 40 with cost 0.25055773375838103\n",
            "train: 0.9058333333333334\n",
            "test: 0.9065\n",
            "epoch 50 with cost 0.24166767823940236\n",
            "train: 0.9090833333333334\n",
            "test: 0.913\n",
            "epoch 60 with cost 0.23348936992067992\n",
            "train: 0.9135\n",
            "test: 0.9165\n",
            "epoch 70 with cost 0.22803327412241614\n",
            "train: 0.9165\n",
            "test: 0.92\n",
            "epoch 80 with cost 0.2225962181028209\n",
            "train: 0.9174166666666667\n",
            "test: 0.9225\n",
            "epoch 90 with cost 0.21845386209811007\n",
            "train: 0.92\n",
            "test: 0.923\n",
            "epoch 100 with cost 0.21576787803067743\n",
            "train: 0.9215833333333333\n",
            "test: 0.9245\n",
            "epoch 110 with cost 0.21476061539802085\n",
            "train: 0.9211666666666667\n",
            "test: 0.9255\n",
            "epoch 120 with cost 0.2098592280641482\n",
            "train: 0.9239166666666667\n",
            "test: 0.9265\n",
            "epoch 130 with cost 0.2079316302105814\n",
            "train: 0.9269166666666667\n",
            "test: 0.928\n",
            "epoch 140 with cost 0.20418501128268715\n",
            "train: 0.9290833333333334\n",
            "test: 0.928\n",
            "epoch 150 with cost 0.20364047869641225\n",
            "train: 0.928\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.2039193155856031\n",
            "train: 0.9260833333333334\n",
            "test: 0.929\n",
            "epoch 170 with cost 0.20079871888781914\n",
            "train: 0.9313333333333333\n",
            "test: 0.931\n",
            "epoch 180 with cost 0.20047685375853272\n",
            "train: 0.9305833333333333\n",
            "test: 0.931\n",
            "epoch 190 with cost 0.1971891857897493\n",
            "train: 0.9319166666666666\n",
            "test: 0.931\n",
            "epoch 200 with cost 0.1960601575825789\n",
            "train: 0.9331666666666667\n",
            "test: 0.9315\n",
            "epoch 210 with cost 0.19409770949770613\n",
            "train: 0.9323333333333333\n",
            "test: 0.9315\n",
            "epoch 220 with cost 0.19330190049448004\n",
            "train: 0.93175\n",
            "test: 0.9335\n",
            "epoch 230 with cost 0.1928395457562383\n",
            "train: 0.93325\n",
            "test: 0.9325\n",
            "epoch 240 with cost 0.1914635123173751\n",
            "train: 0.9360833333333334\n",
            "test: 0.9325\n",
            "epoch 250 with cost 0.1914981055108382\n",
            "train: 0.9368333333333333\n",
            "test: 0.9325\n",
            "epoch 260 with cost 0.19183267750433752\n",
            "train: 0.9341666666666667\n",
            "test: 0.9325\n",
            "epoch 270 with cost 0.18793434697465855\n",
            "train: 0.93875\n",
            "test: 0.933\n",
            "epoch 280 with cost 0.18872658786143082\n",
            "train: 0.937\n",
            "test: 0.9335\n",
            "epoch 290 with cost 0.18753095962872088\n",
            "train: 0.938\n",
            "test: 0.9345\n",
            "epoch 300 with cost 0.18721509989356372\n",
            "train: 0.9391666666666667\n",
            "test: 0.935\n",
            "epoch 310 with cost 0.18698926655900117\n",
            "train: 0.9374166666666667\n",
            "test: 0.9345\n",
            "epoch 320 with cost 0.18727127657459863\n",
            "train: 0.9384166666666667\n",
            "test: 0.934\n",
            "epoch 330 with cost 0.18728192228939444\n",
            "train: 0.9391666666666667\n",
            "test: 0.9345\n",
            "epoch 340 with cost 0.18555743275638525\n",
            "train: 0.9400833333333334\n",
            "test: 0.9355\n",
            "epoch 350 with cost 0.1854591056853025\n",
            "train: 0.9400833333333334\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.18372648427394694\n",
            "train: 0.94325\n",
            "test: 0.9355\n",
            "epoch 370 with cost 0.18544836842599852\n",
            "train: 0.9396666666666667\n",
            "test: 0.9355\n",
            "epoch 380 with cost 0.18519979865596026\n",
            "train: 0.9424166666666667\n",
            "test: 0.936\n",
            "epoch 390 with cost 0.1848634641985681\n",
            "train: 0.94125\n",
            "test: 0.935\n",
            "epoch 400 with cost 0.18491676518289973\n",
            "train: 0.9421666666666667\n",
            "test: 0.9345\n",
            "epoch 410 with cost 0.1840048668786882\n",
            "train: 0.9425\n",
            "test: 0.935\n",
            "epoch 420 with cost 0.1834255291782021\n",
            "train: 0.9424166666666667\n",
            "test: 0.936\n",
            "epoch 430 with cost 0.1828084948508718\n",
            "train: 0.9435833333333333\n",
            "test: 0.936\n",
            "epoch 440 with cost 0.1834245095283503\n",
            "train: 0.9429166666666666\n",
            "test: 0.9365\n",
            "epoch 450 with cost 0.18285938175339805\n",
            "train: 0.9425833333333333\n",
            "test: 0.936\n",
            "epoch 460 with cost 0.18338588555377477\n",
            "train: 0.9446666666666667\n",
            "test: 0.9375\n",
            "epoch 470 with cost 0.18343810203323574\n",
            "train: 0.9455\n",
            "test: 0.9375\n",
            "epoch 480 with cost 0.18323952589309234\n",
            "train: 0.945\n",
            "test: 0.9375\n",
            "epoch 490 with cost 0.1834277469233904\n",
            "train: 0.9465833333333333\n",
            "test: 0.9375\n",
            "epoch 500 with cost 0.18315556949087036\n",
            "train: 0.9466666666666667\n",
            "test: 0.9375\n",
            "epoch 510 with cost 0.18324381989111224\n",
            "train: 0.9439166666666666\n",
            "test: 0.937\n",
            "epoch 520 with cost 0.1834747939051392\n",
            "train: 0.9448333333333333\n",
            "test: 0.9365\n",
            "epoch 530 with cost 0.18307581250332244\n",
            "train: 0.9460833333333334\n",
            "test: 0.9365\n",
            "epoch 540 with cost 0.1828712246200538\n",
            "train: 0.9478333333333333\n",
            "test: 0.937\n",
            "epoch 550 with cost 0.18482537824714249\n",
            "train: 0.9480833333333333\n",
            "test: 0.9375\n",
            "epoch 560 with cost 0.18374448541358732\n",
            "train: 0.9475\n",
            "test: 0.937\n",
            "epoch 570 with cost 0.18360233433098103\n",
            "train: 0.947\n",
            "test: 0.936\n",
            "epoch 580 with cost 0.1842666088799611\n",
            "train: 0.9465\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.184182369811633\n",
            "train: 0.947\n",
            "test: 0.936\n",
            "epoch 600 with cost 0.18483407382126119\n",
            "train: 0.9464166666666667\n",
            "test: 0.936\n",
            "epoch 610 with cost 0.18266241865162464\n",
            "train: 0.94725\n",
            "test: 0.9365\n",
            "epoch 620 with cost 0.18312324081710485\n",
            "train: 0.9485833333333333\n",
            "test: 0.9365\n",
            "epoch 630 with cost 0.1818616459513417\n",
            "train: 0.94925\n",
            "test: 0.9375\n",
            "epoch 640 with cost 0.18247453663441013\n",
            "train: 0.9484166666666667\n",
            "test: 0.9375\n",
            "epoch 650 with cost 0.1825617333653253\n",
            "train: 0.9485\n",
            "test: 0.9375\n",
            "epoch 660 with cost 0.18244548168825012\n",
            "train: 0.94775\n",
            "test: 0.938\n",
            "epoch 670 with cost 0.18268380917297905\n",
            "train: 0.9478333333333333\n",
            "test: 0.938\n",
            "epoch 680 with cost 0.18289735781984492\n",
            "train: 0.94775\n",
            "test: 0.938\n",
            "epoch 690 with cost 0.1823929361447015\n",
            "train: 0.9481666666666667\n",
            "test: 0.938\n",
            "epoch 700 with cost 0.18181678108838234\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 710 with cost 0.18168409224313475\n",
            "train: 0.9481666666666667\n",
            "test: 0.9375\n",
            "epoch 720 with cost 0.18224436069550215\n",
            "train: 0.9485\n",
            "test: 0.937\n",
            "epoch 730 with cost 0.18309403021443502\n",
            "train: 0.9488333333333333\n",
            "test: 0.938\n",
            "epoch 740 with cost 0.1823026758602279\n",
            "train: 0.9479166666666666\n",
            "test: 0.9375\n",
            "epoch 750 with cost 0.1816001603129512\n",
            "train: 0.9495833333333333\n",
            "test: 0.937\n",
            "epoch 760 with cost 0.18143283942262833\n",
            "train: 0.9485\n",
            "test: 0.9385\n",
            "epoch 770 with cost 0.18082240809013167\n",
            "train: 0.9493333333333334\n",
            "test: 0.938\n",
            "epoch 780 with cost 0.18122135171558257\n",
            "train: 0.94875\n",
            "test: 0.938\n",
            "epoch 790 with cost 0.18042013981277022\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 800 with cost 0.17899789908542285\n",
            "train: 0.9490833333333333\n",
            "test: 0.938\n",
            "epoch 810 with cost 0.17973305163459227\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 820 with cost 0.18019402075091193\n",
            "train: 0.9483333333333334\n",
            "test: 0.938\n",
            "epoch 830 with cost 0.17877943483652942\n",
            "train: 0.9491666666666667\n",
            "test: 0.938\n",
            "epoch 840 with cost 0.1784370454514955\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 850 with cost 0.1776931972398369\n",
            "train: 0.9483333333333334\n",
            "test: 0.938\n",
            "epoch 860 with cost 0.17865317412636522\n",
            "train: 0.9485\n",
            "test: 0.9385\n",
            "epoch 870 with cost 0.1778840660302094\n",
            "train: 0.9489166666666666\n",
            "test: 0.9385\n",
            "epoch 880 with cost 0.17703309632713018\n",
            "train: 0.949\n",
            "test: 0.939\n",
            "epoch 890 with cost 0.17761662004256495\n",
            "train: 0.94925\n",
            "test: 0.939\n",
            "epoch 900 with cost 0.17689338563964305\n",
            "train: 0.9484166666666667\n",
            "test: 0.939\n",
            "epoch 910 with cost 0.17553763832962832\n",
            "train: 0.9486666666666667\n",
            "test: 0.939\n",
            "epoch 920 with cost 0.17519395172989569\n",
            "train: 0.9495\n",
            "test: 0.9395\n",
            "epoch 930 with cost 0.17473615742571783\n",
            "train: 0.9501666666666667\n",
            "test: 0.94\n",
            "epoch 940 with cost 0.17391375068344336\n",
            "train: 0.9505\n",
            "test: 0.94\n",
            "epoch 950 with cost 0.17309154455653497\n",
            "train: 0.9494166666666667\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.17402669731953493\n",
            "train: 0.9490833333333333\n",
            "test: 0.9405\n",
            "epoch 970 with cost 0.1723228221110136\n",
            "train: 0.9500833333333333\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.17105886298463965\n",
            "train: 0.9515833333333333\n",
            "test: 0.9405\n",
            "epoch 990 with cost 0.16969248218988062\n",
            "train: 0.9503333333333334\n",
            "test: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvWE3BNBN8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d772e1-abf8-4484-f6a7-76251e5683f0"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used xavier\n",
        "np.mean(np.round(a2t.reshape(-1))==y_test_binary)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9405)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16PSfYXDoqui"
      },
      "source": [
        "# Neural Network Binary (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAQd0oGotCA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ghY0rEo6CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53f7f38-8e5b-4236-dbbc-7b0be7855535"
      },
      "source": [
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_1 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 3s 6ms/step - loss: 0.4599 - accuracy: 0.8262 - val_loss: 0.3280 - val_accuracy: 0.8900\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2818 - accuracy: 0.9045 - val_loss: 0.2685 - val_accuracy: 0.8975\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2425 - accuracy: 0.9092 - val_loss: 0.2431 - val_accuracy: 0.9050\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9136 - val_loss: 0.2280 - val_accuracy: 0.9070\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2102 - accuracy: 0.9185 - val_loss: 0.2165 - val_accuracy: 0.9155\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9215 - val_loss: 0.2105 - val_accuracy: 0.9175\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1942 - accuracy: 0.9230 - val_loss: 0.2029 - val_accuracy: 0.9215\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1888 - accuracy: 0.9264 - val_loss: 0.1980 - val_accuracy: 0.9240\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.9268 - val_loss: 0.1942 - val_accuracy: 0.9275\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9286 - val_loss: 0.1918 - val_accuracy: 0.9260\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9295 - val_loss: 0.1884 - val_accuracy: 0.9290\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1753 - accuracy: 0.9309 - val_loss: 0.1868 - val_accuracy: 0.9305\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9323 - val_loss: 0.1844 - val_accuracy: 0.9300\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9325 - val_loss: 0.1827 - val_accuracy: 0.9310\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9344 - val_loss: 0.1825 - val_accuracy: 0.9300\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1671 - accuracy: 0.9353 - val_loss: 0.1793 - val_accuracy: 0.9320\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1656 - accuracy: 0.9354 - val_loss: 0.1787 - val_accuracy: 0.9345\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9360 - val_loss: 0.1769 - val_accuracy: 0.9335\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1630 - accuracy: 0.9366 - val_loss: 0.1759 - val_accuracy: 0.9335\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9385 - val_loss: 0.1750 - val_accuracy: 0.9345\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9375 - val_loss: 0.1740 - val_accuracy: 0.9345\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9383 - val_loss: 0.1732 - val_accuracy: 0.9345\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1582 - accuracy: 0.9388 - val_loss: 0.1727 - val_accuracy: 0.9350\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1575 - accuracy: 0.9392 - val_loss: 0.1716 - val_accuracy: 0.9355\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1566 - accuracy: 0.9401 - val_loss: 0.1717 - val_accuracy: 0.9340\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9404 - val_loss: 0.1702 - val_accuracy: 0.9370\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1544 - accuracy: 0.9409 - val_loss: 0.1698 - val_accuracy: 0.9345\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9408 - val_loss: 0.1692 - val_accuracy: 0.9350\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1531 - accuracy: 0.9413 - val_loss: 0.1685 - val_accuracy: 0.9380\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9410 - val_loss: 0.1681 - val_accuracy: 0.9360\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9417 - val_loss: 0.1675 - val_accuracy: 0.9370\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9428 - val_loss: 0.1690 - val_accuracy: 0.9350\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1505 - accuracy: 0.9412 - val_loss: 0.1663 - val_accuracy: 0.9370\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9430 - val_loss: 0.1667 - val_accuracy: 0.9365\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9427 - val_loss: 0.1655 - val_accuracy: 0.9380\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.9426 - val_loss: 0.1654 - val_accuracy: 0.9375\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9430 - val_loss: 0.1650 - val_accuracy: 0.9385\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9426 - val_loss: 0.1653 - val_accuracy: 0.9390\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9443 - val_loss: 0.1636 - val_accuracy: 0.9395\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9440 - val_loss: 0.1652 - val_accuracy: 0.9380\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.9446 - val_loss: 0.1630 - val_accuracy: 0.9410\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9452 - val_loss: 0.1624 - val_accuracy: 0.9380\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9442 - val_loss: 0.1620 - val_accuracy: 0.9410\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9456 - val_loss: 0.1618 - val_accuracy: 0.9410\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9452 - val_loss: 0.1616 - val_accuracy: 0.9405\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1424 - accuracy: 0.9464 - val_loss: 0.1642 - val_accuracy: 0.9365\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9457 - val_loss: 0.1607 - val_accuracy: 0.9400\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9464 - val_loss: 0.1609 - val_accuracy: 0.9405\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9462 - val_loss: 0.1611 - val_accuracy: 0.9390\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9470 - val_loss: 0.1598 - val_accuracy: 0.9395\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9466 - val_loss: 0.1597 - val_accuracy: 0.9415\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9475 - val_loss: 0.1591 - val_accuracy: 0.9410\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1394 - accuracy: 0.9474 - val_loss: 0.1593 - val_accuracy: 0.9410\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9477 - val_loss: 0.1585 - val_accuracy: 0.9405\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9477 - val_loss: 0.1584 - val_accuracy: 0.9405\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1382 - accuracy: 0.9485 - val_loss: 0.1582 - val_accuracy: 0.9405\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1376 - accuracy: 0.9484 - val_loss: 0.1603 - val_accuracy: 0.9420\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9480 - val_loss: 0.1592 - val_accuracy: 0.9405\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9479 - val_loss: 0.1577 - val_accuracy: 0.9415\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1365 - accuracy: 0.9488 - val_loss: 0.1573 - val_accuracy: 0.9415\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9492 - val_loss: 0.1571 - val_accuracy: 0.9425\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1354 - accuracy: 0.9493 - val_loss: 0.1588 - val_accuracy: 0.9400\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1355 - accuracy: 0.9493 - val_loss: 0.1566 - val_accuracy: 0.9420\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9497 - val_loss: 0.1564 - val_accuracy: 0.9415\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9493 - val_loss: 0.1563 - val_accuracy: 0.9435\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9504 - val_loss: 0.1567 - val_accuracy: 0.9435\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9508 - val_loss: 0.1561 - val_accuracy: 0.9425\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9500 - val_loss: 0.1554 - val_accuracy: 0.9430\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9501 - val_loss: 0.1560 - val_accuracy: 0.9420\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9500 - val_loss: 0.1549 - val_accuracy: 0.9455\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9513 - val_loss: 0.1547 - val_accuracy: 0.9435\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9507 - val_loss: 0.1546 - val_accuracy: 0.9455\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9508 - val_loss: 0.1568 - val_accuracy: 0.9450\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9498 - val_loss: 0.1544 - val_accuracy: 0.9455\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9512 - val_loss: 0.1538 - val_accuracy: 0.9440\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.9516 - val_loss: 0.1542 - val_accuracy: 0.9440\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9515 - val_loss: 0.1538 - val_accuracy: 0.9450\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9518 - val_loss: 0.1535 - val_accuracy: 0.9460\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1295 - accuracy: 0.9524 - val_loss: 0.1538 - val_accuracy: 0.9465\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9523 - val_loss: 0.1534 - val_accuracy: 0.9445\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9520 - val_loss: 0.1527 - val_accuracy: 0.9455\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9528 - val_loss: 0.1525 - val_accuracy: 0.9465\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9527 - val_loss: 0.1525 - val_accuracy: 0.9460\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9530 - val_loss: 0.1529 - val_accuracy: 0.9470\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1274 - accuracy: 0.9530 - val_loss: 0.1531 - val_accuracy: 0.9455\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1273 - accuracy: 0.9535 - val_loss: 0.1526 - val_accuracy: 0.9465\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9532 - val_loss: 0.1520 - val_accuracy: 0.9460\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1266 - accuracy: 0.9548 - val_loss: 0.1516 - val_accuracy: 0.9470\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9533 - val_loss: 0.1533 - val_accuracy: 0.9455\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9538 - val_loss: 0.1514 - val_accuracy: 0.9475\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9536 - val_loss: 0.1514 - val_accuracy: 0.9475\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1251 - accuracy: 0.9541 - val_loss: 0.1523 - val_accuracy: 0.9450\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1252 - accuracy: 0.9536 - val_loss: 0.1503 - val_accuracy: 0.9475\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9550 - val_loss: 0.1506 - val_accuracy: 0.9470\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9541 - val_loss: 0.1506 - val_accuracy: 0.9475\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9542 - val_loss: 0.1498 - val_accuracy: 0.9490\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9554 - val_loss: 0.1502 - val_accuracy: 0.9475\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9557 - val_loss: 0.1522 - val_accuracy: 0.9465\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9556 - val_loss: 0.1492 - val_accuracy: 0.9490\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1227 - accuracy: 0.9556 - val_loss: 0.1493 - val_accuracy: 0.9475\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9553 - val_loss: 0.1493 - val_accuracy: 0.9485\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9559 - val_loss: 0.1490 - val_accuracy: 0.9475\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 0.9550 - val_loss: 0.1488 - val_accuracy: 0.9475\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9561 - val_loss: 0.1489 - val_accuracy: 0.9485\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9564 - val_loss: 0.1483 - val_accuracy: 0.9495\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9569 - val_loss: 0.1482 - val_accuracy: 0.9485\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9557 - val_loss: 0.1480 - val_accuracy: 0.9485\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9571 - val_loss: 0.1495 - val_accuracy: 0.9460\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9570 - val_loss: 0.1482 - val_accuracy: 0.9480\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9574 - val_loss: 0.1481 - val_accuracy: 0.9475\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9572 - val_loss: 0.1472 - val_accuracy: 0.9500\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9569 - val_loss: 0.1472 - val_accuracy: 0.9500\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9570 - val_loss: 0.1470 - val_accuracy: 0.9500\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9585 - val_loss: 0.1482 - val_accuracy: 0.9480\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9568 - val_loss: 0.1483 - val_accuracy: 0.9475\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9576 - val_loss: 0.1469 - val_accuracy: 0.9495\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1176 - accuracy: 0.9587 - val_loss: 0.1464 - val_accuracy: 0.9500\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9582 - val_loss: 0.1463 - val_accuracy: 0.9500\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9590 - val_loss: 0.1493 - val_accuracy: 0.9475\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1170 - accuracy: 0.9578 - val_loss: 0.1460 - val_accuracy: 0.9505\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9582 - val_loss: 0.1456 - val_accuracy: 0.9500\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9593 - val_loss: 0.1455 - val_accuracy: 0.9500\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1158 - accuracy: 0.9580 - val_loss: 0.1457 - val_accuracy: 0.9510\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1156 - accuracy: 0.9590 - val_loss: 0.1451 - val_accuracy: 0.9525\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1150 - accuracy: 0.9584 - val_loss: 0.1461 - val_accuracy: 0.9495\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.9585 - val_loss: 0.1453 - val_accuracy: 0.9505\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9598 - val_loss: 0.1447 - val_accuracy: 0.9530\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9598 - val_loss: 0.1449 - val_accuracy: 0.9510\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9597 - val_loss: 0.1446 - val_accuracy: 0.9505\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9600 - val_loss: 0.1442 - val_accuracy: 0.9515\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9597 - val_loss: 0.1445 - val_accuracy: 0.9510\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9600 - val_loss: 0.1441 - val_accuracy: 0.9525\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.9599 - val_loss: 0.1440 - val_accuracy: 0.9515\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1124 - accuracy: 0.9598 - val_loss: 0.1443 - val_accuracy: 0.9510\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1124 - accuracy: 0.9605 - val_loss: 0.1441 - val_accuracy: 0.9515\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 0.9596 - val_loss: 0.1446 - val_accuracy: 0.9510\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1116 - accuracy: 0.9607 - val_loss: 0.1429 - val_accuracy: 0.9525\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.9617 - val_loss: 0.1437 - val_accuracy: 0.9510\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9603 - val_loss: 0.1425 - val_accuracy: 0.9535\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9612 - val_loss: 0.1429 - val_accuracy: 0.9530\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.1440 - val_accuracy: 0.9485\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9603 - val_loss: 0.1419 - val_accuracy: 0.9535\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9607 - val_loss: 0.1426 - val_accuracy: 0.9510\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9616 - val_loss: 0.1417 - val_accuracy: 0.9535\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9607 - val_loss: 0.1418 - val_accuracy: 0.9540\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9616 - val_loss: 0.1421 - val_accuracy: 0.9515\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9622 - val_loss: 0.1414 - val_accuracy: 0.9535\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9617 - val_loss: 0.1438 - val_accuracy: 0.9485\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9625 - val_loss: 0.1415 - val_accuracy: 0.9525\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9622 - val_loss: 0.1407 - val_accuracy: 0.9545\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9629 - val_loss: 0.1410 - val_accuracy: 0.9540\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1073 - accuracy: 0.9624 - val_loss: 0.1437 - val_accuracy: 0.9480\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1069 - accuracy: 0.9632 - val_loss: 0.1469 - val_accuracy: 0.9480\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1070 - accuracy: 0.9620 - val_loss: 0.1430 - val_accuracy: 0.9485\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9622 - val_loss: 0.1402 - val_accuracy: 0.9535\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1062 - accuracy: 0.9624 - val_loss: 0.1399 - val_accuracy: 0.9530\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1058 - accuracy: 0.9632 - val_loss: 0.1401 - val_accuracy: 0.9525\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9636 - val_loss: 0.1421 - val_accuracy: 0.9500\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9639 - val_loss: 0.1394 - val_accuracy: 0.9545\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.9635 - val_loss: 0.1393 - val_accuracy: 0.9545\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9644 - val_loss: 0.1405 - val_accuracy: 0.9510\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9638 - val_loss: 0.1385 - val_accuracy: 0.9540\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.9643 - val_loss: 0.1384 - val_accuracy: 0.9535\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9636 - val_loss: 0.1383 - val_accuracy: 0.9540\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9638 - val_loss: 0.1382 - val_accuracy: 0.9540\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9644 - val_loss: 0.1380 - val_accuracy: 0.9535\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9640 - val_loss: 0.1388 - val_accuracy: 0.9530\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.9645 - val_loss: 0.1395 - val_accuracy: 0.9520\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9649 - val_loss: 0.1389 - val_accuracy: 0.9530\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9646 - val_loss: 0.1389 - val_accuracy: 0.9545\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9647 - val_loss: 0.1376 - val_accuracy: 0.9550\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9647 - val_loss: 0.1369 - val_accuracy: 0.9540\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9651 - val_loss: 0.1372 - val_accuracy: 0.9555\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1011 - accuracy: 0.9659 - val_loss: 0.1369 - val_accuracy: 0.9540\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9657 - val_loss: 0.1373 - val_accuracy: 0.9545\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9650 - val_loss: 0.1365 - val_accuracy: 0.9545\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.9663 - val_loss: 0.1368 - val_accuracy: 0.9530\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.9650 - val_loss: 0.1364 - val_accuracy: 0.9545\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9657 - val_loss: 0.1361 - val_accuracy: 0.9545\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9658 - val_loss: 0.1361 - val_accuracy: 0.9540\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0991 - accuracy: 0.9662 - val_loss: 0.1357 - val_accuracy: 0.9545\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9658 - val_loss: 0.1355 - val_accuracy: 0.9550\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0987 - accuracy: 0.9662 - val_loss: 0.1354 - val_accuracy: 0.9545\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - accuracy: 0.9666 - val_loss: 0.1359 - val_accuracy: 0.9555\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0980 - accuracy: 0.9668 - val_loss: 0.1406 - val_accuracy: 0.9475\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9667 - val_loss: 0.1350 - val_accuracy: 0.9545\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9670 - val_loss: 0.1351 - val_accuracy: 0.9540\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9672 - val_loss: 0.1347 - val_accuracy: 0.9535\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0971 - accuracy: 0.9667 - val_loss: 0.1360 - val_accuracy: 0.9565\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9672 - val_loss: 0.1354 - val_accuracy: 0.9560\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0967 - accuracy: 0.9669 - val_loss: 0.1361 - val_accuracy: 0.9540\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9668 - val_loss: 0.1359 - val_accuracy: 0.9535\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9671 - val_loss: 0.1337 - val_accuracy: 0.9550\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9678 - val_loss: 0.1351 - val_accuracy: 0.9545\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9677 - val_loss: 0.1345 - val_accuracy: 0.9560\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9679 - val_loss: 0.1338 - val_accuracy: 0.9535\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9679 - val_loss: 0.1330 - val_accuracy: 0.9540\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9678 - val_loss: 0.1336 - val_accuracy: 0.9545\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9676 - val_loss: 0.1330 - val_accuracy: 0.9545\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9672 - val_loss: 0.1328 - val_accuracy: 0.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTbTkk3Fpu1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f1e77c-8e0e-406d-db9b-4cc46b76a67c"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_2 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 2s 6ms/step - loss: 0.4479 - accuracy: 0.8279 - val_loss: 0.3333 - val_accuracy: 0.8825\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2967 - accuracy: 0.8942 - val_loss: 0.2761 - val_accuracy: 0.8925\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.2561 - accuracy: 0.9018 - val_loss: 0.2506 - val_accuracy: 0.9030\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.2367 - accuracy: 0.9086 - val_loss: 0.2351 - val_accuracy: 0.9030\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.2251 - accuracy: 0.9105 - val_loss: 0.2241 - val_accuracy: 0.9120\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9163 - val_loss: 0.2186 - val_accuracy: 0.9120\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2078 - accuracy: 0.9168 - val_loss: 0.2103 - val_accuracy: 0.9205\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9194 - val_loss: 0.2054 - val_accuracy: 0.9255\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1975 - accuracy: 0.9213 - val_loss: 0.2021 - val_accuracy: 0.9230\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1915 - accuracy: 0.9243 - val_loss: 0.1990 - val_accuracy: 0.9245\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9222 - val_loss: 0.1958 - val_accuracy: 0.9265\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9262 - val_loss: 0.1941 - val_accuracy: 0.9270\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9281 - val_loss: 0.1922 - val_accuracy: 0.9260\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9277 - val_loss: 0.1898 - val_accuracy: 0.9290\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1812 - accuracy: 0.9293 - val_loss: 0.1889 - val_accuracy: 0.9270\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9271 - val_loss: 0.1862 - val_accuracy: 0.9295\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9318 - val_loss: 0.1852 - val_accuracy: 0.9315\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1740 - accuracy: 0.9323 - val_loss: 0.1834 - val_accuracy: 0.9305\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9326 - val_loss: 0.1822 - val_accuracy: 0.9315\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9353 - val_loss: 0.1810 - val_accuracy: 0.9310\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1721 - accuracy: 0.9348 - val_loss: 0.1798 - val_accuracy: 0.9315\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9345 - val_loss: 0.1789 - val_accuracy: 0.9335\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1671 - accuracy: 0.9350 - val_loss: 0.1780 - val_accuracy: 0.9320\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1664 - accuracy: 0.9350 - val_loss: 0.1768 - val_accuracy: 0.9335\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9362 - val_loss: 0.1759 - val_accuracy: 0.9330\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9345 - val_loss: 0.1750 - val_accuracy: 0.9340\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9377 - val_loss: 0.1740 - val_accuracy: 0.9365\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1620 - accuracy: 0.9373 - val_loss: 0.1733 - val_accuracy: 0.9335\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1620 - accuracy: 0.9382 - val_loss: 0.1727 - val_accuracy: 0.9360\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9397 - val_loss: 0.1719 - val_accuracy: 0.9365\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9408 - val_loss: 0.1712 - val_accuracy: 0.9340\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9393 - val_loss: 0.1720 - val_accuracy: 0.9350\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1564 - accuracy: 0.9403 - val_loss: 0.1696 - val_accuracy: 0.9355\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1559 - accuracy: 0.9390 - val_loss: 0.1694 - val_accuracy: 0.9345\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1558 - accuracy: 0.9401 - val_loss: 0.1685 - val_accuracy: 0.9365\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1544 - accuracy: 0.9392 - val_loss: 0.1682 - val_accuracy: 0.9355\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9402 - val_loss: 0.1674 - val_accuracy: 0.9355\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9419 - val_loss: 0.1677 - val_accuracy: 0.9360\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9413 - val_loss: 0.1660 - val_accuracy: 0.9375\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9427 - val_loss: 0.1666 - val_accuracy: 0.9355\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.9435 - val_loss: 0.1649 - val_accuracy: 0.9385\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9437 - val_loss: 0.1642 - val_accuracy: 0.9375\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.9434 - val_loss: 0.1635 - val_accuracy: 0.9395\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9438 - val_loss: 0.1632 - val_accuracy: 0.9390\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9440 - val_loss: 0.1628 - val_accuracy: 0.9390\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9423 - val_loss: 0.1646 - val_accuracy: 0.9375\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1466 - accuracy: 0.9442 - val_loss: 0.1615 - val_accuracy: 0.9400\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.9428 - val_loss: 0.1616 - val_accuracy: 0.9390\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.9438 - val_loss: 0.1608 - val_accuracy: 0.9395\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9447 - val_loss: 0.1602 - val_accuracy: 0.9400\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9464 - val_loss: 0.1602 - val_accuracy: 0.9410\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9455 - val_loss: 0.1590 - val_accuracy: 0.9410\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9441 - val_loss: 0.1588 - val_accuracy: 0.9400\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9455 - val_loss: 0.1580 - val_accuracy: 0.9410\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9482 - val_loss: 0.1579 - val_accuracy: 0.9395\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9459 - val_loss: 0.1573 - val_accuracy: 0.9410\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9480 - val_loss: 0.1579 - val_accuracy: 0.9420\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9471 - val_loss: 0.1578 - val_accuracy: 0.9400\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9487 - val_loss: 0.1563 - val_accuracy: 0.9415\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.9482 - val_loss: 0.1558 - val_accuracy: 0.9400\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9488 - val_loss: 0.1555 - val_accuracy: 0.9410\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9488 - val_loss: 0.1561 - val_accuracy: 0.9405\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1357 - accuracy: 0.9488 - val_loss: 0.1546 - val_accuracy: 0.9405\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1366 - accuracy: 0.9484 - val_loss: 0.1540 - val_accuracy: 0.9410\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1354 - accuracy: 0.9479 - val_loss: 0.1536 - val_accuracy: 0.9405\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1347 - accuracy: 0.9507 - val_loss: 0.1538 - val_accuracy: 0.9430\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1334 - accuracy: 0.9503 - val_loss: 0.1532 - val_accuracy: 0.9415\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1320 - accuracy: 0.9514 - val_loss: 0.1522 - val_accuracy: 0.9415\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9513 - val_loss: 0.1521 - val_accuracy: 0.9420\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9506 - val_loss: 0.1514 - val_accuracy: 0.9420\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9506 - val_loss: 0.1513 - val_accuracy: 0.9415\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9515 - val_loss: 0.1513 - val_accuracy: 0.9440\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9517 - val_loss: 0.1528 - val_accuracy: 0.9470\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9504 - val_loss: 0.1501 - val_accuracy: 0.9425\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9533 - val_loss: 0.1498 - val_accuracy: 0.9420\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9513 - val_loss: 0.1502 - val_accuracy: 0.9445\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9519 - val_loss: 0.1495 - val_accuracy: 0.9430\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1295 - accuracy: 0.9523 - val_loss: 0.1486 - val_accuracy: 0.9440\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1287 - accuracy: 0.9526 - val_loss: 0.1484 - val_accuracy: 0.9455\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9524 - val_loss: 0.1484 - val_accuracy: 0.9455\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9521 - val_loss: 0.1476 - val_accuracy: 0.9440\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9524 - val_loss: 0.1474 - val_accuracy: 0.9445\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9531 - val_loss: 0.1471 - val_accuracy: 0.9435\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9520 - val_loss: 0.1472 - val_accuracy: 0.9465\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.9554 - val_loss: 0.1473 - val_accuracy: 0.9450\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9533 - val_loss: 0.1464 - val_accuracy: 0.9475\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9550 - val_loss: 0.1461 - val_accuracy: 0.9455\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1236 - accuracy: 0.9554 - val_loss: 0.1455 - val_accuracy: 0.9465\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9560 - val_loss: 0.1468 - val_accuracy: 0.9500\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9556 - val_loss: 0.1449 - val_accuracy: 0.9465\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1236 - accuracy: 0.9539 - val_loss: 0.1448 - val_accuracy: 0.9485\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9534 - val_loss: 0.1452 - val_accuracy: 0.9485\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9553 - val_loss: 0.1435 - val_accuracy: 0.9475\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1214 - accuracy: 0.9566 - val_loss: 0.1436 - val_accuracy: 0.9480\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.9558 - val_loss: 0.1434 - val_accuracy: 0.9485\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1205 - accuracy: 0.9557 - val_loss: 0.1428 - val_accuracy: 0.9480\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1198 - accuracy: 0.9566 - val_loss: 0.1427 - val_accuracy: 0.9480\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1207 - accuracy: 0.9561 - val_loss: 0.1434 - val_accuracy: 0.9475\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1195 - accuracy: 0.9572 - val_loss: 0.1417 - val_accuracy: 0.9490\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1189 - accuracy: 0.9563 - val_loss: 0.1423 - val_accuracy: 0.9515\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9563 - val_loss: 0.1415 - val_accuracy: 0.9490\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9591 - val_loss: 0.1413 - val_accuracy: 0.9505\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9577 - val_loss: 0.1406 - val_accuracy: 0.9500\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9584 - val_loss: 0.1409 - val_accuracy: 0.9490\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9569 - val_loss: 0.1401 - val_accuracy: 0.9510\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9591 - val_loss: 0.1400 - val_accuracy: 0.9515\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1173 - accuracy: 0.9571 - val_loss: 0.1395 - val_accuracy: 0.9530\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1146 - accuracy: 0.9588 - val_loss: 0.1400 - val_accuracy: 0.9515\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1173 - accuracy: 0.9577 - val_loss: 0.1392 - val_accuracy: 0.9520\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9582 - val_loss: 0.1392 - val_accuracy: 0.9515\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9568 - val_loss: 0.1383 - val_accuracy: 0.9530\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9605 - val_loss: 0.1387 - val_accuracy: 0.9540\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.9592 - val_loss: 0.1378 - val_accuracy: 0.9545\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9587 - val_loss: 0.1383 - val_accuracy: 0.9530\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9610 - val_loss: 0.1376 - val_accuracy: 0.9540\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.9585 - val_loss: 0.1370 - val_accuracy: 0.9535\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9605 - val_loss: 0.1370 - val_accuracy: 0.9545\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1123 - accuracy: 0.9610 - val_loss: 0.1366 - val_accuracy: 0.9540\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9607 - val_loss: 0.1370 - val_accuracy: 0.9525\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9614 - val_loss: 0.1360 - val_accuracy: 0.9540\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9608 - val_loss: 0.1362 - val_accuracy: 0.9555\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1107 - accuracy: 0.9596 - val_loss: 0.1356 - val_accuracy: 0.9545\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9605 - val_loss: 0.1352 - val_accuracy: 0.9550\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9615 - val_loss: 0.1349 - val_accuracy: 0.9540\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9621 - val_loss: 0.1350 - val_accuracy: 0.9550\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.1344 - val_accuracy: 0.9550\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1095 - accuracy: 0.9629 - val_loss: 0.1345 - val_accuracy: 0.9540\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1091 - accuracy: 0.9617 - val_loss: 0.1345 - val_accuracy: 0.9540\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1089 - accuracy: 0.9630 - val_loss: 0.1337 - val_accuracy: 0.9545\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9622 - val_loss: 0.1335 - val_accuracy: 0.9545\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9623 - val_loss: 0.1338 - val_accuracy: 0.9525\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9622 - val_loss: 0.1329 - val_accuracy: 0.9540\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1077 - accuracy: 0.9626 - val_loss: 0.1330 - val_accuracy: 0.9550\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9633 - val_loss: 0.1327 - val_accuracy: 0.9540\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9622 - val_loss: 0.1328 - val_accuracy: 0.9545\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9617 - val_loss: 0.1332 - val_accuracy: 0.9540\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9641 - val_loss: 0.1317 - val_accuracy: 0.9545\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9631 - val_loss: 0.1320 - val_accuracy: 0.9540\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9632 - val_loss: 0.1318 - val_accuracy: 0.9540\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9630 - val_loss: 0.1313 - val_accuracy: 0.9545\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9643 - val_loss: 0.1322 - val_accuracy: 0.9535\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1042 - accuracy: 0.9647 - val_loss: 0.1307 - val_accuracy: 0.9545\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9632 - val_loss: 0.1306 - val_accuracy: 0.9550\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9627 - val_loss: 0.1304 - val_accuracy: 0.9555\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9634 - val_loss: 0.1305 - val_accuracy: 0.9560\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9643 - val_loss: 0.1298 - val_accuracy: 0.9550\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9639 - val_loss: 0.1303 - val_accuracy: 0.9545\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9644 - val_loss: 0.1302 - val_accuracy: 0.9545\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9660 - val_loss: 0.1292 - val_accuracy: 0.9560\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9658 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9661 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9657 - val_loss: 0.1305 - val_accuracy: 0.9535\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9650 - val_loss: 0.1310 - val_accuracy: 0.9555\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9661 - val_loss: 0.1299 - val_accuracy: 0.9540\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9643 - val_loss: 0.1285 - val_accuracy: 0.9555\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1019 - accuracy: 0.9644 - val_loss: 0.1282 - val_accuracy: 0.9550\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0988 - accuracy: 0.9661 - val_loss: 0.1279 - val_accuracy: 0.9545\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0993 - accuracy: 0.9650 - val_loss: 0.1288 - val_accuracy: 0.9545\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1002 - accuracy: 0.9662 - val_loss: 0.1275 - val_accuracy: 0.9565\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0986 - accuracy: 0.9672 - val_loss: 0.1273 - val_accuracy: 0.9570\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9650 - val_loss: 0.1270 - val_accuracy: 0.9545\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9669 - val_loss: 0.1265 - val_accuracy: 0.9565\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9663 - val_loss: 0.1264 - val_accuracy: 0.9560\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9672 - val_loss: 0.1266 - val_accuracy: 0.9560\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9652 - val_loss: 0.1259 - val_accuracy: 0.9560\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9663 - val_loss: 0.1259 - val_accuracy: 0.9560\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.9682 - val_loss: 0.1258 - val_accuracy: 0.9565\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9662 - val_loss: 0.1260 - val_accuracy: 0.9550\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9675 - val_loss: 0.1260 - val_accuracy: 0.9560\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9682 - val_loss: 0.1258 - val_accuracy: 0.9555\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9683 - val_loss: 0.1256 - val_accuracy: 0.9555\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9679 - val_loss: 0.1248 - val_accuracy: 0.9570\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9685 - val_loss: 0.1252 - val_accuracy: 0.9570\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9681 - val_loss: 0.1248 - val_accuracy: 0.9565\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.9677 - val_loss: 0.1249 - val_accuracy: 0.9570\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9687 - val_loss: 0.1245 - val_accuracy: 0.9565\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9678 - val_loss: 0.1242 - val_accuracy: 0.9565\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9665 - val_loss: 0.1240 - val_accuracy: 0.9560\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0938 - accuracy: 0.9675 - val_loss: 0.1238 - val_accuracy: 0.9555\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.1242 - val_accuracy: 0.9560\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9672 - val_loss: 0.1234 - val_accuracy: 0.9550\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 0.1234 - val_accuracy: 0.9560\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9691 - val_loss: 0.1229 - val_accuracy: 0.9565\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9698 - val_loss: 0.1237 - val_accuracy: 0.9565\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9697 - val_loss: 0.1249 - val_accuracy: 0.9565\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9689 - val_loss: 0.1231 - val_accuracy: 0.9570\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9712 - val_loss: 0.1226 - val_accuracy: 0.9570\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0910 - accuracy: 0.9703 - val_loss: 0.1224 - val_accuracy: 0.9565\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9696 - val_loss: 0.1227 - val_accuracy: 0.9565\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9685 - val_loss: 0.1223 - val_accuracy: 0.9575\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0919 - accuracy: 0.9691 - val_loss: 0.1226 - val_accuracy: 0.9560\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9708 - val_loss: 0.1232 - val_accuracy: 0.9565\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9710 - val_loss: 0.1214 - val_accuracy: 0.9575\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.9710 - val_loss: 0.1223 - val_accuracy: 0.9560\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.9704 - val_loss: 0.1218 - val_accuracy: 0.9580\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0894 - accuracy: 0.9721 - val_loss: 0.1219 - val_accuracy: 0.9555\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0894 - accuracy: 0.9703 - val_loss: 0.1212 - val_accuracy: 0.9575\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9701 - val_loss: 0.1211 - val_accuracy: 0.9575\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9705 - val_loss: 0.1211 - val_accuracy: 0.9560\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9707 - val_loss: 0.1208 - val_accuracy: 0.9575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZrVlg-BHkES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "18b9ae14-e05f-41f3-c2d6-aeca3f305c15"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_loss'])\n",
        "plt.plot(history_2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jggp33GTHkN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "28989f01-77cb-4a8a-85a0-c929ad493b99"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_accuracy'])\n",
        "plt.plot(history_2.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lBU4uykrOqH"
      },
      "source": [
        "# Validation Set (The CORRECT way of training and evaluation process)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hz7bgmgH4lu"
      },
      "source": [
        "#cross validation example\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint #save the model version that achieved lower loss!\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=2) # number of folds, shuffle, seed"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold"
      ],
      "metadata": {
        "id": "-sP5fd-_uNh3",
        "outputId": "442a3db6-1254-4c1d-d8ba-c8b5b38b6fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KFold(n_splits=5, random_state=2, shuffle=True)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqB1wQNMPfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5bd4f0-2bf7-4250-94f8-e53d2d9d227e"
      },
      "source": [
        "# enumerate splits\n",
        "fold = 1\n",
        "for train, val in kfold.split(x_train_binary):\n",
        "  print('train: %s, val: %s' % (train, val))\n",
        "  X_train_binary = x_train_binary[train]\n",
        "  Y_train_binary = y_train_binary[train]\n",
        "  X_val_binary = x_train_binary[val]\n",
        "  Y_val_binary = y_train_binary[val]\n",
        "\n",
        "  seed = 2\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(200, activation=\"relu\"))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  batch_size = 128\n",
        "  epochs = 20\n",
        "\n",
        "  save_model = ModelCheckpoint('best_model_'+str(fold)+'.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
        "\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  history_3 = model.fit(X_train_binary, Y_train_binary, batch_size=batch_size, epochs=epochs, callbacks=[save_model],\n",
        "            validation_data=(X_val_binary, Y_val_binary))\n",
        "  fold+=1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [    0     1     3 ... 11997 11998 11999], val: [    2     4    10 ... 11991 11992 11994]\n",
            "Epoch 1/20\n",
            "\u001b[1m63/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.3082\n",
            "Epoch 1: val_loss improved from inf to 0.15679, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8737 - loss: 0.2948 - val_accuracy: 0.9408 - val_loss: 0.1568\n",
            "Epoch 2/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9273 - loss: 0.1747\n",
            "Epoch 2: val_loss improved from 0.15679 to 0.14304, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9274 - loss: 0.1746 - val_accuracy: 0.9463 - val_loss: 0.1430\n",
            "Epoch 3/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1552\n",
            "Epoch 3: val_loss improved from 0.14304 to 0.13417, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1552 - val_accuracy: 0.9504 - val_loss: 0.1342\n",
            "Epoch 4/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9445 - loss: 0.1412\n",
            "Epoch 4: val_loss improved from 0.13417 to 0.13183, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9445 - loss: 0.1412 - val_accuracy: 0.9471 - val_loss: 0.1318\n",
            "Epoch 5/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9463 - loss: 0.1327\n",
            "Epoch 5: val_loss improved from 0.13183 to 0.11902, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9464 - loss: 0.1327 - val_accuracy: 0.9583 - val_loss: 0.1190\n",
            "Epoch 6/20\n",
            "\u001b[1m51/75\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1229\n",
            "Epoch 6: val_loss improved from 0.11902 to 0.11580, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9519 - loss: 0.1211 - val_accuracy: 0.9579 - val_loss: 0.1158\n",
            "Epoch 7/20\n",
            "\u001b[1m69/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.1145\n",
            "Epoch 7: val_loss improved from 0.11580 to 0.10694, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9578 - loss: 0.1143 - val_accuracy: 0.9638 - val_loss: 0.1069\n",
            "Epoch 8/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1061\n",
            "Epoch 8: val_loss did not improve from 0.10694\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9604 - loss: 0.1060 - val_accuracy: 0.9600 - val_loss: 0.1070\n",
            "Epoch 9/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.0987\n",
            "Epoch 9: val_loss improved from 0.10694 to 0.09991, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.0987 - val_accuracy: 0.9650 - val_loss: 0.0999\n",
            "Epoch 10/20\n",
            "\u001b[1m51/75\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9628 - loss: 0.0968\n",
            "Epoch 10: val_loss improved from 0.09991 to 0.09853, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9638 - loss: 0.0960 - val_accuracy: 0.9654 - val_loss: 0.0985\n",
            "Epoch 11/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.0911\n",
            "Epoch 11: val_loss improved from 0.09853 to 0.09514, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.0911 - val_accuracy: 0.9679 - val_loss: 0.0951\n",
            "Epoch 12/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9706 - loss: 0.0852\n",
            "Epoch 12: val_loss did not improve from 0.09514\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9705 - loss: 0.0852 - val_accuracy: 0.9688 - val_loss: 0.0961\n",
            "Epoch 13/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0824\n",
            "Epoch 13: val_loss improved from 0.09514 to 0.09423, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.0825 - val_accuracy: 0.9700 - val_loss: 0.0942\n",
            "Epoch 14/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0772\n",
            "Epoch 14: val_loss did not improve from 0.09423\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9721 - loss: 0.0773 - val_accuracy: 0.9692 - val_loss: 0.0960\n",
            "Epoch 15/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.0735\n",
            "Epoch 15: val_loss did not improve from 0.09423\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9718 - loss: 0.0735 - val_accuracy: 0.9683 - val_loss: 0.0993\n",
            "Epoch 16/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.0733\n",
            "Epoch 16: val_loss did not improve from 0.09423\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9732 - loss: 0.0733 - val_accuracy: 0.9642 - val_loss: 0.1046\n",
            "Epoch 17/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0688\n",
            "Epoch 17: val_loss improved from 0.09423 to 0.09353, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.0688 - val_accuracy: 0.9675 - val_loss: 0.0935\n",
            "Epoch 18/20\n",
            "\u001b[1m69/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0636\n",
            "Epoch 18: val_loss did not improve from 0.09353\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9788 - loss: 0.0639 - val_accuracy: 0.9679 - val_loss: 0.0955\n",
            "Epoch 19/20\n",
            "\u001b[1m51/75\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0663\n",
            "Epoch 19: val_loss improved from 0.09353 to 0.09348, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9786 - loss: 0.0647 - val_accuracy: 0.9663 - val_loss: 0.0935\n",
            "Epoch 20/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0606\n",
            "Epoch 20: val_loss improved from 0.09348 to 0.08893, saving model to best_model_1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.0606 - val_accuracy: 0.9700 - val_loss: 0.0889\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    3     5     6 ... 11988 11989 11993]\n",
            "Epoch 1/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8584 - loss: 0.3094\n",
            "Epoch 1: val_loss improved from inf to 0.18786, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8602 - loss: 0.3063 - val_accuracy: 0.9271 - val_loss: 0.1879\n",
            "Epoch 2/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9344 - loss: 0.1615\n",
            "Epoch 2: val_loss improved from 0.18786 to 0.17664, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9345 - loss: 0.1617 - val_accuracy: 0.9321 - val_loss: 0.1766\n",
            "Epoch 3/20\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1419\n",
            "Epoch 3: val_loss improved from 0.17664 to 0.16771, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.1420 - val_accuracy: 0.9375 - val_loss: 0.1677\n",
            "Epoch 4/20\n",
            "\u001b[1m68/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9479 - loss: 0.1311\n",
            "Epoch 4: val_loss improved from 0.16771 to 0.16051, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9478 - loss: 0.1319 - val_accuracy: 0.9392 - val_loss: 0.1605\n",
            "Epoch 5/20\n",
            "\u001b[1m51/75\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9561 - loss: 0.1179\n",
            "Epoch 5: val_loss improved from 0.16051 to 0.15348, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9548 - loss: 0.1211 - val_accuracy: 0.9408 - val_loss: 0.1535\n",
            "Epoch 6/20\n",
            "\u001b[1m65/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1134\n",
            "Epoch 6: val_loss improved from 0.15348 to 0.14152, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9574 - loss: 0.1143 - val_accuracy: 0.9458 - val_loss: 0.1415\n",
            "Epoch 7/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9642 - loss: 0.1034\n",
            "Epoch 7: val_loss improved from 0.14152 to 0.13635, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1038 - val_accuracy: 0.9500 - val_loss: 0.1363\n",
            "Epoch 8/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9610 - loss: 0.1020\n",
            "Epoch 8: val_loss improved from 0.13635 to 0.12868, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9610 - loss: 0.1022 - val_accuracy: 0.9538 - val_loss: 0.1287\n",
            "Epoch 9/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.0930\n",
            "Epoch 9: val_loss improved from 0.12868 to 0.11680, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.0934 - val_accuracy: 0.9600 - val_loss: 0.1168\n",
            "Epoch 10/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0859\n",
            "Epoch 10: val_loss improved from 0.11680 to 0.11435, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9692 - loss: 0.0861 - val_accuracy: 0.9588 - val_loss: 0.1143\n",
            "Epoch 11/20\n",
            "\u001b[1m67/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.0829\n",
            "Epoch 11: val_loss improved from 0.11435 to 0.11207, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0836 - val_accuracy: 0.9608 - val_loss: 0.1121\n",
            "Epoch 12/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.0771\n",
            "Epoch 12: val_loss improved from 0.11207 to 0.10756, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9729 - loss: 0.0773 - val_accuracy: 0.9613 - val_loss: 0.1076\n",
            "Epoch 13/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9726 - loss: 0.0764\n",
            "Epoch 13: val_loss did not improve from 0.10756\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0767 - val_accuracy: 0.9629 - val_loss: 0.1076\n",
            "Epoch 14/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.0716\n",
            "Epoch 14: val_loss improved from 0.10756 to 0.10404, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0718 - val_accuracy: 0.9625 - val_loss: 0.1040\n",
            "Epoch 15/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.0678\n",
            "Epoch 15: val_loss did not improve from 0.10404\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9766 - loss: 0.0679 - val_accuracy: 0.9625 - val_loss: 0.1082\n",
            "Epoch 16/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.0664\n",
            "Epoch 16: val_loss did not improve from 0.10404\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9766 - loss: 0.0666 - val_accuracy: 0.9629 - val_loss: 0.1049\n",
            "Epoch 17/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0622\n",
            "Epoch 17: val_loss improved from 0.10404 to 0.10253, saving model to best_model_2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0623 - val_accuracy: 0.9621 - val_loss: 0.1025\n",
            "Epoch 18/20\n",
            "\u001b[1m67/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.0606\n",
            "Epoch 18: val_loss did not improve from 0.10253\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9783 - loss: 0.0609 - val_accuracy: 0.9629 - val_loss: 0.1042\n",
            "Epoch 19/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0600\n",
            "Epoch 19: val_loss did not improve from 0.10253\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.0601 - val_accuracy: 0.9650 - val_loss: 0.1029\n",
            "Epoch 20/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0551\n",
            "Epoch 20: val_loss did not improve from 0.10253\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9812 - loss: 0.0553 - val_accuracy: 0.9650 - val_loss: 0.1054\n",
            "train: [    1     2     3 ... 11994 11995 11996], val: [    0    13    19 ... 11997 11998 11999]\n",
            "Epoch 1/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8605 - loss: 0.3042\n",
            "Epoch 1: val_loss improved from inf to 0.17311, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8636 - loss: 0.2987 - val_accuracy: 0.9388 - val_loss: 0.1731\n",
            "Epoch 2/20\n",
            "\u001b[1m65/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9332 - loss: 0.1693\n",
            "Epoch 2: val_loss improved from 0.17311 to 0.15221, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9337 - loss: 0.1680 - val_accuracy: 0.9429 - val_loss: 0.1522\n",
            "Epoch 3/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9441 - loss: 0.1475\n",
            "Epoch 3: val_loss improved from 0.15221 to 0.14325, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9441 - loss: 0.1474 - val_accuracy: 0.9471 - val_loss: 0.1432\n",
            "Epoch 4/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9510 - loss: 0.1311\n",
            "Epoch 4: val_loss improved from 0.14325 to 0.14117, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9509 - loss: 0.1310 - val_accuracy: 0.9467 - val_loss: 0.1412\n",
            "Epoch 5/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1248\n",
            "Epoch 5: val_loss improved from 0.14117 to 0.12795, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9568 - loss: 0.1246 - val_accuracy: 0.9521 - val_loss: 0.1280\n",
            "Epoch 6/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1119\n",
            "Epoch 6: val_loss improved from 0.12795 to 0.12607, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1119 - val_accuracy: 0.9525 - val_loss: 0.1261\n",
            "Epoch 7/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1072\n",
            "Epoch 7: val_loss improved from 0.12607 to 0.11770, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1072 - val_accuracy: 0.9533 - val_loss: 0.1177\n",
            "Epoch 8/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.0975\n",
            "Epoch 8: val_loss did not improve from 0.11770\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.0976 - val_accuracy: 0.9554 - val_loss: 0.1183\n",
            "Epoch 9/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.0916\n",
            "Epoch 9: val_loss improved from 0.11770 to 0.11649, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.0916 - val_accuracy: 0.9542 - val_loss: 0.1165\n",
            "Epoch 10/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.0886\n",
            "Epoch 10: val_loss improved from 0.11649 to 0.11232, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0886 - val_accuracy: 0.9583 - val_loss: 0.1123\n",
            "Epoch 11/20\n",
            "\u001b[1m67/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0825\n",
            "Epoch 11: val_loss improved from 0.11232 to 0.11177, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.0829 - val_accuracy: 0.9563 - val_loss: 0.1118\n",
            "Epoch 12/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.0798\n",
            "Epoch 12: val_loss improved from 0.11177 to 0.10913, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0801 - val_accuracy: 0.9583 - val_loss: 0.1091\n",
            "Epoch 13/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0811\n",
            "Epoch 13: val_loss did not improve from 0.10913\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0810 - val_accuracy: 0.9533 - val_loss: 0.1152\n",
            "Epoch 14/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0790\n",
            "Epoch 14: val_loss improved from 0.10913 to 0.10513, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0788 - val_accuracy: 0.9629 - val_loss: 0.1051\n",
            "Epoch 15/20\n",
            "\u001b[1m65/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.0743\n",
            "Epoch 15: val_loss improved from 0.10513 to 0.10172, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9737 - loss: 0.0742 - val_accuracy: 0.9625 - val_loss: 0.1017\n",
            "Epoch 16/20\n",
            "\u001b[1m62/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9791 - loss: 0.0691\n",
            "Epoch 16: val_loss improved from 0.10172 to 0.09969, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9787 - loss: 0.0693 - val_accuracy: 0.9638 - val_loss: 0.0997\n",
            "Epoch 17/20\n",
            "\u001b[1m63/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.0679\n",
            "Epoch 17: val_loss did not improve from 0.09969\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9782 - loss: 0.0677 - val_accuracy: 0.9642 - val_loss: 0.1004\n",
            "Epoch 18/20\n",
            "\u001b[1m68/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0674\n",
            "Epoch 18: val_loss did not improve from 0.09969\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9774 - loss: 0.0674 - val_accuracy: 0.9638 - val_loss: 0.1013\n",
            "Epoch 19/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0631\n",
            "Epoch 19: val_loss improved from 0.09969 to 0.09898, saving model to best_model_3.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9803 - loss: 0.0631 - val_accuracy: 0.9646 - val_loss: 0.0990\n",
            "Epoch 20/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0602\n",
            "Epoch 20: val_loss did not improve from 0.09898\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9803 - loss: 0.0602 - val_accuracy: 0.9613 - val_loss: 0.1002\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    7     9    21 ... 11948 11952 11955]\n",
            "Epoch 1/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8429 - loss: 0.3315\n",
            "Epoch 1: val_loss improved from inf to 0.20097, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8464 - loss: 0.3256 - val_accuracy: 0.9258 - val_loss: 0.2010\n",
            "Epoch 2/20\n",
            "\u001b[1m65/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9355 - loss: 0.1722\n",
            "Epoch 2: val_loss improved from 0.20097 to 0.18465, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9354 - loss: 0.1717 - val_accuracy: 0.9308 - val_loss: 0.1847\n",
            "Epoch 3/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9449 - loss: 0.1498\n",
            "Epoch 3: val_loss improved from 0.18465 to 0.17033, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9448 - loss: 0.1498 - val_accuracy: 0.9358 - val_loss: 0.1703\n",
            "Epoch 4/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1392\n",
            "Epoch 4: val_loss improved from 0.17033 to 0.15329, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1392 - val_accuracy: 0.9446 - val_loss: 0.1533\n",
            "Epoch 5/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1291\n",
            "Epoch 5: val_loss improved from 0.15329 to 0.12840, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9542 - loss: 0.1290 - val_accuracy: 0.9550 - val_loss: 0.1284\n",
            "Epoch 6/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9596 - loss: 0.1197\n",
            "Epoch 6: val_loss improved from 0.12840 to 0.11764, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1197 - val_accuracy: 0.9579 - val_loss: 0.1176\n",
            "Epoch 7/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9619 - loss: 0.1121\n",
            "Epoch 7: val_loss improved from 0.11764 to 0.11085, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1122 - val_accuracy: 0.9617 - val_loss: 0.1109\n",
            "Epoch 8/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9634 - loss: 0.1070\n",
            "Epoch 8: val_loss improved from 0.11085 to 0.10643, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9634 - loss: 0.1070 - val_accuracy: 0.9617 - val_loss: 0.1064\n",
            "Epoch 9/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1002\n",
            "Epoch 9: val_loss improved from 0.10643 to 0.10459, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.1002 - val_accuracy: 0.9613 - val_loss: 0.1046\n",
            "Epoch 10/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.0937\n",
            "Epoch 10: val_loss improved from 0.10459 to 0.09989, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9681 - loss: 0.0937 - val_accuracy: 0.9642 - val_loss: 0.0999\n",
            "Epoch 11/20\n",
            "\u001b[1m67/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0915\n",
            "Epoch 11: val_loss did not improve from 0.09989\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0913 - val_accuracy: 0.9617 - val_loss: 0.1010\n",
            "Epoch 12/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0875\n",
            "Epoch 12: val_loss improved from 0.09989 to 0.09790, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9694 - loss: 0.0874 - val_accuracy: 0.9621 - val_loss: 0.0979\n",
            "Epoch 13/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0839\n",
            "Epoch 13: val_loss did not improve from 0.09790\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9708 - loss: 0.0838 - val_accuracy: 0.9638 - val_loss: 0.0990\n",
            "Epoch 14/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.0817\n",
            "Epoch 14: val_loss did not improve from 0.09790\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0814 - val_accuracy: 0.9650 - val_loss: 0.0984\n",
            "Epoch 15/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.0827\n",
            "Epoch 15: val_loss improved from 0.09790 to 0.09624, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0825 - val_accuracy: 0.9642 - val_loss: 0.0962\n",
            "Epoch 16/20\n",
            "\u001b[1m69/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0768\n",
            "Epoch 16: val_loss improved from 0.09624 to 0.09473, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.0765 - val_accuracy: 0.9629 - val_loss: 0.0947\n",
            "Epoch 17/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.0728\n",
            "Epoch 17: val_loss improved from 0.09473 to 0.09394, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9764 - loss: 0.0726 - val_accuracy: 0.9642 - val_loss: 0.0939\n",
            "Epoch 18/20\n",
            "\u001b[1m67/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0673\n",
            "Epoch 18: val_loss did not improve from 0.09394\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0672 - val_accuracy: 0.9629 - val_loss: 0.0970\n",
            "Epoch 19/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0675\n",
            "Epoch 19: val_loss improved from 0.09394 to 0.09374, saving model to best_model_4.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0673 - val_accuracy: 0.9663 - val_loss: 0.0937\n",
            "Epoch 20/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0668\n",
            "Epoch 20: val_loss did not improve from 0.09374\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9783 - loss: 0.0666 - val_accuracy: 0.9650 - val_loss: 0.0962\n",
            "train: [    0     2     3 ... 11997 11998 11999], val: [    1    12    14 ... 11985 11995 11996]\n",
            "Epoch 1/20\n",
            "\u001b[1m61/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8219 - loss: 0.3701\n",
            "Epoch 1: val_loss improved from inf to 0.18220, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8358 - loss: 0.3468 - val_accuracy: 0.9283 - val_loss: 0.1822\n",
            "Epoch 2/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9332 - loss: 0.1759\n",
            "Epoch 2: val_loss improved from 0.18220 to 0.16800, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9332 - loss: 0.1756 - val_accuracy: 0.9342 - val_loss: 0.1680\n",
            "Epoch 3/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9412 - loss: 0.1540\n",
            "Epoch 3: val_loss improved from 0.16800 to 0.15837, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.1539 - val_accuracy: 0.9429 - val_loss: 0.1584\n",
            "Epoch 4/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9478 - loss: 0.1428\n",
            "Epoch 4: val_loss improved from 0.15837 to 0.14461, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9477 - loss: 0.1427 - val_accuracy: 0.9463 - val_loss: 0.1446\n",
            "Epoch 5/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9534 - loss: 0.1298\n",
            "Epoch 5: val_loss did not improve from 0.14461\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9533 - loss: 0.1298 - val_accuracy: 0.9500 - val_loss: 0.1464\n",
            "Epoch 6/20\n",
            "\u001b[1m62/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1212\n",
            "Epoch 6: val_loss improved from 0.14461 to 0.13552, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9574 - loss: 0.1215 - val_accuracy: 0.9533 - val_loss: 0.1355\n",
            "Epoch 7/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1139\n",
            "Epoch 7: val_loss improved from 0.13552 to 0.12741, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9592 - loss: 0.1140 - val_accuracy: 0.9592 - val_loss: 0.1274\n",
            "Epoch 8/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.1085\n",
            "Epoch 8: val_loss did not improve from 0.12741\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9617 - loss: 0.1086 - val_accuracy: 0.9579 - val_loss: 0.1285\n",
            "Epoch 9/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1018\n",
            "Epoch 9: val_loss improved from 0.12741 to 0.12267, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9649 - loss: 0.1019 - val_accuracy: 0.9617 - val_loss: 0.1227\n",
            "Epoch 10/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.0970\n",
            "Epoch 10: val_loss improved from 0.12267 to 0.12044, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.0971 - val_accuracy: 0.9608 - val_loss: 0.1204\n",
            "Epoch 11/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.0930\n",
            "Epoch 11: val_loss did not improve from 0.12044\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.0931 - val_accuracy: 0.9613 - val_loss: 0.1207\n",
            "Epoch 12/20\n",
            "\u001b[1m66/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.0857\n",
            "Epoch 12: val_loss improved from 0.12044 to 0.11572, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9720 - loss: 0.0860 - val_accuracy: 0.9646 - val_loss: 0.1157\n",
            "Epoch 13/20\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.0828\n",
            "Epoch 13: val_loss improved from 0.11572 to 0.11333, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.0828 - val_accuracy: 0.9642 - val_loss: 0.1133\n",
            "Epoch 14/20\n",
            "\u001b[1m70/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9726 - loss: 0.0807\n",
            "Epoch 14: val_loss improved from 0.11333 to 0.11193, saving model to best_model_5.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0809 - val_accuracy: 0.9679 - val_loss: 0.1119\n",
            "Epoch 15/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.0783 - val_accuracy: 0.9650 - val_loss: 0.1133\n",
            "Epoch 16/20\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0710\n",
            "Epoch 16: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9777 - loss: 0.0711 - val_accuracy: 0.9621 - val_loss: 0.1229\n",
            "Epoch 17/20\n",
            "\u001b[1m71/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.0700\n",
            "Epoch 17: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.0701 - val_accuracy: 0.9650 - val_loss: 0.1163\n",
            "Epoch 18/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0646\n",
            "Epoch 18: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9792 - loss: 0.0647 - val_accuracy: 0.9646 - val_loss: 0.1145\n",
            "Epoch 19/20\n",
            "\u001b[1m68/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0643\n",
            "Epoch 19: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9789 - loss: 0.0646 - val_accuracy: 0.9633 - val_loss: 0.1138\n",
            "Epoch 20/20\n",
            "\u001b[1m72/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0613\n",
            "Epoch 20: val_loss did not improve from 0.11193\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9781 - loss: 0.0615 - val_accuracy: 0.9646 - val_loss: 0.1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6PBgtRoOCEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7beb79-875a-4dbd-9dd9-fc7f33fe0499"
      },
      "source": [
        "#Wrong\n",
        "print(np.mean(np.round(history_3.model.predict(X_val_binary))==Y_val_binary.reshape(-1,1)))\n",
        "print(np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "0.9645833333333333\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "0.9635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgBoo6qwOePl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9beea5-45c8-494f-f3e9-1da97492260e"
      },
      "source": [
        "val_accs = []\n",
        "test_accs = []\n",
        "for i in range(1,6):\n",
        "  history_3.model.load_weights('best_model_'+str(i)+'.h5')\n",
        "  val_accs.append(np.mean(np.round(history_3.model.predict(X_val_binary))==Y_val_binary.reshape(-1,1)))\n",
        "  test_accs.append(np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1)))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(test_accs)"
      ],
      "metadata": {
        "id": "iPmvfrLbvuto",
        "outputId": "6b219f70-f3a4-4f4e-d29c-dea31e1ff8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9621999999999999)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hD9hTL2c-fP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}