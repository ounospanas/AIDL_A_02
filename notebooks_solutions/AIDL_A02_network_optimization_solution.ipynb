{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDL_A02_network_optimization_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ounospanas/AIDL_A_02/blob/main/notebooks_solutions/AIDL_A02_network_optimization_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j25rklnwD1-m"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JenmhPusFZ06"
      },
      "source": [
        "#load dataset\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "V7uVONPjF1PP",
        "outputId": "d1baef7a-c3be-4e75-db61-3abba29561af"
      },
      "source": [
        "#visualize some data\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "selected = [0,1,3,5,6,8,16,18,19,23]\n",
        "plt.figure(figsize=(11, 11))\n",
        "for i, s in enumerate(selected):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    #img = plt.imread(x_train[s])\n",
        "    plt.imshow(x_train[s], cmap='gray')\n",
        "    plt.xlabel(classes[y_train[s]],)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 792x792 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8NenH6QEPlH"
      },
      "source": [
        "# normalize data\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)/255\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)/255"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zD2QsqukISO"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWtYiD_eEJsq"
      },
      "source": [
        "# select only t-shirts and ankle boots\n",
        "shirt_train = np.where(y_train==0)\n",
        "dress_train = np.where(y_train==3)\n",
        "\n",
        "shirt_test = np.where(y_test==0)\n",
        "dress_test = np.where(y_test==3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugGWzBtFJA_1"
      },
      "source": [
        "# concatenate them\n",
        "x_train_s = x_train[shirt_train]\n",
        "y_train_s = y_train[shirt_train]\n",
        "\n",
        "x_test_s = x_test[shirt_test]\n",
        "y_test_s = y_test[shirt_test]\n",
        "\n",
        "x_train_d = x_train[dress_train]\n",
        "y_train_d = y_train[dress_train]\n",
        "\n",
        "x_test_d = x_test[dress_test]\n",
        "y_test_d = y_test[dress_test]\n",
        "\n",
        "x_train_binary = np.concatenate([x_train_s, x_train_d])\n",
        "x_test_binary = np.concatenate([x_test_s, x_test_d])\n",
        "\n",
        "y_train_binary = np.concatenate([y_train_s, np.ones_like(y_train_d)])\n",
        "y_test_binary = np.concatenate([y_test_s, np.ones_like(y_test_d)])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtVPYLzMAIL"
      },
      "source": [
        "# Neural Network (Custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW49UROyzfn-"
      },
      "source": [
        "Compute the sigmoid function:\n",
        "$$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYBih-MhImZZ"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute sigmoid function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    g : The sigmoid function.\n",
        "    \"\"\"\n",
        "    a = 1./(1+np.exp(-z))\n",
        "\n",
        "    return a"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wm-R3JFEvH"
      },
      "source": [
        "$a = ReLU(z) = max(z,0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w8lOkkExcO"
      },
      "source": [
        "def relu(z):\n",
        "    \"\"\"\n",
        "    Compute relu function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    a : The relu function.\n",
        "    \"\"\"\n",
        "    a = np.maximum(z,0)\n",
        "\n",
        "    return a"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iElu6cUL-aZ"
      },
      "source": [
        "# check relu function\n",
        "assert relu(-1) == 0\n",
        "assert relu(2) == 2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v435SmJ2hSGF"
      },
      "source": [
        "# Parameter Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dql9nfxkAd"
      },
      "source": [
        "Xavier initialization: $$[-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}}]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0CIw-A92vsz"
      },
      "source": [
        "def init_params_xavier(n_in, n_out):\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check np.random.uniform\n",
        "    np.random.seed(0)\n",
        "    # init random params and multiply it with 0.1\n",
        "    w = np.random.uniform(-np.sqrt(6./(n_in+n_out)), np.sqrt(6./(n_in+n_out)), (n_in, n_out))\n",
        "    b = np.random.randn(n_out)*0.01\n",
        "    return w, b"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyflAZhrXLu_",
        "outputId": "ceb95905-0188-4125-9e27-56730b3d93ba"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_xavier(2,3)\n",
        "print(np.round(w,4) == np.array([[ 0.1069,  0.4715,  0.2251],[ 0.0983, -0.1673,  0.3196]]))\n",
        "print(np.round(b,4) == np.array([0.0095, -0.0015, -0.001]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_vu4QwaBxU"
      },
      "source": [
        "He initialization: $$np.random.randn(n_{in}, n_{out})*\\sqrt{\\frac{2}{n_{in}}}$$, not multiply using 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK-3JBi2aAxp"
      },
      "source": [
        "def init_params_he(n_in, n_out):\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check np.random.uniform\n",
        "    np.random.seed(0)\n",
        "    # init random params and multiply it with 0.1\n",
        "    w = np.random.randn(n_in, n_out)*np.sqrt(2/n_in)\n",
        "    b = np.random.randn(n_out)*np.sqrt(2/n_in)\n",
        "    return w, b"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xEjaH9Vde3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e03f89-5604-4826-d2b9-04a4cc9ecc61"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_he(2,3)\n",
        "print(np.round(w,4) == np.array([[ 1.7641,  0.4002,  0.9787], [ 2.2409,  1.8676, -0.9773]]))\n",
        "print(np.round(b,4) == np.array([0.9501, -0.1514, -0.1032]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M6bkSI46hd"
      },
      "source": [
        "Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAJJ67d4gmD"
      },
      "source": [
        "def costFunction(y, m, a):\n",
        "    \"\"\"\n",
        "    Computes cost for linear regression. \n",
        "    X : feature vector, shape (m x n+1)\n",
        "    y : labels (i.e., dog or cat), shape (m, )\n",
        "    w : parameters for the linear regression, shape (n+1, )\n",
        "    m: data legth\n",
        "\n",
        "    returns\n",
        "    -------\n",
        "    J : value of cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    J = -1/m * np.sum(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "    \n",
        "  \n",
        "    return J"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKNlaT8XZuG"
      },
      "source": [
        "#forward pass\n",
        "def forward(X, w, b, activation = 'relu'):\n",
        "    z = np.dot(X,w) + b\n",
        "    if activation=='relu':\n",
        "        a = relu(z)\n",
        "    else:\n",
        "        a = sigmoid(z)\n",
        "    return a"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AivWfcIMY5ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c3b73b-4195-4f53-c8f9-af4c4c7a76f5"
      },
      "source": [
        "w, b  = init_params_he(2,3)\n",
        "forward(np.asarray([[1,2],[3,4]]), w, b, 'relu')[0].shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zraP5X4zhYq"
      },
      "source": [
        "\\begin{split}ReLU'(z)= \\begin{Bmatrix}1 & z>0 \\\\ \n",
        "0 & z<0 \\end{Bmatrix}\\end{split}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOTSR774duI"
      },
      "source": [
        "#relu gradient\n",
        "def reluBackward(z):\n",
        "    z[z<=0] = 0\n",
        "    z[z>0] = 1\n",
        "    return z"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJK4wk_EA4t"
      },
      "source": [
        "Calculate the derivatives: $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A^{(i)}-Y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbHnEJsZX3Eu"
      },
      "source": [
        "#backpropagation\n",
        "def backward(a, dz):\n",
        "    m = len(a)\n",
        "    dw = np.dot(a.T, dz) / m\n",
        "    db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "    \n",
        "    return dw, db"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbgF0t0pRDfX"
      },
      "source": [
        "$$ w_j := w_j - \\alpha dw_j $$\n",
        "$$ b := b - \\alpha db $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7IbXv7rQgyt"
      },
      "source": [
        "# update parameters for optimization\n",
        "def update(w, b, dw, db, learning_rate=0.01):\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    return w, b"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPnS1ExVDffM"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural(X, y, n_layer_1, lr = 0.01, epochs = 100):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course). \n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = init_params_xavier(n_in, n_layer_1)\n",
        "    w2, b2 = init_params_xavier(n_layer_1, n_out)\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "        \n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        dz1 = np.dot((dz2),w2.T)*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "        \n",
        "        if i%10==0:\n",
        "            \n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1v9e_cjDeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd30c3f-4fd2-4704-d264-2f8fb6c834f3"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 with cost 0.73222077432145\n",
            "train: 0.5068333333333334\n",
            "test: 0.5\n",
            "epoch 10 with cost 0.3449637483262826\n",
            "train: 0.8908333333333334\n",
            "test: 0.882\n",
            "epoch 20 with cost 0.2831690845443682\n",
            "train: 0.897\n",
            "test: 0.894\n",
            "epoch 30 with cost 0.2593395198965416\n",
            "train: 0.9028333333333334\n",
            "test: 0.9015\n",
            "epoch 40 with cost 0.2452727337640431\n",
            "train: 0.9095833333333333\n",
            "test: 0.907\n",
            "epoch 50 with cost 0.23537812157188362\n",
            "train: 0.9139166666666667\n",
            "test: 0.913\n",
            "epoch 60 with cost 0.22785751996816328\n",
            "train: 0.9176666666666666\n",
            "test: 0.917\n",
            "epoch 70 with cost 0.22213162272876213\n",
            "train: 0.9193333333333333\n",
            "test: 0.921\n",
            "epoch 80 with cost 0.2175448627414139\n",
            "train: 0.9216666666666666\n",
            "test: 0.9225\n",
            "epoch 90 with cost 0.2136466884836102\n",
            "train: 0.9239166666666667\n",
            "test: 0.9235\n",
            "epoch 100 with cost 0.21042192808580462\n",
            "train: 0.9250833333333334\n",
            "test: 0.924\n",
            "epoch 110 with cost 0.20771532431056403\n",
            "train: 0.927\n",
            "test: 0.9255\n",
            "epoch 120 with cost 0.20538208768692795\n",
            "train: 0.928\n",
            "test: 0.9275\n",
            "epoch 130 with cost 0.20328904868104014\n",
            "train: 0.9290833333333334\n",
            "test: 0.928\n",
            "epoch 140 with cost 0.2013800963774929\n",
            "train: 0.93\n",
            "test: 0.9285\n",
            "epoch 150 with cost 0.19961861534254216\n",
            "train: 0.9319166666666666\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.1980201693774248\n",
            "train: 0.93275\n",
            "test: 0.9295\n",
            "epoch 170 with cost 0.19658358890152217\n",
            "train: 0.9340833333333334\n",
            "test: 0.931\n",
            "epoch 180 with cost 0.19526674275538497\n",
            "train: 0.9343333333333333\n",
            "test: 0.931\n",
            "epoch 190 with cost 0.194094280049121\n",
            "train: 0.9348333333333333\n",
            "test: 0.9315\n",
            "epoch 200 with cost 0.19305596820873597\n",
            "train: 0.9354166666666667\n",
            "test: 0.9335\n",
            "epoch 210 with cost 0.19210040498229924\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 220 with cost 0.19120432289433184\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 230 with cost 0.19038629547882224\n",
            "train: 0.9366666666666666\n",
            "test: 0.933\n",
            "epoch 240 with cost 0.1896301410584099\n",
            "train: 0.9370833333333334\n",
            "test: 0.933\n",
            "epoch 250 with cost 0.18898113489003068\n",
            "train: 0.9373333333333334\n",
            "test: 0.9325\n",
            "epoch 260 with cost 0.1883895772365391\n",
            "train: 0.93825\n",
            "test: 0.933\n",
            "epoch 270 with cost 0.187873027260566\n",
            "train: 0.9393333333333334\n",
            "test: 0.9335\n",
            "epoch 280 with cost 0.18747552213510196\n",
            "train: 0.9395833333333333\n",
            "test: 0.9335\n",
            "epoch 290 with cost 0.1871796391116818\n",
            "train: 0.9396666666666667\n",
            "test: 0.9345\n",
            "epoch 300 with cost 0.18691658572297357\n",
            "train: 0.94025\n",
            "test: 0.935\n",
            "epoch 310 with cost 0.186685439028307\n",
            "train: 0.9406666666666667\n",
            "test: 0.9345\n",
            "epoch 320 with cost 0.1864629169300269\n",
            "train: 0.9413333333333334\n",
            "test: 0.9355\n",
            "epoch 330 with cost 0.1862074862746406\n",
            "train: 0.942\n",
            "test: 0.9355\n",
            "epoch 340 with cost 0.18595601232142633\n",
            "train: 0.943\n",
            "test: 0.936\n",
            "epoch 350 with cost 0.1856838792859512\n",
            "train: 0.9429166666666666\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.18555748742851813\n",
            "train: 0.9430833333333334\n",
            "test: 0.9355\n",
            "epoch 370 with cost 0.1855693916822616\n",
            "train: 0.9435833333333333\n",
            "test: 0.9355\n",
            "epoch 380 with cost 0.18558261349135038\n",
            "train: 0.9435833333333333\n",
            "test: 0.936\n",
            "epoch 390 with cost 0.18558809649233152\n",
            "train: 0.9436666666666667\n",
            "test: 0.936\n",
            "epoch 400 with cost 0.18560110893932233\n",
            "train: 0.9438333333333333\n",
            "test: 0.9365\n",
            "epoch 410 with cost 0.18559470291092012\n",
            "train: 0.9445\n",
            "test: 0.9365\n",
            "epoch 420 with cost 0.18554222013773664\n",
            "train: 0.9449166666666666\n",
            "test: 0.9365\n",
            "epoch 430 with cost 0.1854775006794311\n",
            "train: 0.9450833333333334\n",
            "test: 0.9365\n",
            "epoch 440 with cost 0.18540716338808846\n",
            "train: 0.9449166666666666\n",
            "test: 0.9355\n",
            "epoch 450 with cost 0.18531191270498382\n",
            "train: 0.9454166666666667\n",
            "test: 0.9355\n",
            "epoch 460 with cost 0.1852470697827609\n",
            "train: 0.9453333333333334\n",
            "test: 0.935\n",
            "epoch 470 with cost 0.18528987254863655\n",
            "train: 0.9456666666666667\n",
            "test: 0.935\n",
            "epoch 480 with cost 0.18534556042763514\n",
            "train: 0.9458333333333333\n",
            "test: 0.936\n",
            "epoch 490 with cost 0.1854095584445661\n",
            "train: 0.9461666666666667\n",
            "test: 0.9355\n",
            "epoch 500 with cost 0.18544449207712985\n",
            "train: 0.9466666666666667\n",
            "test: 0.9365\n",
            "epoch 510 with cost 0.18546397792969813\n",
            "train: 0.9470833333333334\n",
            "test: 0.9365\n",
            "epoch 520 with cost 0.1854318020703331\n",
            "train: 0.9471666666666667\n",
            "test: 0.9355\n",
            "epoch 530 with cost 0.1853339138822911\n",
            "train: 0.94725\n",
            "test: 0.9355\n",
            "epoch 540 with cost 0.18520164011777926\n",
            "train: 0.9474166666666667\n",
            "test: 0.9355\n",
            "epoch 550 with cost 0.18500245360806464\n",
            "train: 0.9474166666666667\n",
            "test: 0.936\n",
            "epoch 560 with cost 0.18479215772975677\n",
            "train: 0.94775\n",
            "test: 0.9365\n",
            "epoch 570 with cost 0.18457449350813657\n",
            "train: 0.9479166666666666\n",
            "test: 0.9365\n",
            "epoch 580 with cost 0.18436324952597355\n",
            "train: 0.9481666666666667\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.18413452920873039\n",
            "train: 0.9485\n",
            "test: 0.937\n",
            "epoch 600 with cost 0.18388537526060436\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 610 with cost 0.18365383403333743\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 620 with cost 0.18347515772253606\n",
            "train: 0.9489166666666666\n",
            "test: 0.938\n",
            "epoch 630 with cost 0.18335629214785285\n",
            "train: 0.949\n",
            "test: 0.938\n",
            "epoch 640 with cost 0.1832960124475722\n",
            "train: 0.9489166666666666\n",
            "test: 0.937\n",
            "epoch 650 with cost 0.18320729819398732\n",
            "train: 0.9488333333333333\n",
            "test: 0.937\n",
            "epoch 660 with cost 0.18303959212553536\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 670 with cost 0.1828566338471163\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 680 with cost 0.18263981999875428\n",
            "train: 0.94875\n",
            "test: 0.9375\n",
            "epoch 690 with cost 0.18242524939504207\n",
            "train: 0.949\n",
            "test: 0.937\n",
            "epoch 700 with cost 0.18217889142094842\n",
            "train: 0.9493333333333334\n",
            "test: 0.937\n",
            "epoch 710 with cost 0.18187524025287466\n",
            "train: 0.9495\n",
            "test: 0.9375\n",
            "epoch 720 with cost 0.18157715496196783\n",
            "train: 0.9494166666666667\n",
            "test: 0.9375\n",
            "epoch 730 with cost 0.1812786783802568\n",
            "train: 0.9493333333333334\n",
            "test: 0.938\n",
            "epoch 740 with cost 0.1809804301190431\n",
            "train: 0.9494166666666667\n",
            "test: 0.938\n",
            "epoch 750 with cost 0.1806987696914577\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 760 with cost 0.18038430345474743\n",
            "train: 0.94975\n",
            "test: 0.938\n",
            "epoch 770 with cost 0.18006445641494634\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 780 with cost 0.17972750066760862\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 790 with cost 0.1793622912055191\n",
            "train: 0.94975\n",
            "test: 0.9375\n",
            "epoch 800 with cost 0.17897629633334394\n",
            "train: 0.9498333333333333\n",
            "test: 0.938\n",
            "epoch 810 with cost 0.1785524328653271\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 820 with cost 0.178114482759856\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 830 with cost 0.1776524838533345\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 840 with cost 0.17717400769478722\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 850 with cost 0.17667338666793178\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 860 with cost 0.17612797914567485\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 870 with cost 0.17555430983877715\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 880 with cost 0.17496176963723117\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 890 with cost 0.17439185782164876\n",
            "train: 0.9498333333333333\n",
            "test: 0.9385\n",
            "epoch 900 with cost 0.17380425833356475\n",
            "train: 0.9498333333333333\n",
            "test: 0.939\n",
            "epoch 910 with cost 0.17316233668657127\n",
            "train: 0.95\n",
            "test: 0.939\n",
            "epoch 920 with cost 0.17243172368725496\n",
            "train: 0.9503333333333334\n",
            "test: 0.939\n",
            "epoch 930 with cost 0.17162811197693179\n",
            "train: 0.9504166666666667\n",
            "test: 0.94\n",
            "epoch 940 with cost 0.17083657717350387\n",
            "train: 0.9505833333333333\n",
            "test: 0.9405\n",
            "epoch 950 with cost 0.1700729248278461\n",
            "train: 0.95075\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.16932735637589208\n",
            "train: 0.9509166666666666\n",
            "test: 0.9405\n",
            "epoch 970 with cost 0.16859314431081102\n",
            "train: 0.95125\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.16785928356436836\n",
            "train: 0.9514166666666667\n",
            "test: 0.9405\n",
            "epoch 990 with cost 0.16705609907695887\n",
            "train: 0.9515\n",
            "test: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3mztup8zhYw"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used xavier\n",
        "# assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9255\n",
        "# if you used he\n",
        "assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9405"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cR9l1cxie3T"
      },
      "source": [
        "# Learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itxh7L79idt_"
      },
      "source": [
        "def lr_scheduling(lr, epoch, schedule = 'step_decay'):\n",
        "\n",
        "  if schedule=='step_decay':\n",
        "    #TO DO, every 50 epochs divide lr by 2\n",
        "    if epoch%50==0:\n",
        "      lr = lr/2\n",
        "  elif schedule=='exponential_decay':\n",
        "    #TO DO, multiply rl every epoch by exp(k), where k = 0.01\n",
        "    k = 0.01\n",
        "    lr = lr * np.exp(-k)\n",
        "  else:\n",
        "    print('No scheduler, please define a correct scheduler!')\n",
        "\n",
        "  return lr"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9To5w026l8cO"
      },
      "source": [
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'step_decay')\n",
        "assert lr==0.00625\n",
        "\n",
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'exponential_decay')\n",
        "assert np.round(lr,4)==0.0135"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyY3lT0K8KKk"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gft-lLiF8PcO"
      },
      "source": [
        "def dropout_forward(a, keep_prob):\n",
        "    dr = np.random.rand(a.shape[0], a.shape[1])                   # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
        "    dr = (dr < keep_prob)                                         # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
        "    a = a*dr                                                      # Step 3: shut down some neurons of A1\n",
        "    a = a/keep_prob     \n",
        "    return a, dr"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRAoEAQn8tUC"
      },
      "source": [
        "def dropout_backward(da, dr, keep_prob):\n",
        "    da = da*dr              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
        "    da = da/keep_prob       # Step 2: Scale the value of neurons that haven't been shut down\n",
        "    return da"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD346J1Z_eWt"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural_dr(X, y, n_layer_1, lr = 0.01, epochs = 100, keep_prob=1.0):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course). \n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = init_params_xavier(n_in, n_layer_1)\n",
        "    w2, b2 = init_params_xavier(n_layer_1, n_out)\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a1, dr = dropout_forward(a1, keep_prob)\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "        \n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        da1 = np.dot((dz2),w2.T)\n",
        "        da1 = dropout_backward(da1,dr,keep_prob)\n",
        "        dz1 = da1*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "        \n",
        "        if i%10==0:\n",
        "            \n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZPqVszmBG08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04be174a-0a57-48d2-c168-d2d0cb7d97f8"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural_dr(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000, 0.8)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 with cost 0.7400915087985281\n",
            "train: 0.5015\n",
            "test: 0.5\n",
            "epoch 10 with cost 0.3479958667942149\n",
            "train: 0.88675\n",
            "test: 0.883\n",
            "epoch 20 with cost 0.28823039171222914\n",
            "train: 0.8973333333333333\n",
            "test: 0.8935\n",
            "epoch 30 with cost 0.26628643339401936\n",
            "train: 0.9026666666666666\n",
            "test: 0.9035\n",
            "epoch 40 with cost 0.250557733758381\n",
            "train: 0.9058333333333334\n",
            "test: 0.9065\n",
            "epoch 50 with cost 0.24166767823940236\n",
            "train: 0.9090833333333334\n",
            "test: 0.913\n",
            "epoch 60 with cost 0.23348936992067992\n",
            "train: 0.9135\n",
            "test: 0.9165\n",
            "epoch 70 with cost 0.22803327412241614\n",
            "train: 0.9165\n",
            "test: 0.92\n",
            "epoch 80 with cost 0.22259621810282093\n",
            "train: 0.9174166666666667\n",
            "test: 0.9225\n",
            "epoch 90 with cost 0.21845386209811007\n",
            "train: 0.92\n",
            "test: 0.923\n",
            "epoch 100 with cost 0.21576787803067748\n",
            "train: 0.9215833333333333\n",
            "test: 0.9245\n",
            "epoch 110 with cost 0.21476061539802085\n",
            "train: 0.9211666666666667\n",
            "test: 0.9255\n",
            "epoch 120 with cost 0.2098592280641482\n",
            "train: 0.9239166666666667\n",
            "test: 0.9265\n",
            "epoch 130 with cost 0.20793163021058145\n",
            "train: 0.9269166666666667\n",
            "test: 0.928\n",
            "epoch 140 with cost 0.2041850112826872\n",
            "train: 0.9290833333333334\n",
            "test: 0.928\n",
            "epoch 150 with cost 0.20364047869641225\n",
            "train: 0.928\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.20391931558560314\n",
            "train: 0.9260833333333334\n",
            "test: 0.929\n",
            "epoch 170 with cost 0.20079871888781917\n",
            "train: 0.9313333333333333\n",
            "test: 0.931\n",
            "epoch 180 with cost 0.20047685375853275\n",
            "train: 0.9305833333333333\n",
            "test: 0.931\n",
            "epoch 190 with cost 0.1971891857897493\n",
            "train: 0.9319166666666666\n",
            "test: 0.931\n",
            "epoch 200 with cost 0.1960601575825789\n",
            "train: 0.9331666666666667\n",
            "test: 0.9315\n",
            "epoch 210 with cost 0.19409770949770616\n",
            "train: 0.9323333333333333\n",
            "test: 0.9315\n",
            "epoch 220 with cost 0.19330190049447998\n",
            "train: 0.93175\n",
            "test: 0.9335\n",
            "epoch 230 with cost 0.19283954575623832\n",
            "train: 0.93325\n",
            "test: 0.9325\n",
            "epoch 240 with cost 0.1914635123173751\n",
            "train: 0.9360833333333334\n",
            "test: 0.9325\n",
            "epoch 250 with cost 0.1914981055108382\n",
            "train: 0.9368333333333333\n",
            "test: 0.9325\n",
            "epoch 260 with cost 0.19183267750433752\n",
            "train: 0.9341666666666667\n",
            "test: 0.9325\n",
            "epoch 270 with cost 0.18793434697465855\n",
            "train: 0.93875\n",
            "test: 0.933\n",
            "epoch 280 with cost 0.18872658786143082\n",
            "train: 0.937\n",
            "test: 0.9335\n",
            "epoch 290 with cost 0.18753095962872093\n",
            "train: 0.938\n",
            "test: 0.9345\n",
            "epoch 300 with cost 0.18721509989356377\n",
            "train: 0.9391666666666667\n",
            "test: 0.935\n",
            "epoch 310 with cost 0.18698926655900122\n",
            "train: 0.9374166666666667\n",
            "test: 0.9345\n",
            "epoch 320 with cost 0.1872712765745987\n",
            "train: 0.9384166666666667\n",
            "test: 0.934\n",
            "epoch 330 with cost 0.18728192228939444\n",
            "train: 0.9391666666666667\n",
            "test: 0.9345\n",
            "epoch 340 with cost 0.18555743275638534\n",
            "train: 0.9400833333333334\n",
            "test: 0.9355\n",
            "epoch 350 with cost 0.18545910568530252\n",
            "train: 0.9400833333333334\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.183726484273947\n",
            "train: 0.94325\n",
            "test: 0.9355\n",
            "epoch 370 with cost 0.18544836842599852\n",
            "train: 0.9396666666666667\n",
            "test: 0.9355\n",
            "epoch 380 with cost 0.18519979865596034\n",
            "train: 0.9424166666666667\n",
            "test: 0.936\n",
            "epoch 390 with cost 0.18486346419856814\n",
            "train: 0.94125\n",
            "test: 0.935\n",
            "epoch 400 with cost 0.18491676518289973\n",
            "train: 0.9421666666666667\n",
            "test: 0.9345\n",
            "epoch 410 with cost 0.1840048668786882\n",
            "train: 0.9425\n",
            "test: 0.935\n",
            "epoch 420 with cost 0.1834255291782021\n",
            "train: 0.9424166666666667\n",
            "test: 0.936\n",
            "epoch 430 with cost 0.1828084948508718\n",
            "train: 0.9435833333333333\n",
            "test: 0.936\n",
            "epoch 440 with cost 0.1834245095283503\n",
            "train: 0.9429166666666666\n",
            "test: 0.9365\n",
            "epoch 450 with cost 0.18285938175339803\n",
            "train: 0.9425833333333333\n",
            "test: 0.936\n",
            "epoch 460 with cost 0.18338588555377477\n",
            "train: 0.9446666666666667\n",
            "test: 0.9375\n",
            "epoch 470 with cost 0.18343810203323574\n",
            "train: 0.9455\n",
            "test: 0.9375\n",
            "epoch 480 with cost 0.18323952589309234\n",
            "train: 0.945\n",
            "test: 0.9375\n",
            "epoch 490 with cost 0.1834277469233904\n",
            "train: 0.9465833333333333\n",
            "test: 0.9375\n",
            "epoch 500 with cost 0.18315556949087036\n",
            "train: 0.9466666666666667\n",
            "test: 0.9375\n",
            "epoch 510 with cost 0.1832438198911123\n",
            "train: 0.9439166666666666\n",
            "test: 0.937\n",
            "epoch 520 with cost 0.1834747939051392\n",
            "train: 0.9448333333333333\n",
            "test: 0.9365\n",
            "epoch 530 with cost 0.18307581250332244\n",
            "train: 0.9460833333333334\n",
            "test: 0.9365\n",
            "epoch 540 with cost 0.1828712246200538\n",
            "train: 0.9478333333333333\n",
            "test: 0.937\n",
            "epoch 550 with cost 0.1848253782471425\n",
            "train: 0.9480833333333333\n",
            "test: 0.9375\n",
            "epoch 560 with cost 0.18374448541358732\n",
            "train: 0.9475\n",
            "test: 0.937\n",
            "epoch 570 with cost 0.18360233433098103\n",
            "train: 0.947\n",
            "test: 0.936\n",
            "epoch 580 with cost 0.1842666088799611\n",
            "train: 0.9465\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.184182369811633\n",
            "train: 0.947\n",
            "test: 0.936\n",
            "epoch 600 with cost 0.18483407382126119\n",
            "train: 0.9464166666666667\n",
            "test: 0.936\n",
            "epoch 610 with cost 0.18266241865162464\n",
            "train: 0.94725\n",
            "test: 0.9365\n",
            "epoch 620 with cost 0.18312324081710485\n",
            "train: 0.9485833333333333\n",
            "test: 0.9365\n",
            "epoch 630 with cost 0.18186164595134172\n",
            "train: 0.94925\n",
            "test: 0.9375\n",
            "epoch 640 with cost 0.18247453663441013\n",
            "train: 0.9484166666666667\n",
            "test: 0.9375\n",
            "epoch 650 with cost 0.1825617333653253\n",
            "train: 0.9485\n",
            "test: 0.9375\n",
            "epoch 660 with cost 0.18244548168825012\n",
            "train: 0.94775\n",
            "test: 0.938\n",
            "epoch 670 with cost 0.18268380917297905\n",
            "train: 0.9478333333333333\n",
            "test: 0.938\n",
            "epoch 680 with cost 0.18289735781984495\n",
            "train: 0.94775\n",
            "test: 0.938\n",
            "epoch 690 with cost 0.1823929361447015\n",
            "train: 0.9481666666666667\n",
            "test: 0.938\n",
            "epoch 700 with cost 0.18181678108838234\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 710 with cost 0.18168409224313475\n",
            "train: 0.9481666666666667\n",
            "test: 0.9375\n",
            "epoch 720 with cost 0.18224436069550218\n",
            "train: 0.9485\n",
            "test: 0.937\n",
            "epoch 730 with cost 0.18309403021443502\n",
            "train: 0.9488333333333333\n",
            "test: 0.938\n",
            "epoch 740 with cost 0.1823026758602279\n",
            "train: 0.9479166666666666\n",
            "test: 0.9375\n",
            "epoch 750 with cost 0.1816001603129512\n",
            "train: 0.9495833333333333\n",
            "test: 0.937\n",
            "epoch 760 with cost 0.18143283942262833\n",
            "train: 0.9485\n",
            "test: 0.9385\n",
            "epoch 770 with cost 0.18082240809013167\n",
            "train: 0.9493333333333334\n",
            "test: 0.938\n",
            "epoch 780 with cost 0.18122135171558257\n",
            "train: 0.94875\n",
            "test: 0.938\n",
            "epoch 790 with cost 0.18042013981277022\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 800 with cost 0.17899789908542285\n",
            "train: 0.9490833333333333\n",
            "test: 0.938\n",
            "epoch 810 with cost 0.17973305163459227\n",
            "train: 0.9485\n",
            "test: 0.938\n",
            "epoch 820 with cost 0.1801940207509119\n",
            "train: 0.9483333333333334\n",
            "test: 0.938\n",
            "epoch 830 with cost 0.17877943483652942\n",
            "train: 0.9491666666666667\n",
            "test: 0.938\n",
            "epoch 840 with cost 0.1784370454514955\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 850 with cost 0.17769319723983687\n",
            "train: 0.9483333333333334\n",
            "test: 0.938\n",
            "epoch 860 with cost 0.1786531741263652\n",
            "train: 0.9485\n",
            "test: 0.9385\n",
            "epoch 870 with cost 0.17788406603020934\n",
            "train: 0.9489166666666666\n",
            "test: 0.9385\n",
            "epoch 880 with cost 0.17703309632713018\n",
            "train: 0.949\n",
            "test: 0.939\n",
            "epoch 890 with cost 0.17761662004256493\n",
            "train: 0.94925\n",
            "test: 0.939\n",
            "epoch 900 with cost 0.176893385639643\n",
            "train: 0.9484166666666667\n",
            "test: 0.939\n",
            "epoch 910 with cost 0.17553763832962832\n",
            "train: 0.9486666666666667\n",
            "test: 0.939\n",
            "epoch 920 with cost 0.17519395172989569\n",
            "train: 0.9495\n",
            "test: 0.9395\n",
            "epoch 930 with cost 0.17473615742571783\n",
            "train: 0.9501666666666667\n",
            "test: 0.94\n",
            "epoch 940 with cost 0.17391375068344345\n",
            "train: 0.9505\n",
            "test: 0.94\n",
            "epoch 950 with cost 0.17309154455653497\n",
            "train: 0.9494166666666667\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.1740266973195349\n",
            "train: 0.9490833333333333\n",
            "test: 0.9405\n",
            "epoch 970 with cost 0.17232282211101357\n",
            "train: 0.9500833333333333\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.17105886298463963\n",
            "train: 0.9515833333333333\n",
            "test: 0.9405\n",
            "epoch 990 with cost 0.16969248218988056\n",
            "train: 0.9503333333333334\n",
            "test: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvWE3BNBN8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e061df-b563-4226-bb58-71f05731e8f8"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used xavier\n",
        "np.mean(np.round(a2t.reshape(-1))==y_test_binary)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9405"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16PSfYXDoqui"
      },
      "source": [
        "# Neural Network Binary (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAQd0oGotCA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ghY0rEo6CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab7400e-2490-4a18-9acb-7be018c2ab72"
      },
      "source": [
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_1 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs, \n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 4s 7ms/step - loss: 0.4139 - accuracy: 0.8523 - val_loss: 0.3127 - val_accuracy: 0.8840\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2724 - accuracy: 0.8995 - val_loss: 0.2640 - val_accuracy: 0.8945\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2373 - accuracy: 0.9098 - val_loss: 0.2397 - val_accuracy: 0.9055\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2187 - accuracy: 0.9158 - val_loss: 0.2258 - val_accuracy: 0.9130\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2067 - accuracy: 0.9191 - val_loss: 0.2151 - val_accuracy: 0.9200\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9222 - val_loss: 0.2095 - val_accuracy: 0.9230\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9241 - val_loss: 0.2024 - val_accuracy: 0.9260\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9276 - val_loss: 0.1979 - val_accuracy: 0.9295\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1825 - accuracy: 0.9283 - val_loss: 0.1946 - val_accuracy: 0.9320\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9300 - val_loss: 0.1928 - val_accuracy: 0.9300\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9324 - val_loss: 0.1892 - val_accuracy: 0.9315\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9331 - val_loss: 0.1881 - val_accuracy: 0.9325\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1721 - accuracy: 0.9342 - val_loss: 0.1858 - val_accuracy: 0.9305\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1701 - accuracy: 0.9343 - val_loss: 0.1842 - val_accuracy: 0.9300\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1680 - accuracy: 0.9358 - val_loss: 0.1841 - val_accuracy: 0.9295\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1666 - accuracy: 0.9357 - val_loss: 0.1811 - val_accuracy: 0.9320\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1653 - accuracy: 0.9369 - val_loss: 0.1803 - val_accuracy: 0.9375\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9376 - val_loss: 0.1788 - val_accuracy: 0.9385\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.9382 - val_loss: 0.1778 - val_accuracy: 0.9370\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9384 - val_loss: 0.1770 - val_accuracy: 0.9330\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9391 - val_loss: 0.1759 - val_accuracy: 0.9350\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9392 - val_loss: 0.1751 - val_accuracy: 0.9365\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1580 - accuracy: 0.9396 - val_loss: 0.1748 - val_accuracy: 0.9320\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1573 - accuracy: 0.9394 - val_loss: 0.1735 - val_accuracy: 0.9360\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9397 - val_loss: 0.1736 - val_accuracy: 0.9325\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9407 - val_loss: 0.1721 - val_accuracy: 0.9375\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.9418 - val_loss: 0.1716 - val_accuracy: 0.9355\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9417 - val_loss: 0.1711 - val_accuracy: 0.9355\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1529 - accuracy: 0.9424 - val_loss: 0.1704 - val_accuracy: 0.9370\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9414 - val_loss: 0.1700 - val_accuracy: 0.9360\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9428 - val_loss: 0.1694 - val_accuracy: 0.9365\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9433 - val_loss: 0.1710 - val_accuracy: 0.9375\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9427 - val_loss: 0.1683 - val_accuracy: 0.9365\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9444 - val_loss: 0.1686 - val_accuracy: 0.9330\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9439 - val_loss: 0.1674 - val_accuracy: 0.9390\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9431 - val_loss: 0.1673 - val_accuracy: 0.9355\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.9442 - val_loss: 0.1670 - val_accuracy: 0.9350\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9442 - val_loss: 0.1673 - val_accuracy: 0.9370\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1459 - accuracy: 0.9451 - val_loss: 0.1656 - val_accuracy: 0.9380\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9448 - val_loss: 0.1670 - val_accuracy: 0.9340\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9455 - val_loss: 0.1650 - val_accuracy: 0.9385\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9456 - val_loss: 0.1644 - val_accuracy: 0.9365\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9456 - val_loss: 0.1640 - val_accuracy: 0.9385\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9463 - val_loss: 0.1638 - val_accuracy: 0.9380\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9448 - val_loss: 0.1635 - val_accuracy: 0.9380\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9460 - val_loss: 0.1661 - val_accuracy: 0.9370\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.9459 - val_loss: 0.1626 - val_accuracy: 0.9380\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1412 - accuracy: 0.9468 - val_loss: 0.1631 - val_accuracy: 0.9360\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9462 - val_loss: 0.1627 - val_accuracy: 0.9390\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9468 - val_loss: 0.1617 - val_accuracy: 0.9370\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9467 - val_loss: 0.1618 - val_accuracy: 0.9390\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9471 - val_loss: 0.1610 - val_accuracy: 0.9385\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9476 - val_loss: 0.1613 - val_accuracy: 0.9365\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9476 - val_loss: 0.1603 - val_accuracy: 0.9390\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1382 - accuracy: 0.9478 - val_loss: 0.1603 - val_accuracy: 0.9385\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9493 - val_loss: 0.1602 - val_accuracy: 0.9385\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9476 - val_loss: 0.1620 - val_accuracy: 0.9375\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9478 - val_loss: 0.1612 - val_accuracy: 0.9370\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9482 - val_loss: 0.1596 - val_accuracy: 0.9380\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1359 - accuracy: 0.9488 - val_loss: 0.1592 - val_accuracy: 0.9390\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1354 - accuracy: 0.9489 - val_loss: 0.1589 - val_accuracy: 0.9400\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9493 - val_loss: 0.1606 - val_accuracy: 0.9375\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9488 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9485 - val_loss: 0.1581 - val_accuracy: 0.9410\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9487 - val_loss: 0.1579 - val_accuracy: 0.9410\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1338 - accuracy: 0.9499 - val_loss: 0.1583 - val_accuracy: 0.9400\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9507 - val_loss: 0.1581 - val_accuracy: 0.9400\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1326 - accuracy: 0.9512 - val_loss: 0.1572 - val_accuracy: 0.9405\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1326 - accuracy: 0.9494 - val_loss: 0.1577 - val_accuracy: 0.9400\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9514 - val_loss: 0.1566 - val_accuracy: 0.9405\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1311 - accuracy: 0.9507 - val_loss: 0.1563 - val_accuracy: 0.9425\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9513 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9506 - val_loss: 0.1583 - val_accuracy: 0.9395\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9508 - val_loss: 0.1559 - val_accuracy: 0.9410\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.9511 - val_loss: 0.1555 - val_accuracy: 0.9430\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9503 - val_loss: 0.1559 - val_accuracy: 0.9405\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9513 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1293 - accuracy: 0.9523 - val_loss: 0.1551 - val_accuracy: 0.9405\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9523 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9523 - val_loss: 0.1551 - val_accuracy: 0.9400\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9528 - val_loss: 0.1543 - val_accuracy: 0.9435\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1274 - accuracy: 0.9530 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9537 - val_loss: 0.1543 - val_accuracy: 0.9400\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1270 - accuracy: 0.9531 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9542 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9533 - val_loss: 0.1541 - val_accuracy: 0.9420\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9544 - val_loss: 0.1536 - val_accuracy: 0.9405\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9538 - val_loss: 0.1531 - val_accuracy: 0.9430\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9544 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9547 - val_loss: 0.1532 - val_accuracy: 0.9415\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9539 - val_loss: 0.1529 - val_accuracy: 0.9435\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9542 - val_loss: 0.1539 - val_accuracy: 0.9435\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1244 - accuracy: 0.9536 - val_loss: 0.1518 - val_accuracy: 0.9435\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9546 - val_loss: 0.1521 - val_accuracy: 0.9430\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1238 - accuracy: 0.9547 - val_loss: 0.1521 - val_accuracy: 0.9440\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1232 - accuracy: 0.9546 - val_loss: 0.1511 - val_accuracy: 0.9435\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9555 - val_loss: 0.1517 - val_accuracy: 0.9430\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9560 - val_loss: 0.1536 - val_accuracy: 0.9415\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9567 - val_loss: 0.1505 - val_accuracy: 0.9435\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9553 - val_loss: 0.1505 - val_accuracy: 0.9445\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9555 - val_loss: 0.1505 - val_accuracy: 0.9430\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9559 - val_loss: 0.1503 - val_accuracy: 0.9445\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9567 - val_loss: 0.1499 - val_accuracy: 0.9450\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9574 - val_loss: 0.1504 - val_accuracy: 0.9440\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9569 - val_loss: 0.1495 - val_accuracy: 0.9450\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9566 - val_loss: 0.1492 - val_accuracy: 0.9455\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9565 - val_loss: 0.1491 - val_accuracy: 0.9455\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9570 - val_loss: 0.1505 - val_accuracy: 0.9445\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9585 - val_loss: 0.1493 - val_accuracy: 0.9450\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9577 - val_loss: 0.1492 - val_accuracy: 0.9465\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.9582 - val_loss: 0.1481 - val_accuracy: 0.9465\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9580 - val_loss: 0.1481 - val_accuracy: 0.9460\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9571 - val_loss: 0.1480 - val_accuracy: 0.9465\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9590 - val_loss: 0.1488 - val_accuracy: 0.9465\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9580 - val_loss: 0.1493 - val_accuracy: 0.9450\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9584 - val_loss: 0.1474 - val_accuracy: 0.9475\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9584 - val_loss: 0.1471 - val_accuracy: 0.9465\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1159 - accuracy: 0.9592 - val_loss: 0.1471 - val_accuracy: 0.9470\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9596 - val_loss: 0.1495 - val_accuracy: 0.9440\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.1465 - val_accuracy: 0.9470\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9590 - val_loss: 0.1462 - val_accuracy: 0.9475\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9595 - val_loss: 0.1460 - val_accuracy: 0.9480\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1145 - accuracy: 0.9590 - val_loss: 0.1460 - val_accuracy: 0.9490\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9592 - val_loss: 0.1455 - val_accuracy: 0.9480\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9596 - val_loss: 0.1462 - val_accuracy: 0.9485\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1135 - accuracy: 0.9592 - val_loss: 0.1455 - val_accuracy: 0.9485\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9605 - val_loss: 0.1450 - val_accuracy: 0.9480\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9604 - val_loss: 0.1450 - val_accuracy: 0.9490\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9601 - val_loss: 0.1446 - val_accuracy: 0.9485\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9607 - val_loss: 0.1443 - val_accuracy: 0.9485\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9602 - val_loss: 0.1445 - val_accuracy: 0.9485\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9603 - val_loss: 0.1442 - val_accuracy: 0.9480\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9604 - val_loss: 0.1439 - val_accuracy: 0.9490\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9612 - val_loss: 0.1440 - val_accuracy: 0.9480\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9612 - val_loss: 0.1439 - val_accuracy: 0.9475\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9603 - val_loss: 0.1446 - val_accuracy: 0.9475\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9624 - val_loss: 0.1427 - val_accuracy: 0.9505\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9613 - val_loss: 0.1431 - val_accuracy: 0.9495\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9616 - val_loss: 0.1426 - val_accuracy: 0.9495\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9616 - val_loss: 0.1425 - val_accuracy: 0.9490\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9620 - val_loss: 0.1437 - val_accuracy: 0.9475\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9617 - val_loss: 0.1416 - val_accuracy: 0.9510\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9614 - val_loss: 0.1423 - val_accuracy: 0.9490\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9627 - val_loss: 0.1414 - val_accuracy: 0.9505\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9621 - val_loss: 0.1418 - val_accuracy: 0.9500\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9621 - val_loss: 0.1413 - val_accuracy: 0.9495\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9623 - val_loss: 0.1410 - val_accuracy: 0.9505\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9622 - val_loss: 0.1427 - val_accuracy: 0.9475\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9621 - val_loss: 0.1411 - val_accuracy: 0.9500\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.1401 - val_accuracy: 0.9495\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1060 - accuracy: 0.9634 - val_loss: 0.1404 - val_accuracy: 0.9495\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9631 - val_loss: 0.1427 - val_accuracy: 0.9460\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9631 - val_loss: 0.1466 - val_accuracy: 0.9490\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9631 - val_loss: 0.1421 - val_accuracy: 0.9460\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9638 - val_loss: 0.1395 - val_accuracy: 0.9515\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9628 - val_loss: 0.1391 - val_accuracy: 0.9515\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1043 - accuracy: 0.9632 - val_loss: 0.1390 - val_accuracy: 0.9530\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9640 - val_loss: 0.1407 - val_accuracy: 0.9475\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1036 - accuracy: 0.9647 - val_loss: 0.1385 - val_accuracy: 0.9510\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9647 - val_loss: 0.1386 - val_accuracy: 0.9495\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1031 - accuracy: 0.9656 - val_loss: 0.1389 - val_accuracy: 0.9485\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9645 - val_loss: 0.1376 - val_accuracy: 0.9520\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9648 - val_loss: 0.1375 - val_accuracy: 0.9525\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9646 - val_loss: 0.1373 - val_accuracy: 0.9525\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9644 - val_loss: 0.1371 - val_accuracy: 0.9520\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9657 - val_loss: 0.1370 - val_accuracy: 0.9515\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9650 - val_loss: 0.1377 - val_accuracy: 0.9495\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9655 - val_loss: 0.1388 - val_accuracy: 0.9490\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9652 - val_loss: 0.1382 - val_accuracy: 0.9505\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9657 - val_loss: 0.1378 - val_accuracy: 0.9500\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1001 - accuracy: 0.9657 - val_loss: 0.1367 - val_accuracy: 0.9505\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9658 - val_loss: 0.1358 - val_accuracy: 0.9520\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0998 - accuracy: 0.9658 - val_loss: 0.1361 - val_accuracy: 0.9515\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9671 - val_loss: 0.1357 - val_accuracy: 0.9525\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0988 - accuracy: 0.9668 - val_loss: 0.1361 - val_accuracy: 0.9510\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9667 - val_loss: 0.1354 - val_accuracy: 0.9510\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0985 - accuracy: 0.9667 - val_loss: 0.1357 - val_accuracy: 0.9515\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9667 - val_loss: 0.1351 - val_accuracy: 0.9520\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0980 - accuracy: 0.9667 - val_loss: 0.1348 - val_accuracy: 0.9515\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9672 - val_loss: 0.1348 - val_accuracy: 0.9515\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9678 - val_loss: 0.1343 - val_accuracy: 0.9515\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9673 - val_loss: 0.1341 - val_accuracy: 0.9525\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9668 - val_loss: 0.1342 - val_accuracy: 0.9500\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9680 - val_loss: 0.1348 - val_accuracy: 0.9515\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9680 - val_loss: 0.1395 - val_accuracy: 0.9470\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9680 - val_loss: 0.1337 - val_accuracy: 0.9515\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9677 - val_loss: 0.1338 - val_accuracy: 0.9520\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0955 - accuracy: 0.9688 - val_loss: 0.1333 - val_accuracy: 0.9525\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9683 - val_loss: 0.1352 - val_accuracy: 0.9525\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.9682 - val_loss: 0.1340 - val_accuracy: 0.9505\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0949 - accuracy: 0.9685 - val_loss: 0.1347 - val_accuracy: 0.9525\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9687 - val_loss: 0.1348 - val_accuracy: 0.9525\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0940 - accuracy: 0.9680 - val_loss: 0.1322 - val_accuracy: 0.9515\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9693 - val_loss: 0.1332 - val_accuracy: 0.9540\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.9687 - val_loss: 0.1331 - val_accuracy: 0.9520\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.9694 - val_loss: 0.1323 - val_accuracy: 0.9540\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9684 - val_loss: 0.1318 - val_accuracy: 0.9515\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9686 - val_loss: 0.1318 - val_accuracy: 0.9520\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0928 - accuracy: 0.9693 - val_loss: 0.1316 - val_accuracy: 0.9515\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9688 - val_loss: 0.1313 - val_accuracy: 0.9520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTbTkk3Fpu1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c48ed6-4697-4fa3-8634-4aaac7a9dee8"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_2 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs, \n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.4285 - accuracy: 0.8330 - val_loss: 0.3137 - val_accuracy: 0.8820\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2834 - accuracy: 0.8942 - val_loss: 0.2659 - val_accuracy: 0.8940\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2503 - accuracy: 0.9016 - val_loss: 0.2419 - val_accuracy: 0.9035\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.9091 - val_loss: 0.2281 - val_accuracy: 0.9120\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9143 - val_loss: 0.2175 - val_accuracy: 0.9190\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9178 - val_loss: 0.2118 - val_accuracy: 0.9215\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.2034 - accuracy: 0.9188 - val_loss: 0.2054 - val_accuracy: 0.9235\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9236 - val_loss: 0.2007 - val_accuracy: 0.9270\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1917 - accuracy: 0.9247 - val_loss: 0.1974 - val_accuracy: 0.9295\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1891 - accuracy: 0.9254 - val_loss: 0.1948 - val_accuracy: 0.9295\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1869 - accuracy: 0.9262 - val_loss: 0.1916 - val_accuracy: 0.9305\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1849 - accuracy: 0.9302 - val_loss: 0.1909 - val_accuracy: 0.9300\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9287 - val_loss: 0.1880 - val_accuracy: 0.9315\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1792 - accuracy: 0.9299 - val_loss: 0.1864 - val_accuracy: 0.9315\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.9308 - val_loss: 0.1854 - val_accuracy: 0.9305\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9317 - val_loss: 0.1830 - val_accuracy: 0.9330\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.9319 - val_loss: 0.1821 - val_accuracy: 0.9370\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.9326 - val_loss: 0.1804 - val_accuracy: 0.9365\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.9351 - val_loss: 0.1795 - val_accuracy: 0.9340\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1686 - accuracy: 0.9347 - val_loss: 0.1788 - val_accuracy: 0.9330\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.9351 - val_loss: 0.1775 - val_accuracy: 0.9345\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9349 - val_loss: 0.1766 - val_accuracy: 0.9365\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9360 - val_loss: 0.1761 - val_accuracy: 0.9340\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9362 - val_loss: 0.1750 - val_accuracy: 0.9365\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.9377 - val_loss: 0.1746 - val_accuracy: 0.9330\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9380 - val_loss: 0.1732 - val_accuracy: 0.9370\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9390 - val_loss: 0.1726 - val_accuracy: 0.9365\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9398 - val_loss: 0.1722 - val_accuracy: 0.9350\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.9397 - val_loss: 0.1714 - val_accuracy: 0.9365\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9406 - val_loss: 0.1707 - val_accuracy: 0.9360\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9393 - val_loss: 0.1701 - val_accuracy: 0.9355\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1567 - accuracy: 0.9406 - val_loss: 0.1714 - val_accuracy: 0.9395\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9402 - val_loss: 0.1687 - val_accuracy: 0.9355\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9399 - val_loss: 0.1689 - val_accuracy: 0.9335\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.9408 - val_loss: 0.1673 - val_accuracy: 0.9375\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9426 - val_loss: 0.1674 - val_accuracy: 0.9350\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1537 - accuracy: 0.9417 - val_loss: 0.1664 - val_accuracy: 0.9370\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1520 - accuracy: 0.9426 - val_loss: 0.1664 - val_accuracy: 0.9385\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1525 - accuracy: 0.9424 - val_loss: 0.1653 - val_accuracy: 0.9375\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9419 - val_loss: 0.1662 - val_accuracy: 0.9340\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1509 - accuracy: 0.9427 - val_loss: 0.1642 - val_accuracy: 0.9370\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9425 - val_loss: 0.1638 - val_accuracy: 0.9375\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1481 - accuracy: 0.9438 - val_loss: 0.1632 - val_accuracy: 0.9375\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9441 - val_loss: 0.1629 - val_accuracy: 0.9380\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9438 - val_loss: 0.1622 - val_accuracy: 0.9380\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9435 - val_loss: 0.1636 - val_accuracy: 0.9385\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1463 - accuracy: 0.9440 - val_loss: 0.1615 - val_accuracy: 0.9375\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9452 - val_loss: 0.1613 - val_accuracy: 0.9385\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9439 - val_loss: 0.1609 - val_accuracy: 0.9380\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1443 - accuracy: 0.9450 - val_loss: 0.1597 - val_accuracy: 0.9395\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1424 - accuracy: 0.9470 - val_loss: 0.1603 - val_accuracy: 0.9385\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1446 - accuracy: 0.9449 - val_loss: 0.1587 - val_accuracy: 0.9390\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9461 - val_loss: 0.1587 - val_accuracy: 0.9370\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1410 - accuracy: 0.9479 - val_loss: 0.1578 - val_accuracy: 0.9390\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1416 - accuracy: 0.9472 - val_loss: 0.1574 - val_accuracy: 0.9395\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9475 - val_loss: 0.1567 - val_accuracy: 0.9390\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9485 - val_loss: 0.1583 - val_accuracy: 0.9395\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9473 - val_loss: 0.1569 - val_accuracy: 0.9385\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1382 - accuracy: 0.9498 - val_loss: 0.1558 - val_accuracy: 0.9385\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9470 - val_loss: 0.1553 - val_accuracy: 0.9380\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9477 - val_loss: 0.1550 - val_accuracy: 0.9395\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9492 - val_loss: 0.1554 - val_accuracy: 0.9380\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9488 - val_loss: 0.1542 - val_accuracy: 0.9395\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.9477 - val_loss: 0.1538 - val_accuracy: 0.9385\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9496 - val_loss: 0.1535 - val_accuracy: 0.9400\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9492 - val_loss: 0.1534 - val_accuracy: 0.9410\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9509 - val_loss: 0.1527 - val_accuracy: 0.9390\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9494 - val_loss: 0.1520 - val_accuracy: 0.9390\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9505 - val_loss: 0.1517 - val_accuracy: 0.9390\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9498 - val_loss: 0.1514 - val_accuracy: 0.9405\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9505 - val_loss: 0.1513 - val_accuracy: 0.9385\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9507 - val_loss: 0.1511 - val_accuracy: 0.9420\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1316 - accuracy: 0.9515 - val_loss: 0.1528 - val_accuracy: 0.9430\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9510 - val_loss: 0.1500 - val_accuracy: 0.9410\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9542 - val_loss: 0.1495 - val_accuracy: 0.9410\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1299 - accuracy: 0.9532 - val_loss: 0.1496 - val_accuracy: 0.9400\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1306 - accuracy: 0.9530 - val_loss: 0.1493 - val_accuracy: 0.9405\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9532 - val_loss: 0.1486 - val_accuracy: 0.9415\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9534 - val_loss: 0.1481 - val_accuracy: 0.9425\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9535 - val_loss: 0.1486 - val_accuracy: 0.9410\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9528 - val_loss: 0.1476 - val_accuracy: 0.9435\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1273 - accuracy: 0.9548 - val_loss: 0.1475 - val_accuracy: 0.9410\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1258 - accuracy: 0.9542 - val_loss: 0.1468 - val_accuracy: 0.9430\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1276 - accuracy: 0.9523 - val_loss: 0.1472 - val_accuracy: 0.9440\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9543 - val_loss: 0.1469 - val_accuracy: 0.9420\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9545 - val_loss: 0.1464 - val_accuracy: 0.9440\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9547 - val_loss: 0.1456 - val_accuracy: 0.9430\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1234 - accuracy: 0.9557 - val_loss: 0.1451 - val_accuracy: 0.9440\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9543 - val_loss: 0.1463 - val_accuracy: 0.9435\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1236 - accuracy: 0.9546 - val_loss: 0.1449 - val_accuracy: 0.9435\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.9553 - val_loss: 0.1445 - val_accuracy: 0.9450\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1231 - accuracy: 0.9563 - val_loss: 0.1449 - val_accuracy: 0.9445\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9543 - val_loss: 0.1435 - val_accuracy: 0.9435\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9573 - val_loss: 0.1433 - val_accuracy: 0.9440\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9572 - val_loss: 0.1431 - val_accuracy: 0.9445\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9565 - val_loss: 0.1426 - val_accuracy: 0.9440\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9572 - val_loss: 0.1423 - val_accuracy: 0.9455\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9578 - val_loss: 0.1426 - val_accuracy: 0.9455\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9570 - val_loss: 0.1415 - val_accuracy: 0.9460\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1200 - accuracy: 0.9549 - val_loss: 0.1418 - val_accuracy: 0.9455\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9574 - val_loss: 0.1413 - val_accuracy: 0.9465\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9582 - val_loss: 0.1413 - val_accuracy: 0.9445\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 0.9571 - val_loss: 0.1406 - val_accuracy: 0.9450\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9588 - val_loss: 0.1407 - val_accuracy: 0.9465\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9581 - val_loss: 0.1402 - val_accuracy: 0.9460\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9592 - val_loss: 0.1398 - val_accuracy: 0.9470\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.9588 - val_loss: 0.1393 - val_accuracy: 0.9470\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9586 - val_loss: 0.1398 - val_accuracy: 0.9480\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9591 - val_loss: 0.1395 - val_accuracy: 0.9470\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9593 - val_loss: 0.1401 - val_accuracy: 0.9480\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9590 - val_loss: 0.1381 - val_accuracy: 0.9470\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9592 - val_loss: 0.1383 - val_accuracy: 0.9465\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9588 - val_loss: 0.1376 - val_accuracy: 0.9470\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1135 - accuracy: 0.9607 - val_loss: 0.1378 - val_accuracy: 0.9465\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1128 - accuracy: 0.9599 - val_loss: 0.1381 - val_accuracy: 0.9490\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9610 - val_loss: 0.1373 - val_accuracy: 0.9470\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9593 - val_loss: 0.1369 - val_accuracy: 0.9470\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9590 - val_loss: 0.1365 - val_accuracy: 0.9480\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1120 - accuracy: 0.9612 - val_loss: 0.1368 - val_accuracy: 0.9490\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1120 - accuracy: 0.9613 - val_loss: 0.1358 - val_accuracy: 0.9485\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1124 - accuracy: 0.9613 - val_loss: 0.1358 - val_accuracy: 0.9490\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.1353 - val_accuracy: 0.9485\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9604 - val_loss: 0.1356 - val_accuracy: 0.9490\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9602 - val_loss: 0.1347 - val_accuracy: 0.9495\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9500\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9617 - val_loss: 0.1342 - val_accuracy: 0.9495\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9629 - val_loss: 0.1342 - val_accuracy: 0.9495\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9631 - val_loss: 0.1345 - val_accuracy: 0.9495\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1082 - accuracy: 0.9626 - val_loss: 0.1338 - val_accuracy: 0.9495\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1071 - accuracy: 0.9625 - val_loss: 0.1335 - val_accuracy: 0.9510\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9638 - val_loss: 0.1345 - val_accuracy: 0.9505\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9619 - val_loss: 0.1329 - val_accuracy: 0.9490\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9631 - val_loss: 0.1329 - val_accuracy: 0.9500\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9632 - val_loss: 0.1325 - val_accuracy: 0.9495\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9631 - val_loss: 0.1321 - val_accuracy: 0.9490\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1059 - accuracy: 0.9634 - val_loss: 0.1328 - val_accuracy: 0.9500\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9631 - val_loss: 0.1315 - val_accuracy: 0.9510\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9630 - val_loss: 0.1314 - val_accuracy: 0.9530\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9646 - val_loss: 0.1312 - val_accuracy: 0.9515\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9633 - val_loss: 0.1308 - val_accuracy: 0.9505\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9640 - val_loss: 0.1318 - val_accuracy: 0.9515\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9646 - val_loss: 0.1304 - val_accuracy: 0.9520\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9649 - val_loss: 0.1305 - val_accuracy: 0.9520\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9641 - val_loss: 0.1303 - val_accuracy: 0.9520\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9639 - val_loss: 0.1304 - val_accuracy: 0.9505\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9656 - val_loss: 0.1298 - val_accuracy: 0.9525\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9647 - val_loss: 0.1299 - val_accuracy: 0.9525\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1001 - accuracy: 0.9657 - val_loss: 0.1301 - val_accuracy: 0.9525\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9651 - val_loss: 0.1288 - val_accuracy: 0.9520\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1017 - accuracy: 0.9643 - val_loss: 0.1288 - val_accuracy: 0.9520\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9655 - val_loss: 0.1284 - val_accuracy: 0.9535\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9653 - val_loss: 0.1293 - val_accuracy: 0.9515\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.9651 - val_loss: 0.1302 - val_accuracy: 0.9525\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9651 - val_loss: 0.1294 - val_accuracy: 0.9530\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9675 - val_loss: 0.1284 - val_accuracy: 0.9520\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9657 - val_loss: 0.1276 - val_accuracy: 0.9530\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.1004 - accuracy: 0.9667 - val_loss: 0.1273 - val_accuracy: 0.9540\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0992 - accuracy: 0.9653 - val_loss: 0.1283 - val_accuracy: 0.9530\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9663 - val_loss: 0.1269 - val_accuracy: 0.9535\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9641 - val_loss: 0.1265 - val_accuracy: 0.9545\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0984 - accuracy: 0.9671 - val_loss: 0.1263 - val_accuracy: 0.9525\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9652 - val_loss: 0.1261 - val_accuracy: 0.9530\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0978 - accuracy: 0.9663 - val_loss: 0.1258 - val_accuracy: 0.9540\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9672 - val_loss: 0.1259 - val_accuracy: 0.9540\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0964 - accuracy: 0.9678 - val_loss: 0.1260 - val_accuracy: 0.9530\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9654 - val_loss: 0.1255 - val_accuracy: 0.9545\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9681 - val_loss: 0.1257 - val_accuracy: 0.9550\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9686 - val_loss: 0.1255 - val_accuracy: 0.9530\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.9666 - val_loss: 0.1252 - val_accuracy: 0.9550\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9685 - val_loss: 0.1251 - val_accuracy: 0.9555\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9701 - val_loss: 0.1254 - val_accuracy: 0.9555\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0940 - accuracy: 0.9679 - val_loss: 0.1244 - val_accuracy: 0.9545\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0952 - accuracy: 0.9682 - val_loss: 0.1247 - val_accuracy: 0.9545\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9685 - val_loss: 0.1244 - val_accuracy: 0.9550\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0937 - accuracy: 0.9692 - val_loss: 0.1247 - val_accuracy: 0.9555\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0944 - accuracy: 0.9678 - val_loss: 0.1242 - val_accuracy: 0.9555\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0945 - accuracy: 0.9680 - val_loss: 0.1237 - val_accuracy: 0.9555\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0938 - accuracy: 0.9688 - val_loss: 0.1243 - val_accuracy: 0.9540\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9678 - val_loss: 0.1237 - val_accuracy: 0.9550\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9675 - val_loss: 0.1240 - val_accuracy: 0.9540\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0931 - accuracy: 0.9688 - val_loss: 0.1227 - val_accuracy: 0.9565\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9688 - val_loss: 0.1232 - val_accuracy: 0.9545\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0937 - accuracy: 0.9686 - val_loss: 0.1228 - val_accuracy: 0.9560\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9702 - val_loss: 0.1229 - val_accuracy: 0.9560\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9688 - val_loss: 0.1248 - val_accuracy: 0.9555\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0923 - accuracy: 0.9700 - val_loss: 0.1227 - val_accuracy: 0.9555\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9697 - val_loss: 0.1226 - val_accuracy: 0.9550\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0892 - accuracy: 0.9713 - val_loss: 0.1224 - val_accuracy: 0.9565\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0905 - accuracy: 0.9691 - val_loss: 0.1224 - val_accuracy: 0.9560\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0911 - accuracy: 0.9707 - val_loss: 0.1219 - val_accuracy: 0.9570\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0904 - accuracy: 0.9694 - val_loss: 0.1226 - val_accuracy: 0.9540\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0906 - accuracy: 0.9708 - val_loss: 0.1220 - val_accuracy: 0.9545\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0896 - accuracy: 0.9701 - val_loss: 0.1209 - val_accuracy: 0.9565\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.0894 - accuracy: 0.9702 - val_loss: 0.1212 - val_accuracy: 0.9565\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9708 - val_loss: 0.1210 - val_accuracy: 0.9570\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.9697 - val_loss: 0.1209 - val_accuracy: 0.9555\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0890 - accuracy: 0.9704 - val_loss: 0.1203 - val_accuracy: 0.9570\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9707 - val_loss: 0.1204 - val_accuracy: 0.9570\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9707 - val_loss: 0.1200 - val_accuracy: 0.9575\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9691 - val_loss: 0.1201 - val_accuracy: 0.9575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZrVlg-BHkES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "07a496a9-a346-48db-ecd3-ce34c3ded6db"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_loss'])\n",
        "plt.plot(history_2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jggp33GTHkN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "cfed4a77-297a-4509-a508-c3469e7f5f5b"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_accuracy'])\n",
        "plt.plot(history_2.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lBU4uykrOqH"
      },
      "source": [
        "# Validation Set (The CORRECT way of training and evaluation process)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hz7bgmgH4lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fc3b36-b431-4cbf-bd98-96f1fab81693"
      },
      "source": [
        "#cross validation example\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=2) # number of folds, shuffle, seed\n",
        "\n",
        "# enumerate splits\n",
        "for train, val in kfold.split(x_train_binary):\n",
        "\tprint('train: %s, val: %s' % (train, val))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [    0     1     3 ... 11997 11998 11999], val: [    2     4    10 ... 11991 11992 11994]\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    3     5     6 ... 11988 11989 11993]\n",
            "train: [    1     2     3 ... 11994 11995 11996], val: [    0    13    19 ... 11997 11998 11999]\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    7     9    21 ... 11948 11952 11955]\n",
            "train: [    0     2     3 ... 11997 11998 11999], val: [    1    12    14 ... 11985 11995 11996]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqB1wQNMPfQ"
      },
      "source": [
        "X_train_binary = x_train_binary[train]\n",
        "Y_train_binary = y_train_binary[train]\n",
        "X_val_binary = x_train_binary[val]\n",
        "Y_val_binary = y_train_binary[val]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVfanC2yMwAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d74f76-ec33-4487-c722-f665af0656ea"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint #save the model version that achieved lower loss!\n",
        "\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "save_model = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_3 = model.fit(X_train_binary, Y_train_binary, batch_size=batch_size, epochs=epochs, callbacks=[save_model],\n",
        "          validation_data=(X_val_binary, Y_val_binary))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.2371 - accuracy: 0.9019\n",
            "Epoch 1: val_loss improved from inf to 0.18887, saving model to best_model.h5\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.2289 - accuracy: 0.9053 - val_loss: 0.1889 - val_accuracy: 0.9250\n",
            "Epoch 2/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.1647 - accuracy: 0.9352\n",
            "Epoch 2: val_loss improved from 0.18887 to 0.16427, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9357 - val_loss: 0.1643 - val_accuracy: 0.9383\n",
            "Epoch 3/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9459\n",
            "Epoch 3: val_loss improved from 0.16427 to 0.14632, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9457 - val_loss: 0.1463 - val_accuracy: 0.9425\n",
            "Epoch 4/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.1381 - accuracy: 0.9472\n",
            "Epoch 4: val_loss improved from 0.14632 to 0.13803, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9458 - val_loss: 0.1380 - val_accuracy: 0.9467\n",
            "Epoch 5/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.1227 - accuracy: 0.9545\n",
            "Epoch 5: val_loss improved from 0.13803 to 0.13390, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9555 - val_loss: 0.1339 - val_accuracy: 0.9546\n",
            "Epoch 6/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.1109 - accuracy: 0.9596\n",
            "Epoch 6: val_loss improved from 0.13390 to 0.12603, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 0.9585 - val_loss: 0.1260 - val_accuracy: 0.9604\n",
            "Epoch 7/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9627\n",
            "Epoch 7: val_loss improved from 0.12603 to 0.12371, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9627 - val_loss: 0.1237 - val_accuracy: 0.9538\n",
            "Epoch 8/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9665\n",
            "Epoch 8: val_loss improved from 0.12371 to 0.11473, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9665 - val_loss: 0.1147 - val_accuracy: 0.9608\n",
            "Epoch 9/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0975 - accuracy: 0.9652\n",
            "Epoch 9: val_loss improved from 0.11473 to 0.11255, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0938 - accuracy: 0.9673 - val_loss: 0.1126 - val_accuracy: 0.9633\n",
            "Epoch 10/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0864 - accuracy: 0.9691\n",
            "Epoch 10: val_loss did not improve from 0.11255\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9693 - val_loss: 0.1173 - val_accuracy: 0.9596\n",
            "Epoch 11/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0857 - accuracy: 0.9714\n",
            "Epoch 11: val_loss did not improve from 0.11255\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9718 - val_loss: 0.1196 - val_accuracy: 0.9600\n",
            "Epoch 12/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0801 - accuracy: 0.9732\n",
            "Epoch 12: val_loss improved from 0.11255 to 0.11213, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9741 - val_loss: 0.1121 - val_accuracy: 0.9629\n",
            "Epoch 13/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9753\n",
            "Epoch 13: val_loss improved from 0.11213 to 0.10852, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9754 - val_loss: 0.1085 - val_accuracy: 0.9642\n",
            "Epoch 14/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0747 - accuracy: 0.9763\n",
            "Epoch 14: val_loss did not improve from 0.10852\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9766 - val_loss: 0.1116 - val_accuracy: 0.9633\n",
            "Epoch 15/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0703 - accuracy: 0.9762\n",
            "Epoch 15: val_loss did not improve from 0.10852\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9766 - val_loss: 0.1096 - val_accuracy: 0.9671\n",
            "Epoch 16/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9771\n",
            "Epoch 16: val_loss improved from 0.10852 to 0.10435, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.9771 - val_loss: 0.1044 - val_accuracy: 0.9700\n",
            "Epoch 17/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0641 - accuracy: 0.9773\n",
            "Epoch 17: val_loss improved from 0.10435 to 0.10184, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0664 - accuracy: 0.9773 - val_loss: 0.1018 - val_accuracy: 0.9675\n",
            "Epoch 18/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0677 - accuracy: 0.9760\n",
            "Epoch 18: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9771 - val_loss: 0.1112 - val_accuracy: 0.9688\n",
            "Epoch 19/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9778\n",
            "Epoch 19: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.9778 - val_loss: 0.1150 - val_accuracy: 0.9646\n",
            "Epoch 20/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0642 - accuracy: 0.9780\n",
            "Epoch 20: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.9780 - val_loss: 0.1102 - val_accuracy: 0.9667\n",
            "Epoch 21/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0598 - accuracy: 0.9796\n",
            "Epoch 21: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9795 - val_loss: 0.1091 - val_accuracy: 0.9679\n",
            "Epoch 22/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0562 - accuracy: 0.9803\n",
            "Epoch 22: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9799 - val_loss: 0.1157 - val_accuracy: 0.9667\n",
            "Epoch 23/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0570 - accuracy: 0.9805\n",
            "Epoch 23: val_loss did not improve from 0.10184\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9811 - val_loss: 0.1064 - val_accuracy: 0.9679\n",
            "Epoch 24/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0504 - accuracy: 0.9829\n",
            "Epoch 24: val_loss improved from 0.10184 to 0.10033, saving model to best_model.h5\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9825 - val_loss: 0.1003 - val_accuracy: 0.9721\n",
            "Epoch 25/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0484 - accuracy: 0.9835\n",
            "Epoch 25: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 0.1032 - val_accuracy: 0.9733\n",
            "Epoch 26/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0497 - accuracy: 0.9834\n",
            "Epoch 26: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9835 - val_loss: 0.1063 - val_accuracy: 0.9712\n",
            "Epoch 27/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9836\n",
            "Epoch 27: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0482 - accuracy: 0.9836 - val_loss: 0.1101 - val_accuracy: 0.9704\n",
            "Epoch 28/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0460 - accuracy: 0.9848\n",
            "Epoch 28: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0473 - accuracy: 0.9846 - val_loss: 0.1053 - val_accuracy: 0.9717\n",
            "Epoch 29/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9846\n",
            "Epoch 29: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9843 - val_loss: 0.1067 - val_accuracy: 0.9708\n",
            "Epoch 30/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0456 - accuracy: 0.9839\n",
            "Epoch 30: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.9841 - val_loss: 0.1137 - val_accuracy: 0.9671\n",
            "Epoch 31/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9855\n",
            "Epoch 31: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 0.1066 - val_accuracy: 0.9717\n",
            "Epoch 32/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9853\n",
            "Epoch 32: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9853 - val_loss: 0.1078 - val_accuracy: 0.9717\n",
            "Epoch 33/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0397 - accuracy: 0.9881\n",
            "Epoch 33: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.9878 - val_loss: 0.1162 - val_accuracy: 0.9671\n",
            "Epoch 34/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0377 - accuracy: 0.9869\n",
            "Epoch 34: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9871 - val_loss: 0.1143 - val_accuracy: 0.9704\n",
            "Epoch 35/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0363 - accuracy: 0.9863\n",
            "Epoch 35: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9847 - val_loss: 0.1317 - val_accuracy: 0.9650\n",
            "Epoch 36/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9885\n",
            "Epoch 36: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 0.1131 - val_accuracy: 0.9721\n",
            "Epoch 37/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0348 - accuracy: 0.9890\n",
            "Epoch 37: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9883 - val_loss: 0.1262 - val_accuracy: 0.9679\n",
            "Epoch 38/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0400 - accuracy: 0.9865\n",
            "Epoch 38: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.1137 - val_accuracy: 0.9729\n",
            "Epoch 39/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9886\n",
            "Epoch 39: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.1284 - val_accuracy: 0.9667\n",
            "Epoch 40/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0330 - accuracy: 0.9898\n",
            "Epoch 40: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.1261 - val_accuracy: 0.9663\n",
            "Epoch 41/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9894\n",
            "Epoch 41: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9894 - val_loss: 0.1222 - val_accuracy: 0.9683\n",
            "Epoch 42/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0293 - accuracy: 0.9904\n",
            "Epoch 42: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: 0.1171 - val_accuracy: 0.9717\n",
            "Epoch 43/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0318 - accuracy: 0.9895\n",
            "Epoch 43: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.1192 - val_accuracy: 0.9721\n",
            "Epoch 44/200\n",
            "60/75 [=======================>......] - ETA: 0s - loss: 0.0310 - accuracy: 0.9904\n",
            "Epoch 44: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0306 - accuracy: 0.9902 - val_loss: 0.1278 - val_accuracy: 0.9671\n",
            "Epoch 45/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0272 - accuracy: 0.9927\n",
            "Epoch 45: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 0.9926 - val_loss: 0.1170 - val_accuracy: 0.9712\n",
            "Epoch 46/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9902\n",
            "Epoch 46: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0310 - accuracy: 0.9903 - val_loss: 0.1253 - val_accuracy: 0.9712\n",
            "Epoch 47/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9908\n",
            "Epoch 47: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 0.1248 - val_accuracy: 0.9712\n",
            "Epoch 48/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0267 - accuracy: 0.9914\n",
            "Epoch 48: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.1277 - val_accuracy: 0.9692\n",
            "Epoch 49/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0280 - accuracy: 0.9923\n",
            "Epoch 49: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9925 - val_loss: 0.1264 - val_accuracy: 0.9712\n",
            "Epoch 50/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9910\n",
            "Epoch 50: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.9911 - val_loss: 0.1384 - val_accuracy: 0.9683\n",
            "Epoch 51/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0273 - accuracy: 0.9915\n",
            "Epoch 51: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9914 - val_loss: 0.1261 - val_accuracy: 0.9712\n",
            "Epoch 52/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9921\n",
            "Epoch 52: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 0.9921 - val_loss: 0.1333 - val_accuracy: 0.9729\n",
            "Epoch 53/200\n",
            "60/75 [=======================>......] - ETA: 0s - loss: 0.0226 - accuracy: 0.9917\n",
            "Epoch 53: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9910 - val_loss: 0.1271 - val_accuracy: 0.9725\n",
            "Epoch 54/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0210 - accuracy: 0.9933\n",
            "Epoch 54: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.1262 - val_accuracy: 0.9733\n",
            "Epoch 55/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9942\n",
            "Epoch 55: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 0.1506 - val_accuracy: 0.9679\n",
            "Epoch 56/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0196 - accuracy: 0.9932\n",
            "Epoch 56: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 0.1302 - val_accuracy: 0.9737\n",
            "Epoch 57/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0183 - accuracy: 0.9939\n",
            "Epoch 57: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 0.1448 - val_accuracy: 0.9679\n",
            "Epoch 58/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9924\n",
            "Epoch 58: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 0.9921 - val_loss: 0.1479 - val_accuracy: 0.9671\n",
            "Epoch 59/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9938\n",
            "Epoch 59: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.1313 - val_accuracy: 0.9712\n",
            "Epoch 60/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9923\n",
            "Epoch 60: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 0.1389 - val_accuracy: 0.9721\n",
            "Epoch 61/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0217 - accuracy: 0.9921\n",
            "Epoch 61: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.9921 - val_loss: 0.1429 - val_accuracy: 0.9708\n",
            "Epoch 62/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9930\n",
            "Epoch 62: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9930 - val_loss: 0.1393 - val_accuracy: 0.9704\n",
            "Epoch 63/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 63: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.1521 - val_accuracy: 0.9712\n",
            "Epoch 64/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0187 - accuracy: 0.9930\n",
            "Epoch 64: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9929 - val_loss: 0.1401 - val_accuracy: 0.9725\n",
            "Epoch 65/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0191 - accuracy: 0.9932\n",
            "Epoch 65: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 0.1373 - val_accuracy: 0.9725\n",
            "Epoch 66/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0239 - accuracy: 0.9908\n",
            "Epoch 66: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.9910 - val_loss: 0.1477 - val_accuracy: 0.9679\n",
            "Epoch 67/200\n",
            "60/75 [=======================>......] - ETA: 0s - loss: 0.0181 - accuracy: 0.9944\n",
            "Epoch 67: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.1502 - val_accuracy: 0.9729\n",
            "Epoch 68/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0159 - accuracy: 0.9949\n",
            "Epoch 68: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.1443 - val_accuracy: 0.9708\n",
            "Epoch 69/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 69: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.1454 - val_accuracy: 0.9717\n",
            "Epoch 70/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0149 - accuracy: 0.9950\n",
            "Epoch 70: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 0.1563 - val_accuracy: 0.9696\n",
            "Epoch 71/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.9945\n",
            "Epoch 71: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9946 - val_loss: 0.1505 - val_accuracy: 0.9700\n",
            "Epoch 72/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0134 - accuracy: 0.9957\n",
            "Epoch 72: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.1580 - val_accuracy: 0.9708\n",
            "Epoch 73/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 73: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0161 - accuracy: 0.9942 - val_loss: 0.1539 - val_accuracy: 0.9708\n",
            "Epoch 74/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0138 - accuracy: 0.9958\n",
            "Epoch 74: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 0.1441 - val_accuracy: 0.9733\n",
            "Epoch 75/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9951\n",
            "Epoch 75: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.1489 - val_accuracy: 0.9733\n",
            "Epoch 76/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch 76: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.1450 - val_accuracy: 0.9721\n",
            "Epoch 77/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9968\n",
            "Epoch 77: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.1552 - val_accuracy: 0.9733\n",
            "Epoch 78/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942\n",
            "Epoch 78: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.1607 - val_accuracy: 0.9737\n",
            "Epoch 79/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9949\n",
            "Epoch 79: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.1605 - val_accuracy: 0.9725\n",
            "Epoch 80/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9950\n",
            "Epoch 80: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.1716 - val_accuracy: 0.9700\n",
            "Epoch 81/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9964\n",
            "Epoch 81: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.1639 - val_accuracy: 0.9733\n",
            "Epoch 82/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0132 - accuracy: 0.9956\n",
            "Epoch 82: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.1685 - val_accuracy: 0.9717\n",
            "Epoch 83/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
            "Epoch 83: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.1659 - val_accuracy: 0.9721\n",
            "Epoch 84/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9958\n",
            "Epoch 84: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.1663 - val_accuracy: 0.9708\n",
            "Epoch 85/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9944\n",
            "Epoch 85: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0150 - accuracy: 0.9946 - val_loss: 0.1623 - val_accuracy: 0.9721\n",
            "Epoch 86/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9964\n",
            "Epoch 86: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 0.1677 - val_accuracy: 0.9692\n",
            "Epoch 87/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
            "Epoch 87: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.1864 - val_accuracy: 0.9638\n",
            "Epoch 88/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 88: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.1755 - val_accuracy: 0.9704\n",
            "Epoch 89/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9974\n",
            "Epoch 89: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.1666 - val_accuracy: 0.9733\n",
            "Epoch 90/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9958\n",
            "Epoch 90: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.1779 - val_accuracy: 0.9712\n",
            "Epoch 91/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0104 - accuracy: 0.9973\n",
            "Epoch 91: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 0.1734 - val_accuracy: 0.9717\n",
            "Epoch 92/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9959\n",
            "Epoch 92: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 0.1797 - val_accuracy: 0.9696\n",
            "Epoch 93/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0102 - accuracy: 0.9973\n",
            "Epoch 93: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 0.1702 - val_accuracy: 0.9737\n",
            "Epoch 94/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963\n",
            "Epoch 94: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.1727 - val_accuracy: 0.9717\n",
            "Epoch 95/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9967\n",
            "Epoch 95: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.1727 - val_accuracy: 0.9737\n",
            "Epoch 96/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0098 - accuracy: 0.9979\n",
            "Epoch 96: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.1791 - val_accuracy: 0.9712\n",
            "Epoch 97/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9968\n",
            "Epoch 97: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.1849 - val_accuracy: 0.9679\n",
            "Epoch 98/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955\n",
            "Epoch 98: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.1701 - val_accuracy: 0.9729\n",
            "Epoch 99/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0085 - accuracy: 0.9969\n",
            "Epoch 99: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.1829 - val_accuracy: 0.9737\n",
            "Epoch 100/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0089 - accuracy: 0.9970\n",
            "Epoch 100: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.1932 - val_accuracy: 0.9683\n",
            "Epoch 101/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9961\n",
            "Epoch 101: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.1817 - val_accuracy: 0.9704\n",
            "Epoch 102/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9979\n",
            "Epoch 102: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.1848 - val_accuracy: 0.9704\n",
            "Epoch 103/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0078 - accuracy: 0.9977\n",
            "Epoch 103: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.1908 - val_accuracy: 0.9742\n",
            "Epoch 104/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976\n",
            "Epoch 104: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.2010 - val_accuracy: 0.9671\n",
            "Epoch 105/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9973\n",
            "Epoch 105: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.1870 - val_accuracy: 0.9717\n",
            "Epoch 106/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0101 - accuracy: 0.9967\n",
            "Epoch 106: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.1973 - val_accuracy: 0.9675\n",
            "Epoch 107/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
            "Epoch 107: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.1833 - val_accuracy: 0.9721\n",
            "Epoch 108/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0079 - accuracy: 0.9978\n",
            "Epoch 108: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.1785 - val_accuracy: 0.9688\n",
            "Epoch 109/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 109: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.1844 - val_accuracy: 0.9712\n",
            "Epoch 110/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0139 - accuracy: 0.9946\n",
            "Epoch 110: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.9944 - val_loss: 0.1861 - val_accuracy: 0.9696\n",
            "Epoch 111/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9923\n",
            "Epoch 111: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 0.9924 - val_loss: 0.1673 - val_accuracy: 0.9721\n",
            "Epoch 112/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0136 - accuracy: 0.9951\n",
            "Epoch 112: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 0.9953 - val_loss: 0.1823 - val_accuracy: 0.9708\n",
            "Epoch 113/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0105 - accuracy: 0.9968\n",
            "Epoch 113: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.1968 - val_accuracy: 0.9704\n",
            "Epoch 114/200\n",
            "61/75 [=======================>......] - ETA: 0s - loss: 0.0126 - accuracy: 0.9955\n",
            "Epoch 114: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9950 - val_loss: 0.1772 - val_accuracy: 0.9696\n",
            "Epoch 115/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n",
            "Epoch 115: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.1951 - val_accuracy: 0.9700\n",
            "Epoch 116/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0062 - accuracy: 0.9983\n",
            "Epoch 116: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.2038 - val_accuracy: 0.9717\n",
            "Epoch 117/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9979\n",
            "Epoch 117: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.1928 - val_accuracy: 0.9717\n",
            "Epoch 118/200\n",
            "60/75 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 0.9984\n",
            "Epoch 118: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.1940 - val_accuracy: 0.9712\n",
            "Epoch 119/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 119: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9968 - val_loss: 0.1985 - val_accuracy: 0.9712\n",
            "Epoch 120/200\n",
            "74/75 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9973\n",
            "Epoch 120: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 0.9972 - val_loss: 0.2035 - val_accuracy: 0.9737\n",
            "Epoch 121/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9960\n",
            "Epoch 121: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9961 - val_loss: 0.2117 - val_accuracy: 0.9683\n",
            "Epoch 122/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0056 - accuracy: 0.9987\n",
            "Epoch 122: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.1943 - val_accuracy: 0.9712\n",
            "Epoch 123/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0107 - accuracy: 0.9972\n",
            "Epoch 123: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.1973 - val_accuracy: 0.9737\n",
            "Epoch 124/200\n",
            "62/75 [=======================>......] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
            "Epoch 124: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.2101 - val_accuracy: 0.9712\n",
            "Epoch 125/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0091 - accuracy: 0.9968\n",
            "Epoch 125: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 0.2151 - val_accuracy: 0.9704\n",
            "Epoch 126/200\n",
            "72/75 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9975\n",
            "Epoch 126: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.2178 - val_accuracy: 0.9712\n",
            "Epoch 127/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0066 - accuracy: 0.9973\n",
            "Epoch 127: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.2249 - val_accuracy: 0.9704\n",
            "Epoch 128/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0094 - accuracy: 0.9969\n",
            "Epoch 128: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.2226 - val_accuracy: 0.9692\n",
            "Epoch 129/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0163 - accuracy: 0.9943\n",
            "Epoch 129: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9936 - val_loss: 0.1909 - val_accuracy: 0.9679\n",
            "Epoch 130/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0151 - accuracy: 0.9948\n",
            "Epoch 130: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.2005 - val_accuracy: 0.9712\n",
            "Epoch 131/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0046 - accuracy: 0.9990\n",
            "Epoch 131: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2031 - val_accuracy: 0.9733\n",
            "Epoch 132/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0044 - accuracy: 0.9990\n",
            "Epoch 132: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.2069 - val_accuracy: 0.9742\n",
            "Epoch 133/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 0.9986\n",
            "Epoch 133: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.2160 - val_accuracy: 0.9725\n",
            "Epoch 134/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0048 - accuracy: 0.9988\n",
            "Epoch 134: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.2084 - val_accuracy: 0.9721\n",
            "Epoch 135/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9963\n",
            "Epoch 135: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9961 - val_loss: 0.2247 - val_accuracy: 0.9700\n",
            "Epoch 136/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 0.9974\n",
            "Epoch 136: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.2053 - val_accuracy: 0.9712\n",
            "Epoch 137/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0056 - accuracy: 0.9979\n",
            "Epoch 137: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.2221 - val_accuracy: 0.9712\n",
            "Epoch 138/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0056 - accuracy: 0.9983\n",
            "Epoch 138: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2094 - val_accuracy: 0.9729\n",
            "Epoch 139/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\n",
            "Epoch 139: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.2238 - val_accuracy: 0.9708\n",
            "Epoch 140/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0046 - accuracy: 0.9989\n",
            "Epoch 140: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.2193 - val_accuracy: 0.9729\n",
            "Epoch 141/200\n",
            "71/75 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9983\n",
            "Epoch 141: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.2279 - val_accuracy: 0.9717\n",
            "Epoch 142/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0087 - accuracy: 0.9974\n",
            "Epoch 142: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.2087 - val_accuracy: 0.9725\n",
            "Epoch 143/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0102 - accuracy: 0.9964\n",
            "Epoch 143: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.2200 - val_accuracy: 0.9704\n",
            "Epoch 144/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0114 - accuracy: 0.9955\n",
            "Epoch 144: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 0.2042 - val_accuracy: 0.9712\n",
            "Epoch 145/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0059 - accuracy: 0.9985\n",
            "Epoch 145: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.2053 - val_accuracy: 0.9717\n",
            "Epoch 146/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977\n",
            "Epoch 146: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.2045 - val_accuracy: 0.9708\n",
            "Epoch 147/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
            "Epoch 147: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2235 - val_accuracy: 0.9696\n",
            "Epoch 148/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9983\n",
            "Epoch 148: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.2252 - val_accuracy: 0.9708\n",
            "Epoch 149/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
            "Epoch 149: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.2269 - val_accuracy: 0.9725\n",
            "Epoch 150/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0049 - accuracy: 0.9983\n",
            "Epoch 150: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.2307 - val_accuracy: 0.9704\n",
            "Epoch 151/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9981\n",
            "Epoch 151: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.2142 - val_accuracy: 0.9721\n",
            "Epoch 152/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9987\n",
            "Epoch 152: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.2161 - val_accuracy: 0.9729\n",
            "Epoch 153/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0035 - accuracy: 0.9992\n",
            "Epoch 153: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.2188 - val_accuracy: 0.9733\n",
            "Epoch 154/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0080 - accuracy: 0.9966\n",
            "Epoch 154: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 0.9967 - val_loss: 0.2535 - val_accuracy: 0.9692\n",
            "Epoch 155/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0062 - accuracy: 0.9978\n",
            "Epoch 155: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.2401 - val_accuracy: 0.9642\n",
            "Epoch 156/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0056 - accuracy: 0.9982\n",
            "Epoch 156: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.2583 - val_accuracy: 0.9679\n",
            "Epoch 157/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0062 - accuracy: 0.9985\n",
            "Epoch 157: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2173 - val_accuracy: 0.9729\n",
            "Epoch 158/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0045 - accuracy: 0.9983\n",
            "Epoch 158: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.2256 - val_accuracy: 0.9754\n",
            "Epoch 159/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0064 - accuracy: 0.9981\n",
            "Epoch 159: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.2265 - val_accuracy: 0.9733\n",
            "Epoch 160/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0053 - accuracy: 0.9980\n",
            "Epoch 160: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9979 - val_loss: 0.2313 - val_accuracy: 0.9725\n",
            "Epoch 161/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0062 - accuracy: 0.9983\n",
            "Epoch 161: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 0.2297 - val_accuracy: 0.9696\n",
            "Epoch 162/200\n",
            "70/75 [===========================>..] - ETA: 0s - loss: 0.0070 - accuracy: 0.9975\n",
            "Epoch 162: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.2213 - val_accuracy: 0.9712\n",
            "Epoch 163/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0063 - accuracy: 0.9985\n",
            "Epoch 163: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.2351 - val_accuracy: 0.9683\n",
            "Epoch 164/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0071 - accuracy: 0.9972\n",
            "Epoch 164: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 0.9974 - val_loss: 0.2247 - val_accuracy: 0.9733\n",
            "Epoch 165/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0058 - accuracy: 0.9981\n",
            "Epoch 165: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.2427 - val_accuracy: 0.9717\n",
            "Epoch 166/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9986\n",
            "Epoch 166: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.2449 - val_accuracy: 0.9708\n",
            "Epoch 167/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0043 - accuracy: 0.9988\n",
            "Epoch 167: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.2356 - val_accuracy: 0.9708\n",
            "Epoch 168/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0052 - accuracy: 0.9984\n",
            "Epoch 168: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.2379 - val_accuracy: 0.9733\n",
            "Epoch 169/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0082 - accuracy: 0.9965\n",
            "Epoch 169: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.9965 - val_loss: 0.2278 - val_accuracy: 0.9721\n",
            "Epoch 170/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 0.9984\n",
            "Epoch 170: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.2419 - val_accuracy: 0.9721\n",
            "Epoch 171/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9972\n",
            "Epoch 171: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 0.9970 - val_loss: 0.2500 - val_accuracy: 0.9721\n",
            "Epoch 172/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n",
            "Epoch 172: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.2449 - val_accuracy: 0.9721\n",
            "Epoch 173/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9972\n",
            "Epoch 173: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.2484 - val_accuracy: 0.9696\n",
            "Epoch 174/200\n",
            "73/75 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9953\n",
            "Epoch 174: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.9953 - val_loss: 0.2440 - val_accuracy: 0.9725\n",
            "Epoch 175/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0050 - accuracy: 0.9985\n",
            "Epoch 175: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.2628 - val_accuracy: 0.9700\n",
            "Epoch 176/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\n",
            "Epoch 176: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.2492 - val_accuracy: 0.9712\n",
            "Epoch 177/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9985\n",
            "Epoch 177: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.2605 - val_accuracy: 0.9712\n",
            "Epoch 178/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9982\n",
            "Epoch 178: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.2576 - val_accuracy: 0.9717\n",
            "Epoch 179/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 0.9983\n",
            "Epoch 179: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.2510 - val_accuracy: 0.9733\n",
            "Epoch 180/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0029 - accuracy: 0.9996\n",
            "Epoch 180: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.2545 - val_accuracy: 0.9721\n",
            "Epoch 181/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0076 - accuracy: 0.9973\n",
            "Epoch 181: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.2468 - val_accuracy: 0.9700\n",
            "Epoch 182/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9971\n",
            "Epoch 182: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.2355 - val_accuracy: 0.9704\n",
            "Epoch 183/200\n",
            "64/75 [========================>.....] - ETA: 0s - loss: 0.0058 - accuracy: 0.9980\n",
            "Epoch 183: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.2419 - val_accuracy: 0.9708\n",
            "Epoch 184/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\n",
            "Epoch 184: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.2489 - val_accuracy: 0.9746\n",
            "Epoch 185/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
            "Epoch 185: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.2439 - val_accuracy: 0.9729\n",
            "Epoch 186/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 186: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.9979 - val_loss: 0.2736 - val_accuracy: 0.9679\n",
            "Epoch 187/200\n",
            "63/75 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 0.9978\n",
            "Epoch 187: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.2425 - val_accuracy: 0.9717\n",
            "Epoch 188/200\n",
            "69/75 [==========================>...] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\n",
            "Epoch 188: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.2610 - val_accuracy: 0.9704\n",
            "Epoch 189/200\n",
            "68/75 [==========================>...] - ETA: 0s - loss: 0.0027 - accuracy: 0.9992\n",
            "Epoch 189: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.2625 - val_accuracy: 0.9700\n",
            "Epoch 190/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0055 - accuracy: 0.9980\n",
            "Epoch 190: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 0.2609 - val_accuracy: 0.9717\n",
            "Epoch 191/200\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9983\n",
            "Epoch 191: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.9983 - val_loss: 0.2536 - val_accuracy: 0.9733\n",
            "Epoch 192/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0031 - accuracy: 0.9992\n",
            "Epoch 192: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.2545 - val_accuracy: 0.9708\n",
            "Epoch 193/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0030 - accuracy: 0.9990\n",
            "Epoch 193: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.2740 - val_accuracy: 0.9708\n",
            "Epoch 194/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0038 - accuracy: 0.9986\n",
            "Epoch 194: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.2551 - val_accuracy: 0.9712\n",
            "Epoch 195/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0096 - accuracy: 0.9959\n",
            "Epoch 195: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9957 - val_loss: 0.2541 - val_accuracy: 0.9692\n",
            "Epoch 196/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0072 - accuracy: 0.9976\n",
            "Epoch 196: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 0.9974 - val_loss: 0.2589 - val_accuracy: 0.9712\n",
            "Epoch 197/200\n",
            "66/75 [=========================>....] - ETA: 0s - loss: 0.0041 - accuracy: 0.9991\n",
            "Epoch 197: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.2513 - val_accuracy: 0.9729\n",
            "Epoch 198/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0036 - accuracy: 0.9989\n",
            "Epoch 198: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.2649 - val_accuracy: 0.9717\n",
            "Epoch 199/200\n",
            "65/75 [=========================>....] - ETA: 0s - loss: 0.0023 - accuracy: 0.9998\n",
            "Epoch 199: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2530 - val_accuracy: 0.9721\n",
            "Epoch 200/200\n",
            "67/75 [=========================>....] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 200: val_loss did not improve from 0.10033\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2482 - val_accuracy: 0.9729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6PBgtRoOCEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036a2d38-6adb-4665-c0d2-4ac5980379cd"
      },
      "source": [
        "np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9685"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgBoo6qwOePl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056b320a-39f5-4060-baf4-3b0c684ff411"
      },
      "source": [
        "history_3.model.load_weights('best_model.h5')\n",
        "np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.963"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqBE6nvYZM-V"
      },
      "source": [
        "#Automatic hyperparameter search platform -> wandb sweeps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-OPEMo0Zgg4"
      },
      "source": [
        "example_1: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb\n",
        "\n",
        "example_2: https://colab.research.google.com/drive/181GCGp36_75C2zm7WLxr9U2QjMXXoibt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3MtwB1waCIb"
      },
      "source": [
        "TO DO:\n",
        "Using grid search and x[:10000] as val set, so x[10000:] as training execute a dummy hyperparameter on the multiclass classification task (not binary).\n",
        "Search for 100 epochs:\n",
        "1. layers: [64, 96, 128]\n",
        "2. learning rate: [0.001, 0.003, 0.005]\n",
        "3. dropout: [0.3, 0.4, 0.5]\n",
        "4. minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZWYNFH_Zdfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ec59e7-42f7-421f-ff51-6cd34a071da9"
      },
      "source": [
        "# WandB  Install the W&B library\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     || 1.8 MB 5.5 MB/s \n",
            "\u001b[K     || 144 kB 45.8 MB/s \n",
            "\u001b[K     || 181 kB 46.9 MB/s \n",
            "\u001b[K     || 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xosCy6fLwnOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe37ba4-2196-4bb1-8a43-a83fa0c06b21"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install wandb -qq\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint,EarlyStopping\n",
        "\n",
        "!wandb login --relogin\n",
        "wandb.login() #just click the link and copy paste the pass"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpkasnesis\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9F9rxx2uzLM"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBvktAlbvGeV"
      },
      "source": [
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'   \n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLq0AxCYvJIp"
      },
      "source": [
        "parameters_dict = {\n",
        " 'learning-rate':{\n",
        "        'values': [0.001, 0.003, 0.005]\n",
        "        },\n",
        "    'fc_layer_size': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.4, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey0INJUYwbus"
      },
      "source": [
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 100}\n",
        "    })"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWp0T2IwwlQA",
        "outputId": "420af2a4-1d69-46c8-86f7-49ee918cc9bf"
      },
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'method': 'grid',\n",
            " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
            " 'parameters': {'dropout': {'values': [0.3, 0.4, 0.5]},\n",
            "                'epochs': {'value': 100},\n",
            "                'fc_layer_size': {'values': [128, 256, 512]},\n",
            "                'learning-rate': {'values': [0.001, 0.003, 0.005]}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o9MxdqEws6Z",
        "outputId": "a6190ae2-a96e-4eed-be18-53a4aca208fc"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"pkasnesis\", project=\"aidl-a02-week5\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: nw4riqdk\n",
            "Sweep URL: https://wandb.ai/pkasnesis/aidl-a02-week5/sweeps/nw4riqdk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EE2OVphzANh"
      },
      "source": [
        "X_val = x_train[:10000]\n",
        "X_train = x_train[10000:]\n",
        "X_test = x_test\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(y_train[10000:],10)\n",
        "Y_val = tf.keras.utils.to_categorical(y_train[:10000],10)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test,10)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D71E1Y7xxJu"
      },
      "source": [
        "# The sweep calls this function with each set of hyperparameters\n",
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'learning_rate': 1e-3,\n",
        "        'dropout': 0.3,\n",
        "        'fc_layer_size': 128,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    \n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Dense(config.fc_layer_size, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
        "    model.add(Dropout(config.dropout))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    batch_size = 128\n",
        "\n",
        "    #save_model = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
        "    adam = tf.keras.optimizers.Adam(lr=config.learning_rate)\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
        "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "              epochs=config.epochs,\n",
        "              validation_data=(X_val, Y_val),\n",
        "              callbacks=[WandbCallback(validation_data=(X_val, Y_val), labels=classes),\n",
        "                          ])#EarlyStopping(patience=10, restore_best_weights=True)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WeOPz-xxxYe",
        "outputId": "3dd1e665-ac2b-4dc8-e61d-b449ef8e2d10"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wzdqghox with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning-rate: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFCsgj9G_XDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2a4c70-8b77-4b4e-e995-565f4b319f16"
      },
      "source": [
        "# restore the model file \"model.h5\" from a specific run by user \"pkasnesis\"\n",
        "# in project \"aidl-a02-week5\" from run \"10pr4joa\"\n",
        "best_model = wandb.restore('model-best.h5', run_path=\"pkasnesis/aidl-a02-week5/l2ylcagj\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n",
        "model.load_weights(best_model.name)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r391/391 [==============================] - 2s 5ms/step - loss: 0.2706 - accuracy: 0.9001 - val_loss: 0.2986 - val_accuracy: 0.8913 - _timestamp: 1650283768.0000 - _runtime: 33.0000\n",
            "Epoch 14/100\n",
            " 67/391 [====>.........................] - ETA: 2s - loss: 0.2549 - accuracy: 0.9035"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bJZpXYyPDu6",
        "outputId": "b4445d1e-1d33-40ef-abc0-f6ae7879a86b"
      },
      "source": [
        "print(np.mean(np.argmax(model.predict(X_val),1)==np.argmax(Y_val,1)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 4s 9ms/step - loss: 0.1378 - accuracy: 0.9464 - val_loss: 0.3348 - val_accuracy: 0.8990 - _timestamp: 1650283911.0000 - _runtime: 176.0000\n",
            "Epoch 67/100\n",
            "135/391 [=========>....................] - ETA: 1s - loss: 0.1355 - accuracy: 0.94740.8913\n",
            "168/391 [===========>..................] - ETA: 1s - loss: 0.1371 - accuracy: 0.9464"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eio9LiOnc7Ni",
        "outputId": "8fb432f4-9768-48ef-a5c5-2fbcecfbe35c"
      },
      "source": [
        "print(np.mean(np.argmax(model.predict(X_test),1)==np.argmax(Y_test,1)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 4s 9ms/step - loss: 0.1549 - accuracy: 0.9412 - val_loss: 0.3138 - val_accuracy: 0.8993 - _timestamp: 1650283874.0000 - _runtime: 139.0000\n",
            "Epoch 51/100\n",
            "211/391 [===============>..............] - ETA: 1s - loss: 0.1549 - accuracy: 0.94090.8867\n",
            "256/391 [==================>...........] - ETA: 0s - loss: 0.1558 - accuracy: 0.9406"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hD9hTL2c-fP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}