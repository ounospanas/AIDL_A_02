{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIDL_A02_network_optimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ounospanas/AIDL_A_02/blob/main/notebooks/AIDL_A02_network_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j25rklnwD1-m"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JenmhPusFZ06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ab1f58-1898-4d87-e88f-f30214ea5af7"
      },
      "source": [
        "#load dataset\n",
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "V7uVONPjF1PP",
        "outputId": "17afdc84-f430-41c2-a0f4-7ab4d0e0fb4f"
      },
      "source": [
        "#visualize some data\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "selected = [0,1,3,5,6,8,16,18,19,23]\n",
        "plt.figure(figsize=(11, 11))\n",
        "for i, s in enumerate(selected):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    #img = plt.imread(x_train[s])\n",
        "    plt.imshow(x_train[s], cmap='gray')\n",
        "    plt.xlabel(classes[y_train[s]],)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x792 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8NenH6QEPlH"
      },
      "source": [
        "# normalize data\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)/255\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)/255"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zD2QsqukISO"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWtYiD_eEJsq"
      },
      "source": [
        "# select only t-shirts and ankle boots\n",
        "shirt_train = np.where(y_train==0)\n",
        "dress_train = np.where(y_train==3)\n",
        "\n",
        "shirt_test = np.where(y_test==0)\n",
        "dress_test = np.where(y_test==3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugGWzBtFJA_1"
      },
      "source": [
        "# concatenate them\n",
        "x_train_s = x_train[shirt_train]\n",
        "y_train_s = y_train[shirt_train]\n",
        "\n",
        "x_test_s = x_test[shirt_test]\n",
        "y_test_s = y_test[shirt_test]\n",
        "\n",
        "x_train_d = x_train[dress_train]\n",
        "y_train_d = y_train[dress_train]\n",
        "\n",
        "x_test_d = x_test[dress_test]\n",
        "y_test_d = y_test[dress_test]\n",
        "\n",
        "x_train_binary = np.concatenate([x_train_s, x_train_d])\n",
        "x_test_binary = np.concatenate([x_test_s, x_test_d])\n",
        "\n",
        "y_train_binary = np.concatenate([y_train_s, np.ones_like(y_train_d)])\n",
        "y_test_binary = np.concatenate([y_test_s, np.ones_like(y_test_d)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtVPYLzMAIL"
      },
      "source": [
        "# Neural Network (Custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW49UROyzfn-"
      },
      "source": [
        "Compute the sigmoid function:\n",
        "$$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYBih-MhImZZ"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute sigmoid function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    g : The sigmoid function.\n",
        "    \"\"\"\n",
        "    a = 1./(1+np.exp(-z))\n",
        "\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wm-R3JFEvH"
      },
      "source": [
        "$a = ReLU(z) = max(z,0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3w8lOkkExcO"
      },
      "source": [
        "def relu(z):\n",
        "    \"\"\"\n",
        "    Compute relu function.\n",
        "    z : the product theta.T * x + b\n",
        "    Returns\n",
        "    -------\n",
        "    a : The relu function.\n",
        "    \"\"\"\n",
        "    a = np.maximum(z,0)\n",
        "\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iElu6cUL-aZ"
      },
      "source": [
        "# check relu function\n",
        "assert relu(-1) == 0\n",
        "assert relu(2) == 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v435SmJ2hSGF"
      },
      "source": [
        "# Parameter Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dql9nfxkAd"
      },
      "source": [
        "Xavier initialization: $$[-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}}]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0CIw-A92vsz"
      },
      "source": [
        "def init_params_xavier(n_in, n_out):\n",
        "\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check formula and use np.random.uniform\n",
        "    np....\n",
        "    w = None\n",
        "    #########\n",
        "    b = np.random.randn(n_out)*0.01\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyflAZhrXLu_",
        "outputId": "d515f5ad-9183-4a11-f3ec-ea91e8c190a1"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_xavier(2,3)\n",
        "print(np.round(w,4) == np.array([[ 0.1069,  0.4715,  0.2251],[ 0.0983, -0.1673,  0.3196]]))\n",
        "print(np.round(b,4) == np.array([0.0095, -0.0015, -0.001]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_vu4QwaBxU"
      },
      "source": [
        "He initialization: $$np.random.randn(n_{in}, n_{out})*\\sqrt{\\frac{2}{n_{in}}}$$, not multiply using 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK-3JBi2aAxp"
      },
      "source": [
        "def init_params_he(n_in, n_out):\n",
        "    # TO DO\n",
        "    # set random seed to 0\n",
        "    # Hint, check formula and use np.random.randn\n",
        "    # for b there is no n_in inside the np.random.randn\n",
        "    np...\n",
        "    w = None\n",
        "    b = None\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xEjaH9Vde3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f427f407-395c-4fe6-fec0-6844214c22e1"
      },
      "source": [
        "# check init params\n",
        "w, b  = init_params_he(2,3)\n",
        "print(np.round(w,4) == np.array([[ 1.7641,  0.4002,  0.9787], [ 2.2409,  1.8676, -0.9773]]))\n",
        "print(np.round(b,4) == np.array([0.9501, -0.1514, -0.1032]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ True  True  True]\n",
            " [ True  True  True]]\n",
            "[ True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M6bkSI46hd"
      },
      "source": [
        "Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdAJJ67d4gmD"
      },
      "source": [
        "def costFunction(y, m, a):\n",
        "    \"\"\"\n",
        "    Computes cost for linear regression.\n",
        "    X : feature vector, shape (m x n+1)\n",
        "    y : labels (i.e., dog or cat), shape (m, )\n",
        "    w : parameters for the linear regression, shape (n+1, )\n",
        "    m: data legth\n",
        "\n",
        "    returns\n",
        "    -------\n",
        "    J : value of cost function.\n",
        "    \"\"\"\n",
        "\n",
        "    J = -1/m * np.sum(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "\n",
        "\n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKNlaT8XZuG"
      },
      "source": [
        "#forward pass\n",
        "def forward(X, w, b, activation = 'relu'):\n",
        "    z = np.dot(X,w) + b\n",
        "    if activation=='relu':\n",
        "        a = relu(z)\n",
        "    else:\n",
        "        a = sigmoid(z)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AivWfcIMY5ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fca11b-6fc1-42cc-92f4-fb0cb5b20355"
      },
      "source": [
        "w, b  = init_params_he(2,3)\n",
        "forward(np.asarray([[1,2],[3,4]]), w, b, 'relu')[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zraP5X4zhYq"
      },
      "source": [
        "\\begin{split}ReLU'(z)= \\begin{Bmatrix}1 & z>0 \\\\\n",
        "0 & z<0 \\end{Bmatrix}\\end{split}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOTSR774duI"
      },
      "source": [
        "#relu gradient\n",
        "def reluBackward(z):\n",
        "    z[z<=0] = 0\n",
        "    z[z>0] = 1\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJK4wk_EA4t"
      },
      "source": [
        "Calculate the derivatives: $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A^{(i)}-Y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbHnEJsZX3Eu"
      },
      "source": [
        "#backpropagation\n",
        "def backward(a, dz):\n",
        "    m = len(a)\n",
        "    dw = np.dot(a.T, dz) / m\n",
        "    db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dw, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbgF0t0pRDfX"
      },
      "source": [
        "$$ w_j := w_j - \\alpha dw_j $$\n",
        "$$ b := b - \\alpha db $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7IbXv7rQgyt"
      },
      "source": [
        "# update parameters for optimization\n",
        "def update(w, b, dw, db, learning_rate=0.01):\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPnS1ExVDffM"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural(X, y, n_layer_1, lr = 0.01, epochs = 100):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course).\n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = None #use init_params_xavier\n",
        "    w2, b2 = None #use init_params_xavier\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        dz1 = np.dot((dz2),w2.T)*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "\n",
        "        if i%10==0:\n",
        "\n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1v9e_cjDeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749d558b-61d3-4388-97d5-5fcfbede49d3"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 with cost 0.73222077432145\n",
            "train: 0.5068333333333334\n",
            "test: 0.5\n",
            "epoch 10 with cost 0.3449637483262826\n",
            "train: 0.8908333333333334\n",
            "test: 0.882\n",
            "epoch 20 with cost 0.2831690845443682\n",
            "train: 0.897\n",
            "test: 0.894\n",
            "epoch 30 with cost 0.2593395198965416\n",
            "train: 0.9028333333333334\n",
            "test: 0.9015\n",
            "epoch 40 with cost 0.2452727337640431\n",
            "train: 0.9095833333333333\n",
            "test: 0.907\n",
            "epoch 50 with cost 0.23537812157188362\n",
            "train: 0.9139166666666667\n",
            "test: 0.913\n",
            "epoch 60 with cost 0.22785751996816328\n",
            "train: 0.9176666666666666\n",
            "test: 0.917\n",
            "epoch 70 with cost 0.22213162272876213\n",
            "train: 0.9193333333333333\n",
            "test: 0.921\n",
            "epoch 80 with cost 0.2175448627414139\n",
            "train: 0.9216666666666666\n",
            "test: 0.9225\n",
            "epoch 90 with cost 0.2136466884836102\n",
            "train: 0.9239166666666667\n",
            "test: 0.9235\n",
            "epoch 100 with cost 0.21042192808580462\n",
            "train: 0.9250833333333334\n",
            "test: 0.924\n",
            "epoch 110 with cost 0.20771532431056403\n",
            "train: 0.927\n",
            "test: 0.9255\n",
            "epoch 120 with cost 0.20538208768692795\n",
            "train: 0.928\n",
            "test: 0.9275\n",
            "epoch 130 with cost 0.20328904868104014\n",
            "train: 0.9290833333333334\n",
            "test: 0.928\n",
            "epoch 140 with cost 0.2013800963774929\n",
            "train: 0.93\n",
            "test: 0.9285\n",
            "epoch 150 with cost 0.19961861534254216\n",
            "train: 0.9319166666666666\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.1980201693774248\n",
            "train: 0.93275\n",
            "test: 0.9295\n",
            "epoch 170 with cost 0.19658358890152217\n",
            "train: 0.9340833333333334\n",
            "test: 0.931\n",
            "epoch 180 with cost 0.19526674275538497\n",
            "train: 0.9343333333333333\n",
            "test: 0.931\n",
            "epoch 190 with cost 0.194094280049121\n",
            "train: 0.9348333333333333\n",
            "test: 0.9315\n",
            "epoch 200 with cost 0.19305596820873597\n",
            "train: 0.9354166666666667\n",
            "test: 0.9335\n",
            "epoch 210 with cost 0.19210040498229924\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 220 with cost 0.19120432289433184\n",
            "train: 0.9358333333333333\n",
            "test: 0.933\n",
            "epoch 230 with cost 0.19038629547882224\n",
            "train: 0.9366666666666666\n",
            "test: 0.933\n",
            "epoch 240 with cost 0.1896301410584099\n",
            "train: 0.9370833333333334\n",
            "test: 0.933\n",
            "epoch 250 with cost 0.18898113489003068\n",
            "train: 0.9373333333333334\n",
            "test: 0.9325\n",
            "epoch 260 with cost 0.1883895772365391\n",
            "train: 0.93825\n",
            "test: 0.933\n",
            "epoch 270 with cost 0.187873027260566\n",
            "train: 0.9393333333333334\n",
            "test: 0.9335\n",
            "epoch 280 with cost 0.18747552213510196\n",
            "train: 0.9395833333333333\n",
            "test: 0.9335\n",
            "epoch 290 with cost 0.1871796391116818\n",
            "train: 0.9396666666666667\n",
            "test: 0.9345\n",
            "epoch 300 with cost 0.18691658572297357\n",
            "train: 0.94025\n",
            "test: 0.935\n",
            "epoch 310 with cost 0.186685439028307\n",
            "train: 0.9406666666666667\n",
            "test: 0.9345\n",
            "epoch 320 with cost 0.1864629169300269\n",
            "train: 0.9413333333333334\n",
            "test: 0.9355\n",
            "epoch 330 with cost 0.1862074862746406\n",
            "train: 0.942\n",
            "test: 0.9355\n",
            "epoch 340 with cost 0.18595601232142633\n",
            "train: 0.943\n",
            "test: 0.936\n",
            "epoch 350 with cost 0.1856838792859512\n",
            "train: 0.9429166666666666\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.18555748742851813\n",
            "train: 0.9430833333333334\n",
            "test: 0.9355\n",
            "epoch 370 with cost 0.1855693916822616\n",
            "train: 0.9435833333333333\n",
            "test: 0.9355\n",
            "epoch 380 with cost 0.18558261349135038\n",
            "train: 0.9435833333333333\n",
            "test: 0.936\n",
            "epoch 390 with cost 0.18558809649233152\n",
            "train: 0.9436666666666667\n",
            "test: 0.936\n",
            "epoch 400 with cost 0.18560110893932233\n",
            "train: 0.9438333333333333\n",
            "test: 0.9365\n",
            "epoch 410 with cost 0.18559470291092012\n",
            "train: 0.9445\n",
            "test: 0.9365\n",
            "epoch 420 with cost 0.18554222013773664\n",
            "train: 0.9449166666666666\n",
            "test: 0.9365\n",
            "epoch 430 with cost 0.1854775006794311\n",
            "train: 0.9450833333333334\n",
            "test: 0.9365\n",
            "epoch 440 with cost 0.18540716338808846\n",
            "train: 0.9449166666666666\n",
            "test: 0.9355\n",
            "epoch 450 with cost 0.18531191270498382\n",
            "train: 0.9454166666666667\n",
            "test: 0.9355\n",
            "epoch 460 with cost 0.1852470697827609\n",
            "train: 0.9453333333333334\n",
            "test: 0.935\n",
            "epoch 470 with cost 0.18528987254863655\n",
            "train: 0.9456666666666667\n",
            "test: 0.935\n",
            "epoch 480 with cost 0.18534556042763514\n",
            "train: 0.9458333333333333\n",
            "test: 0.936\n",
            "epoch 490 with cost 0.1854095584445661\n",
            "train: 0.9461666666666667\n",
            "test: 0.9355\n",
            "epoch 500 with cost 0.18544449207712985\n",
            "train: 0.9466666666666667\n",
            "test: 0.9365\n",
            "epoch 510 with cost 0.18546397792969813\n",
            "train: 0.9470833333333334\n",
            "test: 0.9365\n",
            "epoch 520 with cost 0.1854318020703331\n",
            "train: 0.9471666666666667\n",
            "test: 0.9355\n",
            "epoch 530 with cost 0.1853339138822911\n",
            "train: 0.94725\n",
            "test: 0.9355\n",
            "epoch 540 with cost 0.18520164011777926\n",
            "train: 0.9474166666666667\n",
            "test: 0.9355\n",
            "epoch 550 with cost 0.18500245360806464\n",
            "train: 0.9474166666666667\n",
            "test: 0.936\n",
            "epoch 560 with cost 0.18479215772975677\n",
            "train: 0.94775\n",
            "test: 0.9365\n",
            "epoch 570 with cost 0.18457449350813657\n",
            "train: 0.9479166666666666\n",
            "test: 0.9365\n",
            "epoch 580 with cost 0.18436324952597355\n",
            "train: 0.9481666666666667\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.18413452920873039\n",
            "train: 0.9485\n",
            "test: 0.937\n",
            "epoch 600 with cost 0.18388537526060436\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 610 with cost 0.18365383403333743\n",
            "train: 0.9486666666666667\n",
            "test: 0.9375\n",
            "epoch 620 with cost 0.18347515772253606\n",
            "train: 0.9489166666666666\n",
            "test: 0.938\n",
            "epoch 630 with cost 0.18335629214785285\n",
            "train: 0.949\n",
            "test: 0.938\n",
            "epoch 640 with cost 0.1832960124475722\n",
            "train: 0.9489166666666666\n",
            "test: 0.937\n",
            "epoch 650 with cost 0.18320729819398732\n",
            "train: 0.9488333333333333\n",
            "test: 0.937\n",
            "epoch 660 with cost 0.18303959212553536\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 670 with cost 0.1828566338471163\n",
            "train: 0.94875\n",
            "test: 0.937\n",
            "epoch 680 with cost 0.18263981999875428\n",
            "train: 0.94875\n",
            "test: 0.9375\n",
            "epoch 690 with cost 0.18242524939504207\n",
            "train: 0.949\n",
            "test: 0.937\n",
            "epoch 700 with cost 0.18217889142094842\n",
            "train: 0.9493333333333334\n",
            "test: 0.937\n",
            "epoch 710 with cost 0.18187524025287466\n",
            "train: 0.9495\n",
            "test: 0.9375\n",
            "epoch 720 with cost 0.18157715496196783\n",
            "train: 0.9494166666666667\n",
            "test: 0.9375\n",
            "epoch 730 with cost 0.1812786783802568\n",
            "train: 0.9493333333333334\n",
            "test: 0.938\n",
            "epoch 740 with cost 0.1809804301190431\n",
            "train: 0.9494166666666667\n",
            "test: 0.938\n",
            "epoch 750 with cost 0.1806987696914577\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 760 with cost 0.18038430345474743\n",
            "train: 0.94975\n",
            "test: 0.938\n",
            "epoch 770 with cost 0.18006445641494634\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 780 with cost 0.17972750066760862\n",
            "train: 0.9496666666666667\n",
            "test: 0.938\n",
            "epoch 790 with cost 0.1793622912055191\n",
            "train: 0.94975\n",
            "test: 0.9375\n",
            "epoch 800 with cost 0.17897629633334394\n",
            "train: 0.9498333333333333\n",
            "test: 0.938\n",
            "epoch 810 with cost 0.1785524328653271\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 820 with cost 0.178114482759856\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 830 with cost 0.1776524838533345\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 840 with cost 0.17717400769478722\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 850 with cost 0.17667338666793178\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 860 with cost 0.17612797914567485\n",
            "train: 0.9495833333333333\n",
            "test: 0.9385\n",
            "epoch 870 with cost 0.17555430983877715\n",
            "train: 0.94975\n",
            "test: 0.9385\n",
            "epoch 880 with cost 0.17496176963723117\n",
            "train: 0.9496666666666667\n",
            "test: 0.9385\n",
            "epoch 890 with cost 0.17439185782164876\n",
            "train: 0.9498333333333333\n",
            "test: 0.9385\n",
            "epoch 900 with cost 0.17380425833356475\n",
            "train: 0.9498333333333333\n",
            "test: 0.939\n",
            "epoch 910 with cost 0.17316233668657127\n",
            "train: 0.95\n",
            "test: 0.939\n",
            "epoch 920 with cost 0.17243172368725496\n",
            "train: 0.9503333333333334\n",
            "test: 0.939\n",
            "epoch 930 with cost 0.17162811197693179\n",
            "train: 0.9504166666666667\n",
            "test: 0.94\n",
            "epoch 940 with cost 0.17083657717350387\n",
            "train: 0.9505833333333333\n",
            "test: 0.9405\n",
            "epoch 950 with cost 0.1700729248278461\n",
            "train: 0.95075\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.16932735637589208\n",
            "train: 0.9509166666666666\n",
            "test: 0.9405\n",
            "epoch 970 with cost 0.16859314431081102\n",
            "train: 0.95125\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.16785928356436836\n",
            "train: 0.9514166666666667\n",
            "test: 0.9405\n",
            "epoch 990 with cost 0.16705609907695887\n",
            "train: 0.9515\n",
            "test: 0.9405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3mztup8zhYw"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used he\n",
        "# assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9255\n",
        "# if you used xavier\n",
        "assert np.mean(np.round(a2t.reshape(-1))==y_test_binary) == 0.9405"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cR9l1cxie3T"
      },
      "source": [
        "# Learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itxh7L79idt_"
      },
      "source": [
        "def lr_scheduling(lr, epoch, schedule = 'step_decay'):\n",
        "\n",
        "  if schedule=='step_decay':\n",
        "    #TO DO, every 50 epochs divide lr by 2\n",
        "    if ...\n",
        "      lr = None\n",
        "  elif schedule=='exponential_decay':\n",
        "    #TO DO, multiply rl every epoch by exp(k), where k = 0.01\n",
        "    k = None\n",
        "    lr = None\n",
        "  else:\n",
        "    print('No scheduler, please define a correct scheduler!')\n",
        "\n",
        "  return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9To5w026l8cO"
      },
      "source": [
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'step_decay')\n",
        "assert lr==0.00625\n",
        "\n",
        "lr = 0.1\n",
        "for i in range(200):\n",
        "  lr = lr_scheduling(lr, i, schedule = 'exponential_decay')\n",
        "assert np.round(lr,4)==0.0135"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyY3lT0K8KKk"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gft-lLiF8PcO"
      },
      "source": [
        "def dropout_forward(a, keep_prob):\n",
        "    # TO DO\n",
        "    dr = None                                        # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
        "    dr = None                                         # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
        "    a = None                                         # Step 3: shut down some neurons of A1\n",
        "    a = None                                  # Step 4: Scale the value of neurons that haven't been shut down by dividing with keep_prob\n",
        "    #####\n",
        "    return a, dr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRAoEAQn8tUC"
      },
      "source": [
        "def dropout_backward(da, dr, keep_prob):\n",
        "    # TO DO\n",
        "    da = None             # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
        "    da = None             # Step 2: Scale the value of neurons that haven't been shut down by dividing with keep_prob\n",
        "    ######\n",
        "    return da"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD346J1Z_eWt"
      },
      "source": [
        "#forward pass\n",
        "def dummy_neural_dr(X, y, n_layer_1, lr = 0.01, epochs = 100, keep_prob=0.1):\n",
        "    parameters = {}\n",
        "    gradients = {}\n",
        "    costs = []\n",
        "\n",
        "    n_in = X.shape[1]\n",
        "    n_out = 1\n",
        "\n",
        "    # initialize network with 1 hidden layer (and 1 output of course).\n",
        "    # Layer 1 should have 200 neurons\n",
        "    w1, b1 = init_params_xavier(n_in, n_layer_1)\n",
        "    w2, b2 = init_params_xavier(n_layer_1, n_out)\n",
        "\n",
        "    parameters['w1'] = w1\n",
        "    parameters['b1'] = b1\n",
        "    parameters['w2'] = w2\n",
        "    parameters['b2'] = b2\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        #forward pass\n",
        "        a1 = forward(X, w1, b1, activation = 'relu')\n",
        "        a1, dr = None # TO DO use dropout_forward\n",
        "        a2 = forward(a1, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "        #cost function\n",
        "        cost = costFunction(y, len(y), a2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        #backward pass\n",
        "        dz2 = a2-y\n",
        "        dw2, db2 = backward(a2, dz2)\n",
        "        da1 = np.dot((dz2),w2.T)\n",
        "        da1 = None # TO DO use dropout_backward\n",
        "        dz1 = da1*reluBackward(np.dot(X,w1) + b1)\n",
        "        dw1, db1 = backward(X, dz1)\n",
        "\n",
        "        gradients['dw1'] = dw1\n",
        "        gradients['db1'] = db1\n",
        "        gradients['dw2'] = dw2\n",
        "        gradients['db2'] = db2\n",
        "\n",
        "        #update weights\n",
        "        w2, b2 = update(w2, b2, dw2, db2, lr)\n",
        "        w1, b1 = update(w1, b1, dw1, db1, lr)\n",
        "\n",
        "        parameters['w1'] = w1\n",
        "        parameters['b1'] = b1\n",
        "        parameters['w2'] = w2\n",
        "        parameters['b2'] = b2\n",
        "\n",
        "        if i%10==0:\n",
        "\n",
        "            a1t = forward(x_test_binary, w1, b1, activation = 'relu')\n",
        "            a2t = forward(a1t, w2, b2, activation = 'sigmoid')\n",
        "\n",
        "            print(\"epoch {} with cost {}\".format(i,cost))\n",
        "            print(\"train:\", np.mean(np.round(a2)==y))\n",
        "            print(\"test:\", np.mean(np.round(a2t.reshape(-1))==y_test_binary))\n",
        "\n",
        "    return parameters, a2, costs, gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZPqVszmBG08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811840f9-5675-4cce-b261-8ad982cc4bd5"
      },
      "source": [
        "learning_rate = 0.1\n",
        "a = dummy_neural_dr(x_train_binary, y_train_binary.reshape(-1,1), 200, learning_rate, 1000, 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 with cost 0.8667195906735228\n",
            "train: 0.49233333333333335\n",
            "test: 0.501\n",
            "epoch 10 with cost 0.3981387486880624\n",
            "train: 0.8536666666666667\n",
            "test: 0.8835\n",
            "epoch 20 with cost 0.34152121678958036\n",
            "train: 0.8695833333333334\n",
            "test: 0.891\n",
            "epoch 30 with cost 0.31234081001862835\n",
            "train: 0.8836666666666667\n",
            "test: 0.8975\n",
            "epoch 40 with cost 0.29490961712568986\n",
            "train: 0.8886666666666667\n",
            "test: 0.9055\n",
            "epoch 50 with cost 0.28131085952004203\n",
            "train: 0.8939166666666667\n",
            "test: 0.909\n",
            "epoch 60 with cost 0.2758311176521958\n",
            "train: 0.8979166666666667\n",
            "test: 0.911\n",
            "epoch 70 with cost 0.2709139582033558\n",
            "train: 0.89825\n",
            "test: 0.9175\n",
            "epoch 80 with cost 0.2610186268746109\n",
            "train: 0.9025833333333333\n",
            "test: 0.92\n",
            "epoch 90 with cost 0.25360580261069654\n",
            "train: 0.9075833333333333\n",
            "test: 0.921\n",
            "epoch 100 with cost 0.2522168024499044\n",
            "train: 0.9069166666666667\n",
            "test: 0.923\n",
            "epoch 110 with cost 0.24694134704549286\n",
            "train: 0.9098333333333334\n",
            "test: 0.9235\n",
            "epoch 120 with cost 0.23787234034672605\n",
            "train: 0.9141666666666667\n",
            "test: 0.925\n",
            "epoch 130 with cost 0.24347501681317218\n",
            "train: 0.9095833333333333\n",
            "test: 0.926\n",
            "epoch 140 with cost 0.24009744527459656\n",
            "train: 0.9153333333333333\n",
            "test: 0.9265\n",
            "epoch 150 with cost 0.2344947270041495\n",
            "train: 0.91825\n",
            "test: 0.9285\n",
            "epoch 160 with cost 0.23390674292127034\n",
            "train: 0.9153333333333333\n",
            "test: 0.93\n",
            "epoch 170 with cost 0.23164360016133825\n",
            "train: 0.9164166666666667\n",
            "test: 0.93\n",
            "epoch 180 with cost 0.22738504882452573\n",
            "train: 0.9215\n",
            "test: 0.93\n",
            "epoch 190 with cost 0.22865203587457072\n",
            "train: 0.9181666666666667\n",
            "test: 0.929\n",
            "epoch 200 with cost 0.2246864810699843\n",
            "train: 0.9205\n",
            "test: 0.929\n",
            "epoch 210 with cost 0.22557608372368168\n",
            "train: 0.9213333333333333\n",
            "test: 0.9295\n",
            "epoch 220 with cost 0.2231181226708132\n",
            "train: 0.9215833333333333\n",
            "test: 0.93\n",
            "epoch 230 with cost 0.22460126294594962\n",
            "train: 0.9208333333333333\n",
            "test: 0.9315\n",
            "epoch 240 with cost 0.21798573390344128\n",
            "train: 0.92625\n",
            "test: 0.933\n",
            "epoch 250 with cost 0.22180110746400902\n",
            "train: 0.9236666666666666\n",
            "test: 0.934\n",
            "epoch 260 with cost 0.21352839705738058\n",
            "train: 0.9245\n",
            "test: 0.9335\n",
            "epoch 270 with cost 0.21676474631813114\n",
            "train: 0.92625\n",
            "test: 0.9345\n",
            "epoch 280 with cost 0.21704161698761693\n",
            "train: 0.9254166666666667\n",
            "test: 0.936\n",
            "epoch 290 with cost 0.21345430049710293\n",
            "train: 0.9265833333333333\n",
            "test: 0.9355\n",
            "epoch 300 with cost 0.2146332001427348\n",
            "train: 0.9268333333333333\n",
            "test: 0.9355\n",
            "epoch 310 with cost 0.2126717618156211\n",
            "train: 0.9275833333333333\n",
            "test: 0.936\n",
            "epoch 320 with cost 0.21042047045363876\n",
            "train: 0.9293333333333333\n",
            "test: 0.935\n",
            "epoch 330 with cost 0.21381891227627522\n",
            "train: 0.926\n",
            "test: 0.9365\n",
            "epoch 340 with cost 0.21008909696446823\n",
            "train: 0.9280833333333334\n",
            "test: 0.936\n",
            "epoch 350 with cost 0.21114195155061297\n",
            "train: 0.9250833333333334\n",
            "test: 0.9355\n",
            "epoch 360 with cost 0.21267085225663232\n",
            "train: 0.92725\n",
            "test: 0.937\n",
            "epoch 370 with cost 0.20598280535617988\n",
            "train: 0.931\n",
            "test: 0.937\n",
            "epoch 380 with cost 0.20462662717724447\n",
            "train: 0.931\n",
            "test: 0.937\n",
            "epoch 390 with cost 0.20531138655038397\n",
            "train: 0.93075\n",
            "test: 0.9365\n",
            "epoch 400 with cost 0.2056107693814855\n",
            "train: 0.9301666666666667\n",
            "test: 0.936\n",
            "epoch 410 with cost 0.20575355597164396\n",
            "train: 0.9320833333333334\n",
            "test: 0.9365\n",
            "epoch 420 with cost 0.20391865704359588\n",
            "train: 0.9300833333333334\n",
            "test: 0.936\n",
            "epoch 430 with cost 0.20233772679095727\n",
            "train: 0.9328333333333333\n",
            "test: 0.935\n",
            "epoch 440 with cost 0.20159516208598555\n",
            "train: 0.9315833333333333\n",
            "test: 0.9365\n",
            "epoch 450 with cost 0.20436737403905575\n",
            "train: 0.9311666666666667\n",
            "test: 0.9385\n",
            "epoch 460 with cost 0.2065294900810658\n",
            "train: 0.9299166666666666\n",
            "test: 0.938\n",
            "epoch 470 with cost 0.20627088887592068\n",
            "train: 0.932\n",
            "test: 0.9375\n",
            "epoch 480 with cost 0.20442212586659406\n",
            "train: 0.9308333333333333\n",
            "test: 0.9385\n",
            "epoch 490 with cost 0.20157920996831463\n",
            "train: 0.9348333333333333\n",
            "test: 0.938\n",
            "epoch 500 with cost 0.20312616662350766\n",
            "train: 0.9321666666666667\n",
            "test: 0.938\n",
            "epoch 510 with cost 0.2021794734426448\n",
            "train: 0.9335833333333333\n",
            "test: 0.9365\n",
            "epoch 520 with cost 0.1987362827167649\n",
            "train: 0.9358333333333333\n",
            "test: 0.9365\n",
            "epoch 530 with cost 0.19852158856799276\n",
            "train: 0.9355833333333333\n",
            "test: 0.9365\n",
            "epoch 540 with cost 0.19797591101317713\n",
            "train: 0.93575\n",
            "test: 0.937\n",
            "epoch 550 with cost 0.20140285189193377\n",
            "train: 0.93375\n",
            "test: 0.9375\n",
            "epoch 560 with cost 0.2017041988143555\n",
            "train: 0.9328333333333333\n",
            "test: 0.9355\n",
            "epoch 570 with cost 0.19948321562629379\n",
            "train: 0.93375\n",
            "test: 0.936\n",
            "epoch 580 with cost 0.1950429440722878\n",
            "train: 0.9350833333333334\n",
            "test: 0.9365\n",
            "epoch 590 with cost 0.19819733857291222\n",
            "train: 0.9345\n",
            "test: 0.9365\n",
            "epoch 600 with cost 0.20245250597211792\n",
            "train: 0.9361666666666667\n",
            "test: 0.937\n",
            "epoch 610 with cost 0.1990721012018375\n",
            "train: 0.9340833333333334\n",
            "test: 0.9375\n",
            "epoch 620 with cost 0.19625048921946328\n",
            "train: 0.93675\n",
            "test: 0.9365\n",
            "epoch 630 with cost 0.194946449529046\n",
            "train: 0.9351666666666667\n",
            "test: 0.9365\n",
            "epoch 640 with cost 0.19350611315769703\n",
            "train: 0.9358333333333333\n",
            "test: 0.9375\n",
            "epoch 650 with cost 0.19500253655179758\n",
            "train: 0.93575\n",
            "test: 0.9365\n",
            "epoch 660 with cost 0.19755155866359883\n",
            "train: 0.93375\n",
            "test: 0.937\n",
            "epoch 670 with cost 0.19500862613227612\n",
            "train: 0.9381666666666667\n",
            "test: 0.9375\n",
            "epoch 680 with cost 0.19811437135416454\n",
            "train: 0.9368333333333333\n",
            "test: 0.9375\n",
            "epoch 690 with cost 0.19591599863989403\n",
            "train: 0.9365\n",
            "test: 0.9375\n",
            "epoch 700 with cost 0.19333030191555664\n",
            "train: 0.9390833333333334\n",
            "test: 0.9375\n",
            "epoch 710 with cost 0.19645086590747113\n",
            "train: 0.9386666666666666\n",
            "test: 0.938\n",
            "epoch 720 with cost 0.1944787439036825\n",
            "train: 0.9388333333333333\n",
            "test: 0.9375\n",
            "epoch 730 with cost 0.19786212664628916\n",
            "train: 0.9351666666666667\n",
            "test: 0.9375\n",
            "epoch 740 with cost 0.19422484750833519\n",
            "train: 0.9381666666666667\n",
            "test: 0.938\n",
            "epoch 750 with cost 0.19647718690372523\n",
            "train: 0.937\n",
            "test: 0.938\n",
            "epoch 760 with cost 0.19375873656534415\n",
            "train: 0.9378333333333333\n",
            "test: 0.9375\n",
            "epoch 770 with cost 0.194981378870975\n",
            "train: 0.9363333333333334\n",
            "test: 0.9375\n",
            "epoch 780 with cost 0.1932771456412056\n",
            "train: 0.9373333333333334\n",
            "test: 0.9385\n",
            "epoch 790 with cost 0.19498296116622055\n",
            "train: 0.93825\n",
            "test: 0.9385\n",
            "epoch 800 with cost 0.19179037773612767\n",
            "train: 0.94025\n",
            "test: 0.9385\n",
            "epoch 810 with cost 0.19444222151459511\n",
            "train: 0.93875\n",
            "test: 0.9385\n",
            "epoch 820 with cost 0.19501668224859164\n",
            "train: 0.93975\n",
            "test: 0.939\n",
            "epoch 830 with cost 0.19357704697504233\n",
            "train: 0.9365\n",
            "test: 0.9395\n",
            "epoch 840 with cost 0.1955945806129046\n",
            "train: 0.9385833333333333\n",
            "test: 0.94\n",
            "epoch 850 with cost 0.19109824256611738\n",
            "train: 0.93975\n",
            "test: 0.94\n",
            "epoch 860 with cost 0.19628150983612347\n",
            "train: 0.93975\n",
            "test: 0.9405\n",
            "epoch 870 with cost 0.19255179693475796\n",
            "train: 0.942\n",
            "test: 0.94\n",
            "epoch 880 with cost 0.1917485444225408\n",
            "train: 0.9409166666666666\n",
            "test: 0.94\n",
            "epoch 890 with cost 0.19251335646621248\n",
            "train: 0.9415\n",
            "test: 0.9405\n",
            "epoch 900 with cost 0.19353623915815277\n",
            "train: 0.9406666666666667\n",
            "test: 0.9405\n",
            "epoch 910 with cost 0.1935432369362297\n",
            "train: 0.939\n",
            "test: 0.9405\n",
            "epoch 920 with cost 0.192380168652521\n",
            "train: 0.9415833333333333\n",
            "test: 0.9385\n",
            "epoch 930 with cost 0.19281368840852442\n",
            "train: 0.9393333333333334\n",
            "test: 0.9405\n",
            "epoch 940 with cost 0.19159516617470293\n",
            "train: 0.9400833333333334\n",
            "test: 0.942\n",
            "epoch 950 with cost 0.19207167718861623\n",
            "train: 0.9393333333333334\n",
            "test: 0.9405\n",
            "epoch 960 with cost 0.19271782681431038\n",
            "train: 0.9395\n",
            "test: 0.9395\n",
            "epoch 970 with cost 0.1929383032482198\n",
            "train: 0.93925\n",
            "test: 0.9405\n",
            "epoch 980 with cost 0.1953389592925212\n",
            "train: 0.9404166666666667\n",
            "test: 0.941\n",
            "epoch 990 with cost 0.18961605769472825\n",
            "train: 0.9418333333333333\n",
            "test: 0.941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvWE3BNBN8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f265e06-0a36-45b6-dc24-27ed3ff8881b"
      },
      "source": [
        "# check the test accuracy\n",
        "a1t = forward(x_test_binary, a[0]['w1'], a[0]['b1'], activation = 'relu')\n",
        "a2t = forward(a1t, a[0]['w2'], a[0]['b2'], activation = 'sigmoid')\n",
        "# if you used xavier\n",
        "np.mean(np.round(a2t.reshape(-1))==y_test_binary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16PSfYXDoqui"
      },
      "source": [
        "# Neural Network Binary (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAQd0oGotCA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ghY0rEo6CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea347717-a9d4-493d-8c0b-f8df79d31081"
      },
      "source": [
        "# no dropout\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_1 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 4s 9ms/step - loss: 0.5143 - accuracy: 0.7854 - val_loss: 0.3127 - val_accuracy: 0.8840\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2856 - accuracy: 0.8973 - val_loss: 0.2640 - val_accuracy: 0.8945\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2499 - accuracy: 0.9038 - val_loss: 0.2397 - val_accuracy: 0.9055\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2215 - accuracy: 0.9139 - val_loss: 0.2258 - val_accuracy: 0.9130\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9241 - val_loss: 0.2151 - val_accuracy: 0.9200\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2023 - accuracy: 0.9214 - val_loss: 0.2095 - val_accuracy: 0.9230\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9196 - val_loss: 0.2024 - val_accuracy: 0.9260\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9245 - val_loss: 0.1979 - val_accuracy: 0.9295\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9262 - val_loss: 0.1946 - val_accuracy: 0.9320\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9290 - val_loss: 0.1928 - val_accuracy: 0.9300\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.9350 - val_loss: 0.1892 - val_accuracy: 0.9315\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1739 - accuracy: 0.9361 - val_loss: 0.1881 - val_accuracy: 0.9325\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9328 - val_loss: 0.1858 - val_accuracy: 0.9305\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9323 - val_loss: 0.1842 - val_accuracy: 0.9300\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9374 - val_loss: 0.1841 - val_accuracy: 0.9295\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9350 - val_loss: 0.1811 - val_accuracy: 0.9320\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9343 - val_loss: 0.1803 - val_accuracy: 0.9375\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9361 - val_loss: 0.1788 - val_accuracy: 0.9385\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9359 - val_loss: 0.1778 - val_accuracy: 0.9370\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1671 - accuracy: 0.9374 - val_loss: 0.1770 - val_accuracy: 0.9330\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1582 - accuracy: 0.9402 - val_loss: 0.1759 - val_accuracy: 0.9350\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9352 - val_loss: 0.1751 - val_accuracy: 0.9365\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9418 - val_loss: 0.1748 - val_accuracy: 0.9320\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9368 - val_loss: 0.1735 - val_accuracy: 0.9360\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9403 - val_loss: 0.1736 - val_accuracy: 0.9325\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.9416 - val_loss: 0.1721 - val_accuracy: 0.9375\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9395 - val_loss: 0.1716 - val_accuracy: 0.9355\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1545 - accuracy: 0.9392 - val_loss: 0.1711 - val_accuracy: 0.9355\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.9438 - val_loss: 0.1704 - val_accuracy: 0.9370\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9397 - val_loss: 0.1700 - val_accuracy: 0.9360\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1468 - accuracy: 0.9449 - val_loss: 0.1694 - val_accuracy: 0.9365\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9433 - val_loss: 0.1710 - val_accuracy: 0.9375\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9420 - val_loss: 0.1683 - val_accuracy: 0.9365\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1466 - accuracy: 0.9446 - val_loss: 0.1686 - val_accuracy: 0.9330\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.9433 - val_loss: 0.1674 - val_accuracy: 0.9390\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9447 - val_loss: 0.1673 - val_accuracy: 0.9355\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9438 - val_loss: 0.1670 - val_accuracy: 0.9350\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.9420 - val_loss: 0.1673 - val_accuracy: 0.9370\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9429 - val_loss: 0.1656 - val_accuracy: 0.9380\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9470 - val_loss: 0.1670 - val_accuracy: 0.9340\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.9439 - val_loss: 0.1650 - val_accuracy: 0.9385\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9453 - val_loss: 0.1644 - val_accuracy: 0.9365\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9450 - val_loss: 0.1640 - val_accuracy: 0.9385\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9436 - val_loss: 0.1638 - val_accuracy: 0.9380\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1398 - accuracy: 0.9484 - val_loss: 0.1635 - val_accuracy: 0.9380\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.9451 - val_loss: 0.1661 - val_accuracy: 0.9370\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1457 - accuracy: 0.9455 - val_loss: 0.1626 - val_accuracy: 0.9380\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9483 - val_loss: 0.1631 - val_accuracy: 0.9360\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9476 - val_loss: 0.1627 - val_accuracy: 0.9390\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9468 - val_loss: 0.1617 - val_accuracy: 0.9370\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9435 - val_loss: 0.1618 - val_accuracy: 0.9390\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9493 - val_loss: 0.1610 - val_accuracy: 0.9385\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9460 - val_loss: 0.1613 - val_accuracy: 0.9365\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9489 - val_loss: 0.1603 - val_accuracy: 0.9390\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9489 - val_loss: 0.1603 - val_accuracy: 0.9385\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9465 - val_loss: 0.1602 - val_accuracy: 0.9385\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9495 - val_loss: 0.1620 - val_accuracy: 0.9375\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9491 - val_loss: 0.1612 - val_accuracy: 0.9370\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9507 - val_loss: 0.1596 - val_accuracy: 0.9380\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9471 - val_loss: 0.1592 - val_accuracy: 0.9390\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9480 - val_loss: 0.1589 - val_accuracy: 0.9400\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9486 - val_loss: 0.1606 - val_accuracy: 0.9375\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9478 - val_loss: 0.1583 - val_accuracy: 0.9405\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9500 - val_loss: 0.1581 - val_accuracy: 0.9410\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9484 - val_loss: 0.1579 - val_accuracy: 0.9410\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9521 - val_loss: 0.1583 - val_accuracy: 0.9400\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9481 - val_loss: 0.1581 - val_accuracy: 0.9400\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.9523 - val_loss: 0.1572 - val_accuracy: 0.9405\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1328 - accuracy: 0.9516 - val_loss: 0.1577 - val_accuracy: 0.9400\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9487 - val_loss: 0.1566 - val_accuracy: 0.9405\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9514 - val_loss: 0.1563 - val_accuracy: 0.9425\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9507 - val_loss: 0.1561 - val_accuracy: 0.9415\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9501 - val_loss: 0.1583 - val_accuracy: 0.9395\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.9513 - val_loss: 0.1559 - val_accuracy: 0.9410\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9517 - val_loss: 0.1555 - val_accuracy: 0.9430\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1365 - accuracy: 0.9477 - val_loss: 0.1559 - val_accuracy: 0.9405\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9469 - val_loss: 0.1555 - val_accuracy: 0.9420\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9549 - val_loss: 0.1551 - val_accuracy: 0.9405\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.9523 - val_loss: 0.1553 - val_accuracy: 0.9420\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9535 - val_loss: 0.1551 - val_accuracy: 0.9400\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9538 - val_loss: 0.1543 - val_accuracy: 0.9435\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1251 - accuracy: 0.9540 - val_loss: 0.1543 - val_accuracy: 0.9415\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9546 - val_loss: 0.1544 - val_accuracy: 0.9400\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9550 - val_loss: 0.1546 - val_accuracy: 0.9420\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1289 - accuracy: 0.9519 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9526 - val_loss: 0.1541 - val_accuracy: 0.9420\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9554 - val_loss: 0.1536 - val_accuracy: 0.9405\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1253 - accuracy: 0.9534 - val_loss: 0.1531 - val_accuracy: 0.9430\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.9536 - val_loss: 0.1550 - val_accuracy: 0.9420\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9562 - val_loss: 0.1532 - val_accuracy: 0.9415\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9538 - val_loss: 0.1529 - val_accuracy: 0.9435\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9551 - val_loss: 0.1539 - val_accuracy: 0.9435\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1222 - accuracy: 0.9540 - val_loss: 0.1518 - val_accuracy: 0.9435\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1214 - accuracy: 0.9564 - val_loss: 0.1521 - val_accuracy: 0.9430\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9557 - val_loss: 0.1521 - val_accuracy: 0.9440\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9576 - val_loss: 0.1511 - val_accuracy: 0.9435\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9560 - val_loss: 0.1517 - val_accuracy: 0.9430\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1251 - accuracy: 0.9554 - val_loss: 0.1536 - val_accuracy: 0.9415\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9568 - val_loss: 0.1505 - val_accuracy: 0.9435\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1260 - accuracy: 0.9525 - val_loss: 0.1505 - val_accuracy: 0.9445\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9575 - val_loss: 0.1505 - val_accuracy: 0.9430\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9595 - val_loss: 0.1503 - val_accuracy: 0.9445\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1198 - accuracy: 0.9561 - val_loss: 0.1499 - val_accuracy: 0.9450\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9562 - val_loss: 0.1504 - val_accuracy: 0.9440\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9577 - val_loss: 0.1495 - val_accuracy: 0.9450\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9540 - val_loss: 0.1492 - val_accuracy: 0.9455\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9565 - val_loss: 0.1491 - val_accuracy: 0.9455\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9569 - val_loss: 0.1505 - val_accuracy: 0.9445\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1126 - accuracy: 0.9604 - val_loss: 0.1493 - val_accuracy: 0.9450\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9558 - val_loss: 0.1492 - val_accuracy: 0.9465\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9580 - val_loss: 0.1481 - val_accuracy: 0.9465\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1227 - accuracy: 0.9542 - val_loss: 0.1481 - val_accuracy: 0.9460\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9600 - val_loss: 0.1480 - val_accuracy: 0.9465\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9605 - val_loss: 0.1487 - val_accuracy: 0.9465\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9591 - val_loss: 0.1493 - val_accuracy: 0.9450\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9601 - val_loss: 0.1474 - val_accuracy: 0.9475\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9580 - val_loss: 0.1471 - val_accuracy: 0.9465\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9586 - val_loss: 0.1471 - val_accuracy: 0.9470\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9607 - val_loss: 0.1495 - val_accuracy: 0.9440\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9564 - val_loss: 0.1466 - val_accuracy: 0.9470\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9572 - val_loss: 0.1462 - val_accuracy: 0.9475\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9571 - val_loss: 0.1460 - val_accuracy: 0.9480\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9572 - val_loss: 0.1460 - val_accuracy: 0.9490\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9584 - val_loss: 0.1455 - val_accuracy: 0.9480\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1158 - accuracy: 0.9580 - val_loss: 0.1462 - val_accuracy: 0.9485\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9581 - val_loss: 0.1455 - val_accuracy: 0.9485\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9609 - val_loss: 0.1450 - val_accuracy: 0.9480\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9607 - val_loss: 0.1451 - val_accuracy: 0.9490\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9590 - val_loss: 0.1446 - val_accuracy: 0.9485\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9640 - val_loss: 0.1443 - val_accuracy: 0.9485\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9634 - val_loss: 0.1445 - val_accuracy: 0.9485\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1125 - accuracy: 0.9602 - val_loss: 0.1443 - val_accuracy: 0.9480\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9577 - val_loss: 0.1439 - val_accuracy: 0.9490\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9589 - val_loss: 0.1440 - val_accuracy: 0.9480\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9642 - val_loss: 0.1439 - val_accuracy: 0.9475\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9574 - val_loss: 0.1446 - val_accuracy: 0.9475\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9614 - val_loss: 0.1427 - val_accuracy: 0.9505\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9622 - val_loss: 0.1431 - val_accuracy: 0.9495\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9598 - val_loss: 0.1426 - val_accuracy: 0.9495\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.9646 - val_loss: 0.1425 - val_accuracy: 0.9490\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9617 - val_loss: 0.1437 - val_accuracy: 0.9475\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9631 - val_loss: 0.1416 - val_accuracy: 0.9510\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9622 - val_loss: 0.1423 - val_accuracy: 0.9490\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9625 - val_loss: 0.1414 - val_accuracy: 0.9505\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.9606 - val_loss: 0.1418 - val_accuracy: 0.9500\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9627 - val_loss: 0.1413 - val_accuracy: 0.9495\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9612 - val_loss: 0.1410 - val_accuracy: 0.9505\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9634 - val_loss: 0.1427 - val_accuracy: 0.9480\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9635 - val_loss: 0.1411 - val_accuracy: 0.9500\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9635 - val_loss: 0.1401 - val_accuracy: 0.9495\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 0.9603 - val_loss: 0.1403 - val_accuracy: 0.9495\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9663 - val_loss: 0.1427 - val_accuracy: 0.9460\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9606 - val_loss: 0.1466 - val_accuracy: 0.9485\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9650 - val_loss: 0.1421 - val_accuracy: 0.9460\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9630 - val_loss: 0.1395 - val_accuracy: 0.9515\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1391 - val_accuracy: 0.9515\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9662 - val_loss: 0.1390 - val_accuracy: 0.9530\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9632 - val_loss: 0.1407 - val_accuracy: 0.9475\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9664 - val_loss: 0.1385 - val_accuracy: 0.9510\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1019 - accuracy: 0.9645 - val_loss: 0.1386 - val_accuracy: 0.9495\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9668 - val_loss: 0.1389 - val_accuracy: 0.9480\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9663 - val_loss: 0.1376 - val_accuracy: 0.9520\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1005 - accuracy: 0.9668 - val_loss: 0.1375 - val_accuracy: 0.9525\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9674 - val_loss: 0.1373 - val_accuracy: 0.9525\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9639 - val_loss: 0.1371 - val_accuracy: 0.9520\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9629 - val_loss: 0.1370 - val_accuracy: 0.9515\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9653 - val_loss: 0.1377 - val_accuracy: 0.9495\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9669 - val_loss: 0.1388 - val_accuracy: 0.9490\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9644 - val_loss: 0.1382 - val_accuracy: 0.9505\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9640 - val_loss: 0.1377 - val_accuracy: 0.9500\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9651 - val_loss: 0.1367 - val_accuracy: 0.9505\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9671 - val_loss: 0.1358 - val_accuracy: 0.9520\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9644 - val_loss: 0.1361 - val_accuracy: 0.9515\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0967 - accuracy: 0.9678 - val_loss: 0.1357 - val_accuracy: 0.9525\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9677 - val_loss: 0.1361 - val_accuracy: 0.9510\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9655 - val_loss: 0.1354 - val_accuracy: 0.9510\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9675 - val_loss: 0.1357 - val_accuracy: 0.9510\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9647 - val_loss: 0.1351 - val_accuracy: 0.9520\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9658 - val_loss: 0.1348 - val_accuracy: 0.9510\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9684 - val_loss: 0.1349 - val_accuracy: 0.9515\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9677 - val_loss: 0.1344 - val_accuracy: 0.9515\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9672 - val_loss: 0.1341 - val_accuracy: 0.9525\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9651 - val_loss: 0.1342 - val_accuracy: 0.9500\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9682 - val_loss: 0.1348 - val_accuracy: 0.9515\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9700 - val_loss: 0.1395 - val_accuracy: 0.9475\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9656 - val_loss: 0.1337 - val_accuracy: 0.9515\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9676 - val_loss: 0.1339 - val_accuracy: 0.9525\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9690 - val_loss: 0.1334 - val_accuracy: 0.9525\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9696 - val_loss: 0.1352 - val_accuracy: 0.9525\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9687 - val_loss: 0.1340 - val_accuracy: 0.9510\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9693 - val_loss: 0.1348 - val_accuracy: 0.9525\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9698 - val_loss: 0.1348 - val_accuracy: 0.9525\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9676 - val_loss: 0.1322 - val_accuracy: 0.9515\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0941 - accuracy: 0.9708 - val_loss: 0.1332 - val_accuracy: 0.9540\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9674 - val_loss: 0.1331 - val_accuracy: 0.9520\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9711 - val_loss: 0.1323 - val_accuracy: 0.9540\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9680 - val_loss: 0.1318 - val_accuracy: 0.9515\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9679 - val_loss: 0.1318 - val_accuracy: 0.9525\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9688 - val_loss: 0.1316 - val_accuracy: 0.9515\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9694 - val_loss: 0.1313 - val_accuracy: 0.9520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTbTkk3Fpu1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac86245c-35f3-4ac6-abb6-4c92ef2e28aa"
      },
      "source": [
        "# with dropout\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history_2 = model.fit(x_train_binary, y_train_binary, batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(x_test_binary, y_test_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 0.5292 - accuracy: 0.7556 - val_loss: 0.3137 - val_accuracy: 0.8820\n",
            "Epoch 2/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2936 - accuracy: 0.8932 - val_loss: 0.2659 - val_accuracy: 0.8940\n",
            "Epoch 3/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.8965 - val_loss: 0.2419 - val_accuracy: 0.9035\n",
            "Epoch 4/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2355 - accuracy: 0.9066 - val_loss: 0.2281 - val_accuracy: 0.9120\n",
            "Epoch 5/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2102 - accuracy: 0.9172 - val_loss: 0.2175 - val_accuracy: 0.9190\n",
            "Epoch 6/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2147 - accuracy: 0.9145 - val_loss: 0.2118 - val_accuracy: 0.9215\n",
            "Epoch 7/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.2094 - accuracy: 0.9152 - val_loss: 0.2054 - val_accuracy: 0.9235\n",
            "Epoch 8/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9190 - val_loss: 0.2007 - val_accuracy: 0.9270\n",
            "Epoch 9/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1943 - accuracy: 0.9241 - val_loss: 0.1974 - val_accuracy: 0.9295\n",
            "Epoch 10/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1921 - accuracy: 0.9244 - val_loss: 0.1948 - val_accuracy: 0.9295\n",
            "Epoch 11/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9258 - val_loss: 0.1916 - val_accuracy: 0.9305\n",
            "Epoch 12/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1858 - accuracy: 0.9307 - val_loss: 0.1909 - val_accuracy: 0.9300\n",
            "Epoch 13/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9273 - val_loss: 0.1880 - val_accuracy: 0.9315\n",
            "Epoch 14/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1794 - accuracy: 0.9280 - val_loss: 0.1864 - val_accuracy: 0.9315\n",
            "Epoch 15/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9326 - val_loss: 0.1854 - val_accuracy: 0.9305\n",
            "Epoch 16/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9314 - val_loss: 0.1830 - val_accuracy: 0.9330\n",
            "Epoch 17/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1771 - accuracy: 0.9312 - val_loss: 0.1821 - val_accuracy: 0.9370\n",
            "Epoch 18/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1732 - accuracy: 0.9306 - val_loss: 0.1804 - val_accuracy: 0.9365\n",
            "Epoch 19/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9306 - val_loss: 0.1795 - val_accuracy: 0.9340\n",
            "Epoch 20/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9344 - val_loss: 0.1788 - val_accuracy: 0.9330\n",
            "Epoch 21/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.9377 - val_loss: 0.1775 - val_accuracy: 0.9345\n",
            "Epoch 22/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1732 - accuracy: 0.9314 - val_loss: 0.1766 - val_accuracy: 0.9365\n",
            "Epoch 23/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1638 - accuracy: 0.9382 - val_loss: 0.1761 - val_accuracy: 0.9340\n",
            "Epoch 24/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1663 - accuracy: 0.9346 - val_loss: 0.1750 - val_accuracy: 0.9365\n",
            "Epoch 25/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1639 - accuracy: 0.9391 - val_loss: 0.1746 - val_accuracy: 0.9330\n",
            "Epoch 26/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9381 - val_loss: 0.1732 - val_accuracy: 0.9370\n",
            "Epoch 27/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9348 - val_loss: 0.1726 - val_accuracy: 0.9365\n",
            "Epoch 28/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.9365 - val_loss: 0.1722 - val_accuracy: 0.9350\n",
            "Epoch 29/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9422 - val_loss: 0.1714 - val_accuracy: 0.9365\n",
            "Epoch 30/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1693 - accuracy: 0.9365 - val_loss: 0.1707 - val_accuracy: 0.9360\n",
            "Epoch 31/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1533 - accuracy: 0.9404 - val_loss: 0.1701 - val_accuracy: 0.9355\n",
            "Epoch 32/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9400 - val_loss: 0.1714 - val_accuracy: 0.9395\n",
            "Epoch 33/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1566 - accuracy: 0.9391 - val_loss: 0.1687 - val_accuracy: 0.9355\n",
            "Epoch 34/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9405 - val_loss: 0.1689 - val_accuracy: 0.9335\n",
            "Epoch 35/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9407 - val_loss: 0.1673 - val_accuracy: 0.9375\n",
            "Epoch 36/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9440 - val_loss: 0.1674 - val_accuracy: 0.9350\n",
            "Epoch 37/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9411 - val_loss: 0.1664 - val_accuracy: 0.9370\n",
            "Epoch 38/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1564 - accuracy: 0.9403 - val_loss: 0.1664 - val_accuracy: 0.9385\n",
            "Epoch 39/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9396 - val_loss: 0.1653 - val_accuracy: 0.9375\n",
            "Epoch 40/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1501 - accuracy: 0.9436 - val_loss: 0.1662 - val_accuracy: 0.9340\n",
            "Epoch 41/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9426 - val_loss: 0.1642 - val_accuracy: 0.9370\n",
            "Epoch 42/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.9396 - val_loss: 0.1638 - val_accuracy: 0.9375\n",
            "Epoch 43/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.9448 - val_loss: 0.1632 - val_accuracy: 0.9375\n",
            "Epoch 44/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9428 - val_loss: 0.1629 - val_accuracy: 0.9380\n",
            "Epoch 45/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.9454 - val_loss: 0.1622 - val_accuracy: 0.9380\n",
            "Epoch 46/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9424 - val_loss: 0.1636 - val_accuracy: 0.9385\n",
            "Epoch 47/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9434 - val_loss: 0.1615 - val_accuracy: 0.9375\n",
            "Epoch 48/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9470 - val_loss: 0.1612 - val_accuracy: 0.9385\n",
            "Epoch 49/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9469 - val_loss: 0.1609 - val_accuracy: 0.9380\n",
            "Epoch 50/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9451 - val_loss: 0.1597 - val_accuracy: 0.9395\n",
            "Epoch 51/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.9430 - val_loss: 0.1603 - val_accuracy: 0.9385\n",
            "Epoch 52/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9453 - val_loss: 0.1587 - val_accuracy: 0.9390\n",
            "Epoch 53/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9454 - val_loss: 0.1587 - val_accuracy: 0.9370\n",
            "Epoch 54/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9479 - val_loss: 0.1578 - val_accuracy: 0.9390\n",
            "Epoch 55/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.9465 - val_loss: 0.1574 - val_accuracy: 0.9395\n",
            "Epoch 56/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1417 - accuracy: 0.9458 - val_loss: 0.1567 - val_accuracy: 0.9390\n",
            "Epoch 57/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1357 - accuracy: 0.9510 - val_loss: 0.1583 - val_accuracy: 0.9395\n",
            "Epoch 58/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1354 - accuracy: 0.9510 - val_loss: 0.1569 - val_accuracy: 0.9385\n",
            "Epoch 59/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9512 - val_loss: 0.1558 - val_accuracy: 0.9385\n",
            "Epoch 60/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9446 - val_loss: 0.1553 - val_accuracy: 0.9380\n",
            "Epoch 61/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9476 - val_loss: 0.1551 - val_accuracy: 0.9395\n",
            "Epoch 62/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9477 - val_loss: 0.1554 - val_accuracy: 0.9380\n",
            "Epoch 63/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9469 - val_loss: 0.1542 - val_accuracy: 0.9395\n",
            "Epoch 64/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9504 - val_loss: 0.1538 - val_accuracy: 0.9385\n",
            "Epoch 65/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9498 - val_loss: 0.1535 - val_accuracy: 0.9400\n",
            "Epoch 66/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9516 - val_loss: 0.1534 - val_accuracy: 0.9410\n",
            "Epoch 67/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9470 - val_loss: 0.1527 - val_accuracy: 0.9390\n",
            "Epoch 68/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9507 - val_loss: 0.1520 - val_accuracy: 0.9390\n",
            "Epoch 69/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9535 - val_loss: 0.1517 - val_accuracy: 0.9390\n",
            "Epoch 70/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9478 - val_loss: 0.1514 - val_accuracy: 0.9405\n",
            "Epoch 71/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9515 - val_loss: 0.1513 - val_accuracy: 0.9385\n",
            "Epoch 72/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9493 - val_loss: 0.1511 - val_accuracy: 0.9420\n",
            "Epoch 73/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1400 - accuracy: 0.9502 - val_loss: 0.1528 - val_accuracy: 0.9430\n",
            "Epoch 74/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9493 - val_loss: 0.1501 - val_accuracy: 0.9415\n",
            "Epoch 75/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1289 - accuracy: 0.9526 - val_loss: 0.1495 - val_accuracy: 0.9410\n",
            "Epoch 76/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9509 - val_loss: 0.1496 - val_accuracy: 0.9400\n",
            "Epoch 77/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9500 - val_loss: 0.1493 - val_accuracy: 0.9405\n",
            "Epoch 78/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9553 - val_loss: 0.1486 - val_accuracy: 0.9420\n",
            "Epoch 79/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.9520 - val_loss: 0.1480 - val_accuracy: 0.9430\n",
            "Epoch 80/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1252 - accuracy: 0.9551 - val_loss: 0.1486 - val_accuracy: 0.9410\n",
            "Epoch 81/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1264 - accuracy: 0.9531 - val_loss: 0.1476 - val_accuracy: 0.9435\n",
            "Epoch 82/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.9557 - val_loss: 0.1475 - val_accuracy: 0.9410\n",
            "Epoch 83/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1287 - accuracy: 0.9551 - val_loss: 0.1468 - val_accuracy: 0.9430\n",
            "Epoch 84/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1241 - accuracy: 0.9550 - val_loss: 0.1472 - val_accuracy: 0.9440\n",
            "Epoch 85/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9536 - val_loss: 0.1468 - val_accuracy: 0.9420\n",
            "Epoch 86/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1236 - accuracy: 0.9536 - val_loss: 0.1464 - val_accuracy: 0.9440\n",
            "Epoch 87/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1232 - accuracy: 0.9551 - val_loss: 0.1456 - val_accuracy: 0.9430\n",
            "Epoch 88/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1214 - accuracy: 0.9554 - val_loss: 0.1450 - val_accuracy: 0.9440\n",
            "Epoch 89/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9537 - val_loss: 0.1463 - val_accuracy: 0.9440\n",
            "Epoch 90/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9567 - val_loss: 0.1449 - val_accuracy: 0.9435\n",
            "Epoch 91/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9547 - val_loss: 0.1444 - val_accuracy: 0.9445\n",
            "Epoch 92/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9562 - val_loss: 0.1449 - val_accuracy: 0.9445\n",
            "Epoch 93/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9553 - val_loss: 0.1435 - val_accuracy: 0.9435\n",
            "Epoch 94/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9572 - val_loss: 0.1433 - val_accuracy: 0.9440\n",
            "Epoch 95/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9572 - val_loss: 0.1431 - val_accuracy: 0.9445\n",
            "Epoch 96/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9597 - val_loss: 0.1426 - val_accuracy: 0.9440\n",
            "Epoch 97/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9567 - val_loss: 0.1423 - val_accuracy: 0.9455\n",
            "Epoch 98/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9572 - val_loss: 0.1426 - val_accuracy: 0.9455\n",
            "Epoch 99/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.9571 - val_loss: 0.1415 - val_accuracy: 0.9460\n",
            "Epoch 100/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9529 - val_loss: 0.1418 - val_accuracy: 0.9455\n",
            "Epoch 101/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9581 - val_loss: 0.1413 - val_accuracy: 0.9465\n",
            "Epoch 102/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9614 - val_loss: 0.1413 - val_accuracy: 0.9445\n",
            "Epoch 103/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9565 - val_loss: 0.1406 - val_accuracy: 0.9450\n",
            "Epoch 104/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9583 - val_loss: 0.1407 - val_accuracy: 0.9465\n",
            "Epoch 105/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9585 - val_loss: 0.1402 - val_accuracy: 0.9460\n",
            "Epoch 106/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 0.9580 - val_loss: 0.1398 - val_accuracy: 0.9470\n",
            "Epoch 107/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9596 - val_loss: 0.1393 - val_accuracy: 0.9470\n",
            "Epoch 108/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9588 - val_loss: 0.1398 - val_accuracy: 0.9480\n",
            "Epoch 109/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9634 - val_loss: 0.1394 - val_accuracy: 0.9465\n",
            "Epoch 110/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9586 - val_loss: 0.1401 - val_accuracy: 0.9485\n",
            "Epoch 111/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9598 - val_loss: 0.1381 - val_accuracy: 0.9470\n",
            "Epoch 112/200\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 0.1196 - accuracy: 0.9561 - val_loss: 0.1383 - val_accuracy: 0.9465\n",
            "Epoch 113/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9613 - val_loss: 0.1377 - val_accuracy: 0.9470\n",
            "Epoch 114/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9630 - val_loss: 0.1378 - val_accuracy: 0.9465\n",
            "Epoch 115/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9613 - val_loss: 0.1381 - val_accuracy: 0.9485\n",
            "Epoch 116/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9618 - val_loss: 0.1373 - val_accuracy: 0.9470\n",
            "Epoch 117/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9594 - val_loss: 0.1369 - val_accuracy: 0.9470\n",
            "Epoch 118/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9584 - val_loss: 0.1364 - val_accuracy: 0.9480\n",
            "Epoch 119/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9627 - val_loss: 0.1368 - val_accuracy: 0.9495\n",
            "Epoch 120/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9605 - val_loss: 0.1358 - val_accuracy: 0.9485\n",
            "Epoch 121/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9592 - val_loss: 0.1358 - val_accuracy: 0.9490\n",
            "Epoch 122/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 0.9587 - val_loss: 0.1353 - val_accuracy: 0.9485\n",
            "Epoch 123/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1116 - accuracy: 0.9602 - val_loss: 0.1356 - val_accuracy: 0.9490\n",
            "Epoch 124/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9593 - val_loss: 0.1348 - val_accuracy: 0.9495\n",
            "Epoch 125/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1119 - accuracy: 0.9608 - val_loss: 0.1349 - val_accuracy: 0.9500\n",
            "Epoch 126/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9629 - val_loss: 0.1341 - val_accuracy: 0.9500\n",
            "Epoch 127/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9635 - val_loss: 0.1342 - val_accuracy: 0.9495\n",
            "Epoch 128/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1104 - accuracy: 0.9636 - val_loss: 0.1345 - val_accuracy: 0.9495\n",
            "Epoch 129/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9620 - val_loss: 0.1338 - val_accuracy: 0.9495\n",
            "Epoch 130/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9638 - val_loss: 0.1335 - val_accuracy: 0.9510\n",
            "Epoch 131/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9661 - val_loss: 0.1345 - val_accuracy: 0.9505\n",
            "Epoch 132/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9613 - val_loss: 0.1329 - val_accuracy: 0.9490\n",
            "Epoch 133/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9612 - val_loss: 0.1329 - val_accuracy: 0.9495\n",
            "Epoch 134/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1086 - accuracy: 0.9620 - val_loss: 0.1325 - val_accuracy: 0.9495\n",
            "Epoch 135/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9662 - val_loss: 0.1321 - val_accuracy: 0.9490\n",
            "Epoch 136/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9605 - val_loss: 0.1328 - val_accuracy: 0.9500\n",
            "Epoch 137/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.9610 - val_loss: 0.1315 - val_accuracy: 0.9510\n",
            "Epoch 138/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9628 - val_loss: 0.1314 - val_accuracy: 0.9530\n",
            "Epoch 139/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9627 - val_loss: 0.1312 - val_accuracy: 0.9515\n",
            "Epoch 140/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9660 - val_loss: 0.1308 - val_accuracy: 0.9510\n",
            "Epoch 141/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9656 - val_loss: 0.1318 - val_accuracy: 0.9515\n",
            "Epoch 142/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9664 - val_loss: 0.1304 - val_accuracy: 0.9520\n",
            "Epoch 143/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9677 - val_loss: 0.1305 - val_accuracy: 0.9520\n",
            "Epoch 144/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9650 - val_loss: 0.1304 - val_accuracy: 0.9520\n",
            "Epoch 145/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9643 - val_loss: 0.1304 - val_accuracy: 0.9505\n",
            "Epoch 146/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.9654 - val_loss: 0.1298 - val_accuracy: 0.9525\n",
            "Epoch 147/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9643 - val_loss: 0.1299 - val_accuracy: 0.9525\n",
            "Epoch 148/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1011 - accuracy: 0.9664 - val_loss: 0.1301 - val_accuracy: 0.9525\n",
            "Epoch 149/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9677 - val_loss: 0.1288 - val_accuracy: 0.9520\n",
            "Epoch 150/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.9645 - val_loss: 0.1288 - val_accuracy: 0.9520\n",
            "Epoch 151/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9619 - val_loss: 0.1283 - val_accuracy: 0.9540\n",
            "Epoch 152/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9656 - val_loss: 0.1293 - val_accuracy: 0.9515\n",
            "Epoch 153/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9634 - val_loss: 0.1302 - val_accuracy: 0.9530\n",
            "Epoch 154/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9674 - val_loss: 0.1294 - val_accuracy: 0.9530\n",
            "Epoch 155/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9664 - val_loss: 0.1284 - val_accuracy: 0.9520\n",
            "Epoch 156/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9636 - val_loss: 0.1276 - val_accuracy: 0.9530\n",
            "Epoch 157/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9691 - val_loss: 0.1273 - val_accuracy: 0.9540\n",
            "Epoch 158/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9637 - val_loss: 0.1283 - val_accuracy: 0.9525\n",
            "Epoch 159/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9686 - val_loss: 0.1269 - val_accuracy: 0.9535\n",
            "Epoch 160/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9640 - val_loss: 0.1265 - val_accuracy: 0.9545\n",
            "Epoch 161/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9676 - val_loss: 0.1264 - val_accuracy: 0.9525\n",
            "Epoch 162/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9675 - val_loss: 0.1261 - val_accuracy: 0.9530\n",
            "Epoch 163/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.1257 - val_accuracy: 0.9540\n",
            "Epoch 164/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9691 - val_loss: 0.1258 - val_accuracy: 0.9550\n",
            "Epoch 165/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9668 - val_loss: 0.1260 - val_accuracy: 0.9530\n",
            "Epoch 166/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9622 - val_loss: 0.1255 - val_accuracy: 0.9545\n",
            "Epoch 167/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9680 - val_loss: 0.1256 - val_accuracy: 0.9550\n",
            "Epoch 168/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9689 - val_loss: 0.1255 - val_accuracy: 0.9530\n",
            "Epoch 169/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9667 - val_loss: 0.1251 - val_accuracy: 0.9550\n",
            "Epoch 170/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9677 - val_loss: 0.1252 - val_accuracy: 0.9555\n",
            "Epoch 171/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9705 - val_loss: 0.1254 - val_accuracy: 0.9560\n",
            "Epoch 172/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9681 - val_loss: 0.1244 - val_accuracy: 0.9545\n",
            "Epoch 173/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9683 - val_loss: 0.1247 - val_accuracy: 0.9545\n",
            "Epoch 174/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0931 - accuracy: 0.9707 - val_loss: 0.1244 - val_accuracy: 0.9550\n",
            "Epoch 175/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9700 - val_loss: 0.1247 - val_accuracy: 0.9555\n",
            "Epoch 176/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9668 - val_loss: 0.1242 - val_accuracy: 0.9555\n",
            "Epoch 177/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9701 - val_loss: 0.1238 - val_accuracy: 0.9555\n",
            "Epoch 178/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9666 - val_loss: 0.1243 - val_accuracy: 0.9540\n",
            "Epoch 179/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9668 - val_loss: 0.1237 - val_accuracy: 0.9550\n",
            "Epoch 180/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9696 - val_loss: 0.1240 - val_accuracy: 0.9540\n",
            "Epoch 181/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9677 - val_loss: 0.1227 - val_accuracy: 0.9560\n",
            "Epoch 182/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0928 - accuracy: 0.9689 - val_loss: 0.1232 - val_accuracy: 0.9545\n",
            "Epoch 183/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9678 - val_loss: 0.1228 - val_accuracy: 0.9560\n",
            "Epoch 184/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9709 - val_loss: 0.1229 - val_accuracy: 0.9560\n",
            "Epoch 185/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0894 - accuracy: 0.9714 - val_loss: 0.1248 - val_accuracy: 0.9555\n",
            "Epoch 186/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9675 - val_loss: 0.1227 - val_accuracy: 0.9555\n",
            "Epoch 187/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9688 - val_loss: 0.1226 - val_accuracy: 0.9550\n",
            "Epoch 188/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.9713 - val_loss: 0.1224 - val_accuracy: 0.9565\n",
            "Epoch 189/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9700 - val_loss: 0.1224 - val_accuracy: 0.9560\n",
            "Epoch 190/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9700 - val_loss: 0.1219 - val_accuracy: 0.9570\n",
            "Epoch 191/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.9696 - val_loss: 0.1226 - val_accuracy: 0.9540\n",
            "Epoch 192/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9718 - val_loss: 0.1220 - val_accuracy: 0.9545\n",
            "Epoch 193/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9713 - val_loss: 0.1209 - val_accuracy: 0.9565\n",
            "Epoch 194/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9707 - val_loss: 0.1212 - val_accuracy: 0.9565\n",
            "Epoch 195/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.9704 - val_loss: 0.1210 - val_accuracy: 0.9570\n",
            "Epoch 196/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0848 - accuracy: 0.9734 - val_loss: 0.1209 - val_accuracy: 0.9555\n",
            "Epoch 197/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9700 - val_loss: 0.1203 - val_accuracy: 0.9570\n",
            "Epoch 198/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9691 - val_loss: 0.1204 - val_accuracy: 0.9570\n",
            "Epoch 199/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9698 - val_loss: 0.1200 - val_accuracy: 0.9575\n",
            "Epoch 200/200\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9681 - val_loss: 0.1201 - val_accuracy: 0.9575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZrVlg-BHkES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "499346f8-c326-47e0-ff16-b22ce9adaa73"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_loss'])\n",
        "plt.plot(history_2.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jggp33GTHkN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a23adf88-08ab-4a61-c77d-4a18f7ddc514"
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history_1.history['val_accuracy'])\n",
        "plt.plot(history_2.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['test', 'test_dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lBU4uykrOqH"
      },
      "source": [
        "# Validation Set (The CORRECT way of training and evaluation process)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hz7bgmgH4lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690ea0e8-715e-4854-9da4-73dc7a762638"
      },
      "source": [
        "#cross validation example\n",
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "kfold = KFold(5, True, 2) # number of folds, shuffle, seed\n",
        "\n",
        "# enumerate splits\n",
        "for train, val in kfold.split(x_train_binary):\n",
        "\tprint('train: %s, val: %s' % (train, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [    0     1     3 ... 11997 11998 11999], val: [    2     4    10 ... 11991 11992 11994]\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    3     5     6 ... 11988 11989 11993]\n",
            "train: [    1     2     3 ... 11994 11995 11996], val: [    0    13    19 ... 11997 11998 11999]\n",
            "train: [    0     1     2 ... 11997 11998 11999], val: [    7     9    21 ... 11948 11952 11955]\n",
            "train: [    0     2     3 ... 11997 11998 11999], val: [    1    12    14 ... 11985 11995 11996]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqB1wQNMPfQ"
      },
      "source": [
        "X_train_binary = x_train_binary[train]\n",
        "Y_train_binary = y_train_binary[train]\n",
        "X_val_binary = x_train_binary[val]\n",
        "Y_val_binary = y_train_binary[val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVfanC2yMwAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5cbef2-734b-4132-c641-aaa5b72891b6"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint #save the model version that achieved lower loss!\n",
        "\n",
        "seed = 2\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "# save best model version\n",
        "save_model = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=2)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_3 = model.fit(X_train_binary, Y_train_binary, batch_size=batch_size, epochs=epochs, callbacks=[save_model],\n",
        "          validation_data=(X_val_binary, Y_val_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "75/75 [==============================] - 1s 6ms/step - loss: 0.3040 - accuracy: 0.8679 - val_loss: 0.1889 - val_accuracy: 0.9250\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.18887, saving model to best_model.h5\n",
            "Epoch 2/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.9282 - val_loss: 0.1643 - val_accuracy: 0.9383\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.18887 to 0.16427, saving model to best_model.h5\n",
            "Epoch 3/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9432 - val_loss: 0.1463 - val_accuracy: 0.9425\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.16427 to 0.14632, saving model to best_model.h5\n",
            "Epoch 4/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9533 - val_loss: 0.1380 - val_accuracy: 0.9467\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.14632 to 0.13803, saving model to best_model.h5\n",
            "Epoch 5/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9545 - val_loss: 0.1339 - val_accuracy: 0.9546\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.13803 to 0.13390, saving model to best_model.h5\n",
            "Epoch 6/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9563 - val_loss: 0.1260 - val_accuracy: 0.9604\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.13390 to 0.12603, saving model to best_model.h5\n",
            "Epoch 7/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9613 - val_loss: 0.1237 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.12603 to 0.12371, saving model to best_model.h5\n",
            "Epoch 8/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9658 - val_loss: 0.1147 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.12371 to 0.11473, saving model to best_model.h5\n",
            "Epoch 9/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9648 - val_loss: 0.1126 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.11473 to 0.11255, saving model to best_model.h5\n",
            "Epoch 10/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9714 - val_loss: 0.1173 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.11255\n",
            "Epoch 11/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9736 - val_loss: 0.1193 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.11255\n",
            "Epoch 12/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9722 - val_loss: 0.1115 - val_accuracy: 0.9629\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.11255 to 0.11151, saving model to best_model.h5\n",
            "Epoch 13/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9705 - val_loss: 0.1095 - val_accuracy: 0.9650\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.11151 to 0.10950, saving model to best_model.h5\n",
            "Epoch 14/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9750 - val_loss: 0.1089 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.10950 to 0.10895, saving model to best_model.h5\n",
            "Epoch 15/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9738 - val_loss: 0.1116 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.10895\n",
            "Epoch 16/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0712 - accuracy: 0.9745 - val_loss: 0.1056 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.10895 to 0.10562, saving model to best_model.h5\n",
            "Epoch 17/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9785 - val_loss: 0.1021 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.10562 to 0.10210, saving model to best_model.h5\n",
            "Epoch 18/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9758 - val_loss: 0.1155 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.10210\n",
            "Epoch 19/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9792 - val_loss: 0.1151 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.10210\n",
            "Epoch 20/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9796 - val_loss: 0.1069 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.10210\n",
            "Epoch 21/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9804 - val_loss: 0.1073 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.10210\n",
            "Epoch 22/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9802 - val_loss: 0.1200 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.10210\n",
            "Epoch 23/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9790 - val_loss: 0.1068 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.10210\n",
            "Epoch 24/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9846 - val_loss: 0.0993 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.10210 to 0.09927, saving model to best_model.h5\n",
            "Epoch 25/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9849 - val_loss: 0.1031 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.09927\n",
            "Epoch 26/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.1074 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.09927\n",
            "Epoch 27/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9813 - val_loss: 0.1071 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.09927\n",
            "Epoch 28/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9841 - val_loss: 0.1063 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.09927\n",
            "Epoch 29/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9837 - val_loss: 0.1084 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.09927\n",
            "Epoch 30/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.9844 - val_loss: 0.1167 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.09927\n",
            "Epoch 31/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.9846 - val_loss: 0.1086 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.09927\n",
            "Epoch 32/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.1068 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.09927\n",
            "Epoch 33/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9902 - val_loss: 0.1105 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.09927\n",
            "Epoch 34/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.9856 - val_loss: 0.1149 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.09927\n",
            "Epoch 35/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9886 - val_loss: 0.1343 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.09927\n",
            "Epoch 36/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 0.1116 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.09927\n",
            "Epoch 37/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.1186 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.09927\n",
            "Epoch 38/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9871 - val_loss: 0.1158 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.09927\n",
            "Epoch 39/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 0.1443 - val_accuracy: 0.9596\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.09927\n",
            "Epoch 40/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 0.1219 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.09927\n",
            "Epoch 41/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.1273 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.09927\n",
            "Epoch 42/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.1177 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.09927\n",
            "Epoch 43/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 0.1200 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.09927\n",
            "Epoch 44/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 0.1379 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.09927\n",
            "Epoch 45/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9904 - val_loss: 0.1187 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.09927\n",
            "Epoch 46/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.1316 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.09927\n",
            "Epoch 47/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.1292 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.09927\n",
            "Epoch 48/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.1278 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.09927\n",
            "Epoch 49/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 0.1247 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.09927\n",
            "Epoch 50/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9906 - val_loss: 0.1317 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.09927\n",
            "Epoch 51/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9902 - val_loss: 0.1237 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.09927\n",
            "Epoch 52/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.1338 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.09927\n",
            "Epoch 53/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9919 - val_loss: 0.1292 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.09927\n",
            "Epoch 54/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 0.1276 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.09927\n",
            "Epoch 55/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.1532 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.09927\n",
            "Epoch 56/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.1316 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.09927\n",
            "Epoch 57/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9955 - val_loss: 0.1429 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.09927\n",
            "Epoch 58/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.1509 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.09927\n",
            "Epoch 59/200\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9931 - val_loss: 0.1320 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.09927\n",
            "Epoch 60/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.9914 - val_loss: 0.1479 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.09927\n",
            "Epoch 61/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.1413 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.09927\n",
            "Epoch 62/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.1428 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.09927\n",
            "Epoch 63/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.1520 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.09927\n",
            "Epoch 64/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9942 - val_loss: 0.1403 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.09927\n",
            "Epoch 65/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.1380 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.09927\n",
            "Epoch 66/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9922 - val_loss: 0.1526 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.09927\n",
            "Epoch 67/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9935 - val_loss: 0.1452 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.09927\n",
            "Epoch 68/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.1442 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.09927\n",
            "Epoch 69/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.1415 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.09927\n",
            "Epoch 70/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 0.1564 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.09927\n",
            "Epoch 71/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.1580 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.09927\n",
            "Epoch 72/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.1570 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.09927\n",
            "Epoch 73/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.1571 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.09927\n",
            "Epoch 74/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9965 - val_loss: 0.1459 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.09927\n",
            "Epoch 75/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.1442 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.09927\n",
            "Epoch 76/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.1558 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.09927\n",
            "Epoch 77/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.1659 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.09927\n",
            "Epoch 78/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9937 - val_loss: 0.1627 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.09927\n",
            "Epoch 79/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.1618 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.09927\n",
            "Epoch 80/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.1621 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.09927\n",
            "Epoch 81/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.1628 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.09927\n",
            "Epoch 82/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.1625 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.09927\n",
            "Epoch 83/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9960 - val_loss: 0.1638 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.09927\n",
            "Epoch 84/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.1553 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.09927\n",
            "Epoch 85/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.1810 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.09927\n",
            "Epoch 86/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.1753 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.09927\n",
            "Epoch 87/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.1759 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.09927\n",
            "Epoch 88/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.1682 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.09927\n",
            "Epoch 89/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9953 - val_loss: 0.1672 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.09927\n",
            "Epoch 90/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9956 - val_loss: 0.1829 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.09927\n",
            "Epoch 91/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9969 - val_loss: 0.1765 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.09927\n",
            "Epoch 92/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9949 - val_loss: 0.1743 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.09927\n",
            "Epoch 93/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.1733 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.09927\n",
            "Epoch 94/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.1702 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.09927\n",
            "Epoch 95/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.1758 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.09927\n",
            "Epoch 96/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.1832 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.09927\n",
            "Epoch 97/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.1973 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.09927\n",
            "Epoch 98/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9940 - val_loss: 0.1753 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.09927\n",
            "Epoch 99/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.1759 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.09927\n",
            "Epoch 100/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.1808 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.09927\n",
            "Epoch 101/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9968 - val_loss: 0.1824 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.09927\n",
            "Epoch 102/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.1872 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.09927\n",
            "Epoch 103/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.1973 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.09927\n",
            "Epoch 104/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 0.1855 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.09927\n",
            "Epoch 105/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.1950 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.09927\n",
            "Epoch 106/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.1987 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.09927\n",
            "Epoch 107/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9942 - val_loss: 0.1908 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.09927\n",
            "Epoch 108/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.1839 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.09927\n",
            "Epoch 109/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9931 - val_loss: 0.1773 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.09927\n",
            "Epoch 110/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.1743 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.09927\n",
            "Epoch 111/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.1873 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.09927\n",
            "Epoch 112/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 0.1888 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.09927\n",
            "Epoch 113/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.1918 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.09927\n",
            "Epoch 114/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 0.1923 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.09927\n",
            "Epoch 115/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.1946 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.09927\n",
            "Epoch 116/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9975 - val_loss: 0.1995 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.09927\n",
            "Epoch 117/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.2018 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.09927\n",
            "Epoch 118/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.1906 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.09927\n",
            "Epoch 119/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 0.2002 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.09927\n",
            "Epoch 120/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.2174 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.09927\n",
            "Epoch 121/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9957 - val_loss: 0.2231 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.09927\n",
            "Epoch 122/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.2066 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.09927\n",
            "Epoch 123/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 0.2065 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.09927\n",
            "Epoch 124/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2032 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.09927\n",
            "Epoch 125/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9978 - val_loss: 0.1963 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.09927\n",
            "Epoch 126/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9960 - val_loss: 0.2028 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.09927\n",
            "Epoch 127/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.2127 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.09927\n",
            "Epoch 128/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 0.2037 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.09927\n",
            "Epoch 129/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.2345 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.09927\n",
            "Epoch 130/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.2220 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.09927\n",
            "Epoch 131/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.2122 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.09927\n",
            "Epoch 132/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 0.2048 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.09927\n",
            "Epoch 133/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.2182 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.09927\n",
            "Epoch 134/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9981 - val_loss: 0.1993 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.09927\n",
            "Epoch 135/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9963 - val_loss: 0.2032 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.09927\n",
            "Epoch 136/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.2031 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.09927\n",
            "Epoch 137/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2319 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.09927\n",
            "Epoch 138/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.2130 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.09927\n",
            "Epoch 139/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.2498 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.09927\n",
            "Epoch 140/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.2326 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.09927\n",
            "Epoch 141/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9944 - val_loss: 0.2346 - val_accuracy: 0.9667\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.09927\n",
            "Epoch 142/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.2169 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.09927\n",
            "Epoch 143/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.2190 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.09927\n",
            "Epoch 144/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.2131 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.09927\n",
            "Epoch 145/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.1992 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.09927\n",
            "Epoch 146/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.2122 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.09927\n",
            "Epoch 147/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.2043 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.09927\n",
            "Epoch 148/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.2695 - val_accuracy: 0.9550\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.09927\n",
            "Epoch 149/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9922 - val_loss: 0.2166 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.09927\n",
            "Epoch 150/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9962 - val_loss: 0.2225 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.09927\n",
            "Epoch 151/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.2114 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.09927\n",
            "Epoch 152/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.2249 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.09927\n",
            "Epoch 153/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.9980 - val_loss: 0.2208 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.09927\n",
            "Epoch 154/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.2194 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.09927\n",
            "Epoch 155/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.2088 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.09927\n",
            "Epoch 156/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.2268 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.09927\n",
            "Epoch 157/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.2207 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.09927\n",
            "Epoch 158/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.2176 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.09927\n",
            "Epoch 159/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.2290 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.09927\n",
            "Epoch 160/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2258 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.09927\n",
            "Epoch 161/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.2219 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.09927\n",
            "Epoch 162/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2323 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.09927\n",
            "Epoch 163/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.2431 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.09927\n",
            "Epoch 164/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.2419 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.09927\n",
            "Epoch 165/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.2334 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.09927\n",
            "Epoch 166/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9971 - val_loss: 0.2248 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.09927\n",
            "Epoch 167/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.2577 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.09927\n",
            "Epoch 168/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9977 - val_loss: 0.2290 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.09927\n",
            "Epoch 169/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.2623 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.09927\n",
            "Epoch 170/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.2319 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.09927\n",
            "Epoch 171/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.2446 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.09927\n",
            "Epoch 172/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.2294 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.09927\n",
            "Epoch 173/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.2245 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.09927\n",
            "Epoch 174/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.2326 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.09927\n",
            "Epoch 175/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.2367 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.09927\n",
            "Epoch 176/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.2345 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.09927\n",
            "Epoch 177/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.2385 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.09927\n",
            "Epoch 178/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.2354 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.09927\n",
            "Epoch 179/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.2548 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.09927\n",
            "Epoch 180/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9969 - val_loss: 0.2503 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.09927\n",
            "Epoch 181/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9970 - val_loss: 0.2371 - val_accuracy: 0.9692\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.09927\n",
            "Epoch 182/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9968 - val_loss: 0.2509 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.09927\n",
            "Epoch 183/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.2348 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.09927\n",
            "Epoch 184/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.2378 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.09927\n",
            "Epoch 185/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9986 - val_loss: 0.2541 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.09927\n",
            "Epoch 186/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.2733 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.09927\n",
            "Epoch 187/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.2566 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.09927\n",
            "Epoch 188/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.2577 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.09927\n",
            "Epoch 189/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.2549 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.09927\n",
            "Epoch 190/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9974 - val_loss: 0.2728 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.09927\n",
            "Epoch 191/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.2489 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.09927\n",
            "Epoch 192/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.2936 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.09927\n",
            "Epoch 193/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.2537 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.09927\n",
            "Epoch 194/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.2376 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.09927\n",
            "Epoch 195/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.2753 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.09927\n",
            "Epoch 196/200\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 0.9981 - val_loss: 0.2984 - val_accuracy: 0.9704\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.09927\n",
            "Epoch 197/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 0.2705 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.09927\n",
            "Epoch 198/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9961 - val_loss: 0.2360 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.09927\n",
            "Epoch 199/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.2615 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.09927\n",
            "Epoch 200/200\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.2750 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.09927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6PBgtRoOCEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710c9262-fe97-4cc4-be35-b34549f9b11d"
      },
      "source": [
        "np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgBoo6qwOePl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebd3ff5-4480-43c8-b1a8-b7ed4491dc69"
      },
      "source": [
        "# load best val model to compute test accuracy\n",
        "history_3.model.load_weights('best_model.h5')\n",
        "np.mean(np.round(history_3.model.predict(x_test_binary))==y_test_binary.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}